#!/bin/bash
# DAT409 Workshop - Database Setup and Data Loading Script
# ===========================================================================
# IDEMPOTENT SCRIPT - Safe to run multiple times
# ===========================================================================
# This script:
# - Drops and recreates all tables fresh each run
# - Truncates existing data before loading
# - Handles user creation/permissions idempotently  
# - Creates only essential indexes (vector, FTS, trigram)
# - Loads 21,704 products with embeddings for Lab 1
# - Creates 50 hardcoded products with RLS for Lab 2
# 
# Prerequisites:
# - Run bootstrap-code-editor.sh first
# - Enable Bedrock Cohere Embed English v3 model
# 
# Usage: ./setup-database.sh

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

log() {
    echo -e "${GREEN}[$(date +'%H:%M:%S')]${NC} $1"
}

warn() {
    echo -e "${YELLOW}[$(date +'%H:%M:%S')] WARNING:${NC} $1"
}

error() {
    echo -e "${RED}[$(date +'%H:%M:%S')] ERROR:${NC} $1"
    exit 1
}

# Load environment variables
if [ -f "/workshop/.env" ]; then
    source /workshop/.env
else
    error ".env file not found. Please run bootstrap-code-editor.sh first"
fi

# Verify database credentials
if [ -z "$DB_HOST" ] || [ -z "$DB_USER" ] || [ -z "$DB_PASSWORD" ] || [ -z "$DB_NAME" ]; then
    error "Database credentials not configured. Check .env file"
fi

log "==================== DAT409 Database Setup ===================="
log "This is an IDEMPOTENT script - safe to run multiple times"
log "It will DROP and RECREATE all tables, then reload data fresh"
log "=============================================================="
log "Database: $DB_HOST:$DB_PORT/$DB_NAME"
log "User: $DB_USER"

# Test database connection
log "Testing database connection..."
if PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -c "SELECT 'Connection successful' as status;" &>/dev/null; then
    log "‚úÖ Database connection successful"
else
    error "Database connection failed. Please check credentials"
fi

# Test Bedrock access
log "Testing Bedrock access..."
BODY_JSON='{"texts":["test"],"input_type":"search_document","embedding_types":["float"],"truncate":"END"}'
BODY_BASE64=$(echo "$BODY_JSON" | base64)

if aws bedrock-runtime invoke-model \
    --model-id cohere.embed-english-v3 \
    --body "$BODY_BASE64" \
    --region "$AWS_REGION" \
    /tmp/bedrock_test.json 2>/dev/null; then
    
    # Verify the response contains embeddings
    if [ -f /tmp/bedrock_test.json ] && [ $(stat -c%s /tmp/bedrock_test.json 2>/dev/null || stat -f%z /tmp/bedrock_test.json 2>/dev/null) -gt 100 ]; then
        log "‚úÖ Bedrock Cohere model accessible and working"
        rm -f /tmp/bedrock_test.json
    else
        error "‚ùå Bedrock model responded but output seems invalid"
    fi
else
    error "‚ùå Bedrock Cohere model not accessible. Please enable it in the console first!"
    echo "To enable:"
    echo "1. Go to https://console.aws.amazon.com/bedrock"
    echo "2. Click 'Model access' in the left sidebar"
    echo "3. Enable 'Cohere Embed English v3'"
    echo "4. Wait for 'Access granted' status"
    exit 1
fi

# ===========================================================================
# LAB 1: CREATE SCHEMA AND TABLES
# ===========================================================================

log "==================== Lab 1: Creating Schema and Tables (Idempotent) ===================="

PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" << 'LAB1_SCHEMA'
-- Create required extensions FIRST
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;

-- Create bedrock_integration schema
CREATE SCHEMA IF NOT EXISTS bedrock_integration;

-- Drop existing tables CASCADE to handle foreign key dependencies
DROP TABLE IF EXISTS public.knowledge_base CASCADE;
DROP TABLE IF EXISTS bedrock_integration.product_catalog CASCADE;

-- Create product_catalog table with COMPLETE columns from parallel-fast-loader.py
CREATE TABLE bedrock_integration.product_catalog (
    "productId" VARCHAR(255) PRIMARY KEY,
    product_description TEXT,
    imgurl TEXT,
    producturl TEXT,
    stars NUMERIC,
    reviews INT,
    price NUMERIC,
    category_id INT,
    isbestseller BOOLEAN,
    boughtinlastmonth INT,
    category_name VARCHAR(255),
    quantity INT,
    embedding vector(1024)
);

SELECT 'Lab 1 schema created successfully with all columns (idempotent)' as status;
LAB1_SCHEMA

if [ $? -eq 0 ]; then
    log "‚úÖ Lab 1 schema and tables created with complete DDL"
else
    error "Failed to create Lab 1 schema"
fi

# ===========================================================================
# LAB 1: LOAD PRODUCTS WITH EMBEDDINGS
# ===========================================================================

log "==================== Lab 1: Loading Product Data ===================="
log "This will load 21,704 products with Cohere embeddings"
log "Expected duration: 5-8 minutes"

# Create Python data loader script
cat > /tmp/load_products.py << 'LOADER_EOF'
#!/usr/bin/env python3
import sys
import os
import time
import json
import boto3
import psycopg
from pathlib import Path
import pandas as pd
import numpy as np
from pgvector.psycopg import register_vector
from pandarallel import pandarallel
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("DAT409 Workshop Data Loader")
print("Loading 21,704 products with embeddings...")
print("="*60)

# Get database credentials from environment
DB_HOST = os.getenv('DB_HOST')
DB_PORT = os.getenv('DB_PORT', '5432')
DB_NAME = os.getenv('DB_NAME')
DB_USER = os.getenv('DB_USER')
DB_PASSWORD = os.getenv('DB_PASSWORD')
AWS_REGION = os.getenv('AWS_REGION', 'us-west-2')

# Configuration
BATCH_SIZE = 1000
PARALLEL_WORKERS = 10

# Set up paths
WORKSHOP_DIR = Path("/workshop")
DATA_FILE = WORKSHOP_DIR / "lab1-hybrid-search/data/amazon-products.csv"

# Check if data file exists, if not download it
if not DATA_FILE.exists():
    print(f"Data file not found, downloading from GitHub...")
    os.system(f"mkdir -p {DATA_FILE.parent}")
    os.system(f"curl -fsSL https://raw.githubusercontent.com/aws-samples/sample-dat409-hybrid-search-workshop-prod/main/lab1-hybrid-search/data/amazon-products.csv -o {DATA_FILE}")

if not DATA_FILE.exists():
    print("‚ùå Could not download data file")
    sys.exit(1)

print(f"‚úÖ Data file found: {DATA_FILE}")

start_time = time.time()

# Initialize Bedrock client (global for parallel access)
bedrock_runtime = boto3.client('bedrock-runtime', region_name=AWS_REGION)

def generate_embedding_cohere(text):
    """Generate Cohere embedding with error handling"""
    try:
        if pd.isna(text) or str(text).strip() == '':
            return np.zeros(1024).tolist()
        
        clean_text = str(text)[:2000].strip()
        
        body = {
            "texts": [clean_text],
            "input_type": "search_document",
            "embedding_types": ["float"],
            "truncate": "END"
        }
        
        response = bedrock_runtime.invoke_model(
            modelId="cohere.embed-english-v3",
            body=json.dumps(body),
            contentType="application/json",
            accept="application/json"
        )
        
        result = json.loads(response['body'].read())
        
        if 'embeddings' in result and 'float' in result['embeddings']:
            embedding = result['embeddings']['float'][0]
            if len(embedding) == 1024:
                return embedding
        
        return np.zeros(1024).tolist()
    
    except Exception as e:
        return np.zeros(1024).tolist()

# Test database connection
print("\nüîó Testing database connection...")
try:
    conn = psycopg.connect(
        host=DB_HOST,
        port=DB_PORT,
        dbname=DB_NAME,
        user=DB_USER,
        password=DB_PASSWORD,
        autocommit=True
    )
    print("‚úÖ Database connection successful")
    conn.close()
except psycopg.OperationalError as e:
    print(f"‚ùå Database connection failed: {e}")
    sys.exit(1)

# Load product data
print("\nüìÇ Loading product data...")
df = pd.read_csv(str(DATA_FILE))

# Clean up missing values with proper defaults for ALL columns
df = df.dropna(subset=['product_description'])
df = df.fillna({
    'stars': 0,
    'reviews': 0,
    'price': 0,
    'category_id': 0,
    'isbestseller': False,
    'boughtinlastmonth': 0,
    'category_name': 'Unknown',
    'quantity': 0,
    'imgurl': '',
    'producturl': ''
})

# Ensure unique productIds
if 'productId' not in df.columns or df['productId'].isna().any():
    df['productId'] = ['B' + str(i).zfill(6) for i in range(len(df))]

print(f"‚úÖ Processed {len(df)} products")

# Clean text fields
df['product_description'] = df['product_description'].str[:2000]
df['imgurl'] = df['imgurl'].astype(str).str[:500]
df['producturl'] = df['producturl'].astype(str).str[:500]
df['category_name'] = df['category_name'].astype(str).str[:255]

# Ensure proper data types
df['stars'] = pd.to_numeric(df['stars'], errors='coerce').fillna(0)
df['reviews'] = pd.to_numeric(df['reviews'], errors='coerce').fillna(0).astype(int)
df['price'] = pd.to_numeric(df['price'], errors='coerce').fillna(0)
df['category_id'] = pd.to_numeric(df['category_id'], errors='coerce').fillna(0).astype(int)
df['isbestseller'] = df['isbestseller'].astype(bool)
df['boughtinlastmonth'] = pd.to_numeric(df['boughtinlastmonth'], errors='coerce').fillna(0).astype(int)
df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce').fillna(0).astype(int)

# Initialize pandarallel for parallel processing
print("\nüß† Generating embeddings in parallel...")
print(f"   Using {PARALLEL_WORKERS} parallel workers")
print("   This will take 5-8 minutes for 21,704 products...")

pandarallel.initialize(progress_bar=True, nb_workers=PARALLEL_WORKERS, verbose=0)

# Generate embeddings in parallel
embed_start_time = time.time()
df['embedding'] = df['product_description'].parallel_apply(generate_embedding_cohere)
embed_time = time.time() - embed_start_time

print(f"\n‚úÖ Embeddings generated in {embed_time/60:.1f} minutes")
print(f"   Rate: {len(df)/embed_time:.1f} products/second")

# Connect to database
conn = psycopg.connect(
    host=DB_HOST,
    port=DB_PORT,
    dbname=DB_NAME,
    user=DB_USER,
    password=DB_PASSWORD,
    autocommit=False
)

# Register pgvector
register_vector(conn)

# Clear existing data for idempotent operation
print("\nüßπ Clearing existing data...")
with conn.cursor() as cur:
    cur.execute("TRUNCATE TABLE bedrock_integration.product_catalog CASCADE;")
    conn.commit()
print("‚úÖ Table cleared")

# Insert data in batches
print("\nüíæ Inserting data into database...")
total_processed = 0

with tqdm(total=len(df), desc="Inserting products") as pbar:
    for i in range(0, len(df), BATCH_SIZE):
        batch = df.iloc[i:i+BATCH_SIZE]
        batch_start = time.time()
        
        with conn.cursor() as cur:
            for _, row in batch.iterrows():
                try:
                    # Simple INSERT since table is truncated
                    cur.execute('''
                        INSERT INTO bedrock_integration.product_catalog 
                        ("productId", product_description, imgurl, producturl, 
                         stars, reviews, price, category_id, isbestseller,
                         boughtinlastmonth, category_name, quantity, embedding)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ''', (
                        row['productId'],
                        str(row['product_description']),
                        str(row['imgurl']),
                        str(row['producturl']),
                        float(row['stars']),
                        int(row['reviews']),
                        float(row['price']),
                        int(row['category_id']),
                        bool(row['isbestseller']),
                        int(row['boughtinlastmonth']),
                        str(row['category_name']),
                        int(row['quantity']),
                        row['embedding']
                    ))
                except Exception as e:
                    print(f"Error inserting product {row.get('productId', 'unknown')}: {e}")
                    continue
        
        conn.commit()
        total_processed += len(batch)
        batch_time = time.time() - batch_start
        pbar.update(len(batch))

# Create only essential indexes
print("\nüîç Creating essential indexes...")
essential_indexes = [
    ("HNSW vector index", "product_catalog_embedding_idx", """
        CREATE INDEX product_catalog_embedding_idx 
        ON bedrock_integration.product_catalog 
        USING hnsw (embedding vector_cosine_ops)
        WITH (m = 16, ef_construction = 64);
    """),
    ("Full-text search GIN", "product_catalog_fts_idx", """
        CREATE INDEX product_catalog_fts_idx 
        ON bedrock_integration.product_catalog
        USING GIN (to_tsvector('english', coalesce(product_description, '')));
    """),
    ("Trigram GIN", "product_catalog_trgm_idx", """
        CREATE INDEX product_catalog_trgm_idx 
        ON bedrock_integration.product_catalog
        USING GIN (product_description gin_trgm_ops);
    """)
]

with conn.cursor() as cur:
    for name, index_name, sql in essential_indexes:
        try:
            print(f"  Creating {name}...")
            # Drop if exists for idempotent operation
            cur.execute(f"DROP INDEX IF EXISTS bedrock_integration.{index_name};")
            cur.execute(sql)
            conn.commit()
            print(f"  ‚úÖ {name} created")
        except Exception as e:
            print(f"  ‚ö†Ô∏è {name} error: {e}")
            conn.rollback()

# Final statistics
cur = conn.cursor()
cur.execute("SELECT COUNT(*) FROM bedrock_integration.product_catalog;")
final_count = cur.fetchone()[0]
cur.execute("SELECT COUNT(*) FROM bedrock_integration.product_catalog WHERE embedding IS NOT NULL;")
embeddings_count = cur.fetchone()[0]

# For vector type, we know dimensions are 1024
avg_dims = 1024

conn.close()

total_time = time.time() - start_time

print("\n" + "="*60)
print("‚úÖ Data loading completed successfully!")
print(f"   Total rows loaded: {final_count:,}")
print(f"   Rows with embeddings: {embeddings_count:,}")
print(f"   Embedding dimensions: {avg_dims}")
print(f"   Total time: {total_time/60:.1f} minutes")
print("="*60)
LOADER_EOF

# Run the Python loader
log "Running Python data loader..."
if python3 /tmp/load_products.py; then
    log "‚úÖ Lab 1 data loaded successfully"
else
    error "Failed to load Lab 1 data"
fi

# ===========================================================================
# LAB 2: CREATE KNOWLEDGE BASE AND RLS WITH 50 PRODUCTS
# ===========================================================================

log "==================== Lab 2: Creating Knowledge Base and RLS ===================="

# Create temporary SQL file for Lab 2
cat > /tmp/lab2_setup.sql << 'EOF_LAB2'
-- ============================================================
-- LAB 2: MCP with PostgreSQL RLS - 50 Product Setup
-- ============================================================

-- 1. Drop and recreate knowledge_base table
DROP TABLE IF EXISTS public.knowledge_base CASCADE;
CREATE TABLE public.knowledge_base (
    id SERIAL PRIMARY KEY,
    product_id VARCHAR(255),
    content TEXT NOT NULL,
    content_type VARCHAR(50) NOT NULL,
    persona_access VARCHAR(50)[] NOT NULL,
    severity VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    metadata JSONB DEFAULT '{}',
    
    CONSTRAINT fk_product 
        FOREIGN KEY (product_id) 
        REFERENCES bedrock_integration.product_catalog("productId")
        ON DELETE CASCADE
);

-- Create essential indexes only
CREATE INDEX idx_kb_product_id ON knowledge_base(product_id);
CREATE INDEX idx_kb_persona_access ON knowledge_base USING GIN (persona_access);
CREATE INDEX idx_kb_content_type ON knowledge_base(content_type);

-- 2. Create RLS users (ignore if they exist)
CREATE USER customer_user WITH PASSWORD 'customer123';
CREATE USER agent_user WITH PASSWORD 'agent123';
CREATE USER pm_user WITH PASSWORD 'pm123';

-- 3. Grant permissions
GRANT USAGE ON SCHEMA public TO customer_user, agent_user, pm_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO customer_user, agent_user, pm_user;
GRANT USAGE ON SCHEMA bedrock_integration TO customer_user, agent_user, pm_user;
GRANT SELECT ON ALL TABLES IN SCHEMA bedrock_integration TO customer_user, agent_user, pm_user;

-- 4. Enable RLS
ALTER TABLE knowledge_base ENABLE ROW LEVEL SECURITY;

-- 5. Create RLS policies
CREATE POLICY customer_policy ON knowledge_base
    FOR SELECT TO customer_user
    USING ('customer' = ANY(persona_access));

CREATE POLICY agent_policy ON knowledge_base
    FOR SELECT TO agent_user  
    USING ('customer' = ANY(persona_access) OR 'agent' = ANY(persona_access));

CREATE POLICY pm_policy ON knowledge_base
    FOR SELECT TO pm_user
    USING (true);

-- 6. Populate with 50 hardcoded products
-- Insert sample data directly without DO block to avoid syntax issues
INSERT INTO knowledge_base (product_id, content, content_type, persona_access, severity) 
SELECT 
    p."productId",
    'Q: What is the warranty? A: Standard 1-year manufacturer warranty.',
    'product_faq',
    ARRAY['customer', 'support_agent', 'product_manager'],
    'low'
FROM bedrock_integration.product_catalog p
WHERE p."productId" IN (
    'B07X6C9RMF', 'B08N5NQ869', 'B086DL32R3', 'B08SGC46M9', 'B07DGR98VQ',
    'B08R59YH7W', 'B08CKHPP52', 'B08M125RNW', 'B0849J7W5X', 'B08F6GPQQ7',
    'B08FD54PN9', 'B07QKXM2D3', 'B01CW4CEMS', 'B07X27JNQ5', 'B07ZB2RNTW',
    'B07YB8HZ8T', 'B08ZXJJTYJ', 'B0829KDY9X', 'B093DDPDXL', 'B07PM2NBGT',
    'B07TTH5TMW', 'B07B7NXV4R', 'B011MYEMKQ', 'B07YP9VK7Q', 'B07ZB2QF2V',
    'B0CFR1JB15', 'B00HT6E2NY', 'B0CBJRXFVJ', 'B00PBGQ0SY', 'B0168MB1RO',
    'B0C8JGHXXB', 'B0C8JDM69N', 'B0C2PXPWMR', 'B0C8JK6TSH', 'B0C3RKQPHR',
    'B07GG3XXNX', 'B0899GLP7R', 'B07PJ67CKC', 'B088C4NHRS', 'B07WHMQNPC',
    'B07YMV9VMT', 'B07ZPMCW64', 'B0856W45VL', 'B07W1HKYQK', 'B07R3WY95C',
    'B01CW49AGG', 'B07X81M2D2', 'B07X2M8KTR', 'B08JCS7QKL', 'B083GKZWVX'
)
LIMIT 50;

-- Add support tickets for high-review products
INSERT INTO knowledge_base (product_id, content, content_type, persona_access, severity)
SELECT 
    p."productId",
    'Support ticket: Device connectivity issue resolved',
    'support_ticket',
    ARRAY['support_agent', 'product_manager'],
    'medium'
FROM bedrock_integration.product_catalog p
WHERE p.reviews > 20000
AND p."productId" IN (
    'B07X6C9RMF', 'B08N5NQ869', 'B086DL32R3', 'B08SGC46M9', 'B07DGR98VQ',
    'B08R59YH7W', 'B08CKHPP52', 'B08M125RNW', 'B0849J7W5X', 'B08F6GPQQ7',
    'B08FD54PN9', 'B07QKXM2D3', 'B01CW4CEMS', 'B07X27JNQ5', 'B07ZB2RNTW',
    'B07YB8HZ8T', 'B08ZXJJTYJ', 'B0829KDY9X', 'B093DDPDXL', 'B07PM2NBGT',
    'B07TTH5TMW', 'B07B7NXV4R', 'B011MYEMKQ', 'B07YP9VK7Q', 'B07ZB2QF2V',
    'B0CFR1JB15', 'B00HT6E2NY', 'B0CBJRXFVJ', 'B00PBGQ0SY', 'B0168MB1RO',
    'B0C8JGHXXB', 'B0C8JDM69N', 'B0C2PXPWMR', 'B0C8JK6TSH', 'B0C3RKQPHR',
    'B07GG3XXNX', 'B0899GLP7R', 'B07PJ67CKC', 'B088C4NHRS', 'B07WHMQNPC',
    'B07YMV9VMT', 'B07ZPMCW64', 'B0856W45VL', 'B07W1HKYQK', 'B07R3WY95C',
    'B01CW49AGG', 'B07X81M2D2', 'B07X2M8KTR', 'B08JCS7QKL', 'B083GKZWVX'
);

-- Add analytics for expensive products
INSERT INTO knowledge_base (product_id, content, content_type, persona_access, severity)
SELECT 
    p."productId",
    'Analytics: Weekly sales meeting targets',
    'analytics',
    ARRAY['product_manager'],
    NULL
FROM bedrock_integration.product_catalog p
WHERE p.price > 50
AND p."productId" IN (
    'B07X6C9RMF', 'B08N5NQ869', 'B086DL32R3', 'B08SGC46M9', 'B07DGR98VQ',
    'B08R59YH7W', 'B08CKHPP52', 'B08M125RNW', 'B0849J7W5X', 'B08F6GPQQ7',
    'B08FD54PN9', 'B07QKXM2D3', 'B01CW4CEMS', 'B07X27JNQ5', 'B07ZB2RNTW',
    'B07YB8HZ8T', 'B08ZXJJTYJ', 'B0829KDY9X', 'B093DDPDXL', 'B07PM2NBGT',
    'B07TTH5TMW', 'B07B7NXV4R', 'B011MYEMKQ', 'B07YP9VK7Q', 'B07ZB2QF2V',
    'B0CFR1JB15', 'B00HT6E2NY', 'B0CBJRXFVJ', 'B00PBGQ0SY', 'B0168MB1RO',
    'B0C8JGHXXB', 'B0C8JDM69N', 'B0C2PXPWMR', 'B0C8JK6TSH', 'B0C3RKQPHR',
    'B07GG3XXNX', 'B0899GLP7R', 'B07PJ67CKC', 'B088C4NHRS', 'B07WHMQNPC',
    'B07YMV9VMT', 'B07ZPMCW64', 'B0856W45VL', 'B07W1HKYQK', 'B07R3WY95C',
    'B01CW49AGG', 'B07X81M2D2', 'B07X2M8KTR', 'B08JCS7QKL', 'B083GKZWVX'
);

-- Add general entries
INSERT INTO knowledge_base (product_id, content, content_type, persona_access, severity) VALUES
(NULL, 'Holiday return policy extended through January', 'product_faq', ARRAY['customer', 'support_agent', 'product_manager'], 'low'),
(NULL, 'System maintenance scheduled for Sunday 2am', 'internal_note', ARRAY['support_agent', 'product_manager'], 'medium');

-- Verification
SELECT 'Lab 2 completed' as status;
SELECT content_type, COUNT(*) FROM knowledge_base GROUP BY content_type;
EOF_LAB2

# Execute Lab 2 SQL (ignore user creation errors)
log "Executing Lab 2 SQL..."
PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" -f /tmp/lab2_setup.sql || true

# Verify the important parts worked
KB_COUNT=$(PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -t -c "SELECT COUNT(*) FROM public.knowledge_base;" 2>/dev/null | xargs)

if [ "$KB_COUNT" -gt 0 ]; then
    log "‚úÖ Lab 2 RLS and knowledge base created successfully"
    rm -f /tmp/lab2_setup.sql
else
    error "Lab 2 setup failed - no knowledge base entries created"
fi

# ===========================================================================
# CREATE TEST SCRIPT
# ===========================================================================

log "Creating test script..."
cat > /workshop/lab2-mcp-agent/test_personas.sh << 'TEST_SCRIPT'
#!/bin/bash
source /workshop/.env
echo "Testing RLS personas..."
echo "Customer view:"
PGPASSWORD=customer123 psql -h $PGHOST -U customer_user -d $PGDATABASE -c "SELECT content_type, COUNT(*) FROM knowledge_base GROUP BY content_type;"
echo "Agent view:"
PGPASSWORD=agent123 psql -h $PGHOST -U agent_user -d $PGDATABASE -c "SELECT content_type, COUNT(*) FROM knowledge_base GROUP BY content_type;"
echo "PM view:"
PGPASSWORD=pm123 psql -h $PGHOST -U pm_user -d $PGDATABASE -c "SELECT content_type, COUNT(*) FROM knowledge_base GROUP BY content_type;"
TEST_SCRIPT
chmod +x /workshop/lab2-mcp-agent/test_personas.sh

# ===========================================================================
# FINAL VERIFICATION
# ===========================================================================

log "==================== Final Verification ===================="

LAB1_COUNT=$(PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -t -c "SELECT COUNT(*) FROM bedrock_integration.product_catalog;" 2>/dev/null | xargs)

LAB1_EMBEDDINGS=$(PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -t -c "SELECT COUNT(*) FROM bedrock_integration.product_catalog WHERE embedding IS NOT NULL;" 2>/dev/null | xargs)

LAB2_COUNT=$(PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -t -c "SELECT COUNT(*) FROM public.knowledge_base;" 2>/dev/null | xargs)

LAB2_POLICIES=$(PGPASSWORD="$DB_PASSWORD" psql -h "$DB_HOST" -p "$DB_PORT" -U "$DB_USER" -d "$DB_NAME" \
    -t -c "SELECT COUNT(*) FROM pg_policies WHERE tablename='knowledge_base';" 2>/dev/null | xargs)

log "==================== Setup Complete! ===================="
echo ""
echo "üìä LAB 1 - Hybrid Search:"
echo "   ‚úÖ Products loaded: $LAB1_COUNT"
echo "   ‚úÖ Products with embeddings: $LAB1_EMBEDDINGS"
echo ""
echo "üîí LAB 2 - MCP with RLS:"
echo "   ‚úÖ Knowledge base entries: $LAB2_COUNT"
echo "   ‚úÖ RLS policies created: $LAB2_POLICIES"
echo ""
echo "üîç Test Commands:"
echo "   Lab 1: psql -c \"SELECT COUNT(*) FROM bedrock_integration.product_catalog;\""
echo "   Lab 2: /workshop/lab2-mcp-agent/test_personas.sh"
echo ""
echo "üöÄ All database setup completed successfully!"
log "========================================================"