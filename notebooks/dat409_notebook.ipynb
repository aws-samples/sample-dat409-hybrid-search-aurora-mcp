{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT409 | Implement hybrid search with Aurora PostgreSQL for MCP retrieval\n",
    "\n",
    "## üöÄ Build Production-Ready Hybrid Search with Amazon Aurora PostgreSQL & Amazon Bedrock\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Builder's Session\n",
    "- **Duration**: 60 minutes\n",
    "- **Level**: 400 (Expert)\n",
    "- **Prerequisites**: Aurora PostgreSQL cluster, AWS credentials configured\n",
    "\n",
    "### üéØ Your Mission: The Black Friday Playbook\n",
    "\n",
    "**It's November 1st.** Black Friday is 28 days away. You have engineering logs from 365 days of operations - insights from DBAs, SREs, developers, and data engineers.\n",
    "\n",
    "**The Challenge**: Different teams describe the same problems differently:\n",
    "- DBA: \"FATAL: remaining connection slots are reserved\"\n",
    "- Developer: \"HikariPool-1 - Connection timeout after 30000ms\"\n",
    "- SRE: \"CloudWatch Alarm: DatabaseConnections > 990\"\n",
    "\n",
    "**Your Solution**: Build a hybrid search system that understands ALL these perspectives using:\n",
    "- **Trigram search** (pg_trgm) - Handles typos AND exact matches\n",
    "- **Semantic search** (pgvector) - Understands concepts\n",
    "- **ML Reranking** (Cohere) - Optimizes relevance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Module 1: Environment Setup\n",
    "\n",
    "Install and import all required packages for hybrid search implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "print(\"üì¶ Installing required packages...\")\n",
    "%pip install --quiet --upgrade psycopg pgvector boto3 pandas numpy tqdm python-dotenv ipywidgets 2>/dev/null\n",
    "\n",
    "# Import all libraries\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from difflib import SequenceMatcher\n",
    "import psycopg\n",
    "from pgvector.psycopg import register_vector\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(\"\\nüéØ Ready to build hybrid search for Black Friday preparation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Module 2: Load and Analyze Historical Data\n",
    "\n",
    "Load incident logs from the past year and understand the data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze historical incident data\n",
    "print(\"üìÇ Loading Historical Incident Data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    with open('../data/incident_logs.json', 'r') as f:\n",
    "        incident_logs = json.load(f)\n",
    "    print(\"‚úÖ Loaded dataset\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Dataset not found at ../data/incident_logs.json\")\n",
    "    print(\"   Please ensure the incident_logs.json file is in the data directory\")\n",
    "    raise\n",
    "\n",
    "print(f\"‚úÖ Total logs: {len(incident_logs):,}\")\n",
    "print(f\"üìÖ Timespan: 365 days of engineering observations\\n\")\n",
    "\n",
    "# Analyze distribution\n",
    "personas = {}\n",
    "severities = {}\n",
    "critical_samples = []\n",
    "\n",
    "for log in incident_logs:\n",
    "    persona = log['mcp_metadata']['persona']\n",
    "    severity = log['mcp_metadata'].get('severity', 'info')\n",
    "    personas[persona] = personas.get(persona, 0) + 1\n",
    "    severities[severity] = severities.get(severity, 0) + 1\n",
    "    \n",
    "    if severity == 'critical' and len(critical_samples) < 2:\n",
    "        critical_samples.append((persona, log['content'][:100]))\n",
    "\n",
    "# Display analysis\n",
    "print(\"üë• Engineering Teams (Different Perspectives):\")\n",
    "for persona, count in sorted(personas.items()):\n",
    "    print(f\"  {persona:15} {count:4} logs\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Severity Distribution:\")\n",
    "for severity, count in sorted(severities.items()):\n",
    "    emoji = {'critical': 'üî¥', 'warning': 'üü°', 'info': 'üîµ'}.get(severity, '‚ö™')\n",
    "    percentage = (count / len(incident_logs)) * 100\n",
    "    print(f\"  {emoji} {severity:8}: {count:4} logs ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüî¥ Sample Critical Incidents:\")\n",
    "for persona, content in critical_samples:\n",
    "    print(f\"  [{persona}] {content}...\")\n",
    "\n",
    "print(\"\\nüí° Challenge: Each team uses different terminology for the same issue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÑÔ∏è Module 3: Connect to Aurora PostgreSQL\n",
    "\n",
    "Establish connection to Aurora PostgreSQL with pgvector support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Aurora PostgreSQL\n",
    "print(\"üîå Connecting to Aurora PostgreSQL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('DB_HOST'),\n",
    "    'dbname': os.getenv('DB_NAME'),\n",
    "    'user': os.getenv('DB_USER'),\n",
    "    'password': os.getenv('DB_PASSWORD'),\n",
    "    'port': int(os.getenv('DB_PORT', '5432'))\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create a connection to Aurora PostgreSQL with pgvector support\"\"\"\n",
    "    conn = psycopg.connect(**DB_CONFIG, autocommit=True)\n",
    "    register_vector(conn)\n",
    "    return conn\n",
    "\n",
    "# Establish connection\n",
    "conn = get_db_connection()\n",
    "\n",
    "# Verify connection and extensions\n",
    "with conn.cursor() as cur:\n",
    "    cur.execute(\"SELECT version();\")\n",
    "    db_version = cur.fetchone()[0]\n",
    "    \n",
    "    cur.execute(\"SELECT extname FROM pg_extension WHERE extname IN ('vector', 'pg_trgm');\")\n",
    "    extensions = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "print(f\"‚úÖ Connected successfully\")\n",
    "print(f\"   Version: {db_version.split(',')[0]}\")\n",
    "print(f\"   Extensions: {', '.join(extensions) if extensions else 'Will be enabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Module 4: Create Schema and Load Data\n",
    "\n",
    "Create optimized schema for hybrid search and load data with embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema\n",
    "print(\"üèóÔ∏è Setting Up Database Schema\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # Enable extensions\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "    cur.execute(\"CREATE EXTENSION IF NOT EXISTS pg_trgm;\")\n",
    "    print(\"‚úÖ Extensions enabled: pgvector, pg_trgm\")\n",
    "    \n",
    "    # Create table\n",
    "    cur.execute(\"DROP TABLE IF EXISTS incident_logs CASCADE;\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE incident_logs (\n",
    "            doc_id TEXT PRIMARY KEY,\n",
    "            content TEXT NOT NULL,\n",
    "            persona TEXT NOT NULL,\n",
    "            timestamp TIMESTAMPTZ NOT NULL,\n",
    "            severity TEXT DEFAULT 'info',\n",
    "            metrics JSONB,\n",
    "            content_embedding vector(1024),\n",
    "            created_at TIMESTAMPTZ DEFAULT NOW()\n",
    "        );\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ Table created with hybrid search capabilities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Module 5: Generate Embeddings with Amazon Bedrock\n",
    "\n",
    "Set up Cohere Embed v3 for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Amazon Bedrock\n",
    "print(\"üß† Setting Up Amazon Bedrock with Cohere Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize Bedrock clients\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name='bedrock-agent-runtime',\n",
    "    region_name='us-west-2'\n",
    ")\n",
    "\n",
    "USING_REAL_EMBEDDINGS = True\n",
    "\n",
    "def generate_embedding(text: str, input_type: str = 'search_document') -> List[float]:\n",
    "    \"\"\"Generate embeddings using Cohere Embed v3\"\"\"\n",
    "    global USING_REAL_EMBEDDINGS\n",
    "    \n",
    "    if len(text) > 2048:\n",
    "        text = text[:2048]\n",
    "    \n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='cohere.embed-english-v3',\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\n",
    "                'texts': [text],\n",
    "                'input_type': input_type,\n",
    "                'truncate': 'END'\n",
    "            })\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        USING_REAL_EMBEDDINGS = True\n",
    "        return response_body['embeddings'][0]\n",
    "    except Exception as e:\n",
    "        USING_REAL_EMBEDDINGS = False\n",
    "        np.random.seed(hash(text) % 2**32)\n",
    "        return np.random.randn(1024).tolist()\n",
    "\n",
    "def generate_embeddings_batch(texts: List[str], input_type: str = 'search_document') -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for multiple texts efficiently\"\"\"\n",
    "    global USING_REAL_EMBEDDINGS\n",
    "    max_batch_size = 96\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), max_batch_size):\n",
    "        batch_texts = texts[i:i+max_batch_size]\n",
    "        batch_texts = [text[:2048] if len(text) > 2048 else text for text in batch_texts]\n",
    "        \n",
    "        try:\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                modelId='cohere.embed-english-v3',\n",
    "                contentType='application/json',\n",
    "                accept='application/json',\n",
    "                body=json.dumps({\n",
    "                    'texts': batch_texts,\n",
    "                    'input_type': input_type,\n",
    "                    'truncate': 'END'\n",
    "                })\n",
    "            )\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            all_embeddings.extend(response_body['embeddings'])\n",
    "            USING_REAL_EMBEDDINGS = True\n",
    "        except Exception as e:\n",
    "            USING_REAL_EMBEDDINGS = False\n",
    "            for text in batch_texts:\n",
    "                np.random.seed(hash(text) % 2**32)\n",
    "                all_embeddings.append(np.random.randn(1024).tolist())\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Test embedding\n",
    "test_embedding = generate_embedding(\"test\", 'search_query')\n",
    "print(f\"‚úÖ Embeddings ready (1024 dimensions)\")\n",
    "print(f\"   {'Using REAL Cohere embeddings' if USING_REAL_EMBEDDINGS else 'Using MOCK embeddings (Bedrock unavailable)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Module 6: Load Data with Embeddings\n",
    "\n",
    "Load incident logs into Aurora with batch embedding generation for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with embeddings\n",
    "print(\"üíæ Loading Data with Embeddings\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def load_data_optimized(conn, logs: List[Dict], batch_size: int = 50):\n",
    "    \"\"\"Load data with batch embedding generation\"\"\"\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"TRUNCATE TABLE incident_logs;\")\n",
    "        conn.commit()\n",
    "        \n",
    "        stats = {'loaded': 0, 'errors': 0, 'api_calls': 0}\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        print(f\"Generating embeddings for {len(logs)} logs...\")\n",
    "        all_embeddings = []\n",
    "        texts_for_embedding = [log['content'] for log in logs]\n",
    "        \n",
    "        with tqdm(total=len(texts_for_embedding), desc=\"Embeddings\", unit=\"text\") as pbar:\n",
    "            for i in range(0, len(texts_for_embedding), 96):\n",
    "                batch_texts = texts_for_embedding[i:i+96]\n",
    "                batch_embeddings = generate_embeddings_batch(batch_texts, 'search_document')\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                stats['api_calls'] += 1\n",
    "                pbar.update(len(batch_texts))\n",
    "        \n",
    "        # Insert data\n",
    "        print(f\"Inserting into database...\")\n",
    "        \n",
    "        with tqdm(total=len(logs), desc=\"Inserting\", unit=\"log\") as pbar:\n",
    "            for i in range(0, len(logs), batch_size):\n",
    "                batch_logs = logs[i:i+batch_size]\n",
    "                batch_embeddings = all_embeddings[i:i+batch_size]\n",
    "                \n",
    "                insert_data = []\n",
    "                for log, embedding in zip(batch_logs, batch_embeddings):\n",
    "                    insert_data.append((\n",
    "                        log['doc_id'],\n",
    "                        log['content'],\n",
    "                        log['mcp_metadata']['persona'],\n",
    "                        log['mcp_metadata']['timestamp'],\n",
    "                        log['mcp_metadata'].get('severity', 'info'),\n",
    "                        json.dumps(log['mcp_metadata'].get('metrics', {})),\n",
    "                        embedding\n",
    "                    ))\n",
    "                \n",
    "                try:\n",
    "                    cur.executemany(\"\"\"\n",
    "                        INSERT INTO incident_logs (\n",
    "                            doc_id, content, persona, timestamp,\n",
    "                            severity, metrics, content_embedding\n",
    "                        ) VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT (doc_id) DO NOTHING;\n",
    "                    \"\"\", insert_data)\n",
    "                    stats['loaded'] += len(insert_data)\n",
    "                    pbar.update(len(batch_logs))\n",
    "                except Exception as e:\n",
    "                    stats['errors'] += len(batch_logs)\n",
    "                    pbar.update(len(batch_logs))\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Load the data\n",
    "start_time = time.time()\n",
    "stats = load_data_optimized(conn, incident_logs, batch_size=100)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Data Loading Complete!\")\n",
    "print(f\"   Loaded: {stats['loaded']:,} logs in {elapsed:.1f}s\")\n",
    "print(f\"   API Calls: {stats['api_calls']} (batch processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Module 7: Create Search Indexes\n",
    "\n",
    "Build high-performance indexes for millisecond-latency search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexes\n",
    "print(\"‚ö° Creating High-Performance Indexes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "with conn.cursor() as cur:\n",
    "    # HNSW index for vectors\n",
    "    print(\"Creating HNSW index for semantic search...\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_embedding\n",
    "        ON incident_logs USING hnsw (content_embedding vector_cosine_ops)\n",
    "        WITH (m = 16, ef_construction = 64);\n",
    "    \"\"\")\n",
    "    \n",
    "    # GIN index for trigrams\n",
    "    print(\"Creating GIN index for trigram search...\")\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_trgm\n",
    "        ON incident_logs USING gin(content gin_trgm_ops);\n",
    "    \"\"\")\n",
    "    \n",
    "    # Metadata indexes\n",
    "    print(\"Creating metadata indexes...\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_persona ON incident_logs(persona);\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_severity ON incident_logs(severity);\")\n",
    "    cur.execute(\"CREATE INDEX IF NOT EXISTS idx_timestamp ON incident_logs(timestamp);\")\n",
    "    \n",
    "    # Update statistics\n",
    "    cur.execute(\"ANALYZE incident_logs;\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All indexes created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Module 8: Implement Core Search Methods\n",
    "\n",
    "Build trigram and semantic search functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core search methods\n",
    "print(\"üîç Implementing Core Search Methods\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def trigram_search(query: str, conn, limit: int = 5) -> List[Tuple]:\n",
    "    \"\"\"Trigram search - handles typos AND exact matches\"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT doc_id, content, persona, severity,\n",
    "                   GREATEST(\n",
    "                       similarity(%s, content),\n",
    "                       word_similarity(%s, content),\n",
    "                       strict_word_similarity(%s, content)\n",
    "                   ) as score\n",
    "            FROM incident_logs\n",
    "            WHERE similarity(%s, content) > 0.1\n",
    "               OR word_similarity(%s, content) > 0.2\n",
    "               OR %s <%% content\n",
    "            ORDER BY score DESC\n",
    "            LIMIT %s;\n",
    "        \"\"\", (query, query, query, query, query, query, limit))\n",
    "        return cur.fetchall()\n",
    "\n",
    "def semantic_search(query: str, conn, limit: int = 5) -> List[Tuple]:\n",
    "    \"\"\"Semantic search - understands concepts\"\"\"\n",
    "    query_embedding = generate_embedding(query, 'search_query')\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT doc_id, content, persona, severity,\n",
    "                   1 - (content_embedding <=> %s::vector) as score\n",
    "            FROM incident_logs\n",
    "            WHERE content_embedding IS NOT NULL\n",
    "            ORDER BY content_embedding <=> %s::vector\n",
    "            LIMIT %s;\n",
    "        \"\"\", (query_embedding, query_embedding, limit))\n",
    "        return cur.fetchall()\n",
    "\n",
    "# Quick test\n",
    "test_query = \"connection pool exhausted\"\n",
    "print(f\"Testing search with: '{test_query}'\\n\")\n",
    "\n",
    "trgm = trigram_search(test_query, conn, 3)\n",
    "print(f\"Trigram found {len(trgm)} results\")\n",
    "if trgm:\n",
    "    print(f\"  Top: {trgm[0][1][:60]}...\")\n",
    "\n",
    "sem = semantic_search(test_query, conn, 3)\n",
    "print(f\"\\nSemantic found {len(sem)} results\")\n",
    "if sem:\n",
    "    print(f\"  Top: {sem[0][1][:60]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Search methods implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Module 9: Hybrid Search with Reciprocal Rank Fusion\n",
    "\n",
    "Combine both search methods for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid search implementation\n",
    "print(\"üöÄ Implementing Hybrid Search\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str, \n",
    "    conn, \n",
    "    weights: Optional[Dict] = None, \n",
    "    limit: int = 10,\n",
    "    persona_filter: Optional[str] = None,\n",
    "    severity_filter: Optional[str] = None\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Hybrid search combining trigram and semantic with deduplication\"\"\"\n",
    "    \n",
    "    # Auto-detect weights\n",
    "    if weights is None:\n",
    "        query_upper = query.upper()\n",
    "        if any(term in query_upper for term in ['PG-', 'ERROR:', 'FATAL:']):\n",
    "            weights = {'semantic': 0.3, 'trigram': 0.7}\n",
    "        elif len(query.split()) > 5:\n",
    "            weights = {'semantic': 0.7, 'trigram': 0.3}\n",
    "        else:\n",
    "            weights = {'semantic': 0.5, 'trigram': 0.5}\n",
    "    \n",
    "    # Build filters\n",
    "    filter_conditions = []\n",
    "    filter_params = []\n",
    "    \n",
    "    if persona_filter:\n",
    "        filter_conditions.append(\"persona = %s\")\n",
    "        filter_params.append(persona_filter)\n",
    "    \n",
    "    if severity_filter:\n",
    "        filter_conditions.append(\"severity = %s\")\n",
    "        filter_params.append(severity_filter)\n",
    "    \n",
    "    where_clause = \" AND \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"\n",
    "    \n",
    "    # Get results from both methods\n",
    "    fetch_limit = limit * 3\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        # Trigram search\n",
    "        trigram_query = f\"\"\"\n",
    "            SELECT doc_id, content, persona, severity,\n",
    "                   GREATEST(\n",
    "                       similarity(%s, content),\n",
    "                       word_similarity(%s, content),\n",
    "                       strict_word_similarity(%s, content)\n",
    "                   ) as score\n",
    "            FROM incident_logs\n",
    "            WHERE (similarity(%s, content) > 0.1\n",
    "                   OR word_similarity(%s, content) > 0.2\n",
    "                   OR %s <%% content) {where_clause}\n",
    "            ORDER BY score DESC\n",
    "            LIMIT %s;\n",
    "        \"\"\"\n",
    "        cur.execute(trigram_query,\n",
    "                   [query, query, query, query, query, query] + filter_params + [fetch_limit])\n",
    "        trgm_results = cur.fetchall()\n",
    "        \n",
    "        # Semantic search\n",
    "        query_embedding = generate_embedding(query, 'search_query')\n",
    "        semantic_query = f\"\"\"\n",
    "            SELECT doc_id, content, persona, severity,\n",
    "                   1 - (content_embedding <=> %s::vector) as score\n",
    "            FROM incident_logs\n",
    "            WHERE content_embedding IS NOT NULL {where_clause}\n",
    "            ORDER BY content_embedding <=> %s::vector\n",
    "            LIMIT %s;\n",
    "        \"\"\"\n",
    "        cur.execute(semantic_query, \n",
    "                   [query_embedding] + filter_params + [query_embedding, fetch_limit])\n",
    "        sem_results = cur.fetchall()\n",
    "    \n",
    "    # Combine with reciprocal rank fusion\n",
    "    all_results = {}\n",
    "    \n",
    "    # Process trigram\n",
    "    for i, (doc_id, content, persona, severity, score) in enumerate(trgm_results):\n",
    "        all_results[doc_id] = {\n",
    "            'content': content, 'persona': persona, 'severity': severity,\n",
    "            'trigram_score': score, 'trigram_rank': 1 / (i + 1),\n",
    "            'semantic_score': 0, 'semantic_rank': 0,\n",
    "            'found_by': ['trigram']\n",
    "        }\n",
    "    \n",
    "    # Process semantic\n",
    "    for i, (doc_id, content, persona, severity, score) in enumerate(sem_results):\n",
    "        if doc_id not in all_results:\n",
    "            all_results[doc_id] = {\n",
    "                'content': content, 'persona': persona, 'severity': severity,\n",
    "                'trigram_score': 0, 'trigram_rank': 0,\n",
    "                'semantic_score': 0, 'semantic_rank': 0,\n",
    "                'found_by': []\n",
    "            }\n",
    "        all_results[doc_id]['semantic_score'] = score\n",
    "        all_results[doc_id]['semantic_rank'] = 1 / (i + 1)\n",
    "        if 'semantic' not in all_results[doc_id]['found_by']:\n",
    "            all_results[doc_id]['found_by'].append('semantic')\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    for doc_id, data in all_results.items():\n",
    "        score_component = (\n",
    "            weights['trigram'] * data['trigram_score'] + \n",
    "            weights['semantic'] * data['semantic_score']\n",
    "        )\n",
    "        rank_component = (\n",
    "            weights['trigram'] * data['trigram_rank'] + \n",
    "            weights['semantic'] * data['semantic_rank']\n",
    "        )\n",
    "        consensus_boost = 1.1 if len(data['found_by']) > 1 else 1.0\n",
    "        data['hybrid_score'] = (0.7 * score_component + 0.3 * rank_component) * consensus_boost\n",
    "    \n",
    "    # Sort and deduplicate\n",
    "    sorted_results = sorted(\n",
    "        all_results.items(),\n",
    "        key=lambda x: x[1]['hybrid_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Deduplicate similar content\n",
    "    deduplicated_results = []\n",
    "    seen_hashes = set()\n",
    "    \n",
    "    for doc_id, data in sorted_results:\n",
    "        normalized_content = ' '.join(data['content'][:200].lower().split())\n",
    "        content_hash = hashlib.md5(normalized_content.encode()).hexdigest()\n",
    "        \n",
    "        if content_hash not in seen_hashes:\n",
    "            deduplicated_results.append((doc_id, data))\n",
    "            seen_hashes.add(content_hash)\n",
    "            if len(deduplicated_results) >= limit:\n",
    "                break\n",
    "    \n",
    "    return deduplicated_results\n",
    "\n",
    "# Test hybrid search\n",
    "test_query = \"connection pool exhausted timeout\"\n",
    "results = hybrid_search(test_query, conn, limit=3)\n",
    "\n",
    "print(f\"Hybrid search for: '{test_query}'\\n\")\n",
    "print(f\"Found {len(results)} results:\\n\")\n",
    "\n",
    "for i, (doc_id, data) in enumerate(results, 1):\n",
    "    emoji = {'critical': 'üî¥', 'warning': 'üü°', 'info': 'üîµ'}.get(data['severity'], '‚ö™')\n",
    "    print(f\"{i}. {emoji} [{data['persona']}] Score: {data['hybrid_score']:.3f}\")\n",
    "    print(f\"   Found by: {', '.join(data['found_by'])}\")\n",
    "    print(f\"   {data['content'][:80]}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Hybrid search implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Module 10: ML Reranking with Cohere\n",
    "\n",
    "Add Cohere Rerank v3.5 for optimal relevance scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Reranking\n",
    "print(\"üéØ Implementing ML Reranking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def rerank_with_cohere(query: str, search_results: List[Tuple], limit: int = 5) -> List[Tuple]:\n",
    "    \"\"\"Rerank results using Cohere Rerank v3.5\"\"\"\n",
    "    \n",
    "    if not search_results:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Prepare documents\n",
    "        documents = []\n",
    "        for doc_id, result in search_results:\n",
    "            doc_text = (\n",
    "                f\"[SEVERITY: {result['severity'].upper()}] \"\n",
    "                f\"[TEAM: {result['persona'].upper()}] \"\n",
    "                f\"{result['content']}\"\n",
    "            )\n",
    "            documents.append({\n",
    "                \"type\": \"INLINE\",\n",
    "                \"inlineDocumentSource\": {\n",
    "                    \"type\": \"TEXT\",\n",
    "                    \"textDocument\": {\"text\": doc_text}\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Call Bedrock Rerank\n",
    "        model_arn = \"arn:aws:bedrock:us-west-2::foundation-model/cohere.rerank-v3-5:0\"\n",
    "        \n",
    "        response = bedrock_agent_runtime.rerank(\n",
    "            queries=[{\"type\": \"TEXT\", \"textQuery\": {\"text\": query}}],\n",
    "            sources=documents,\n",
    "            rerankingConfiguration={\n",
    "                \"type\": \"BEDROCK_RERANKING_MODEL\",\n",
    "                \"bedrockRerankingConfiguration\": {\n",
    "                    \"numberOfResults\": min(limit, len(search_results)),\n",
    "                    \"modelConfiguration\": {\"modelArn\": model_arn}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Process reranked results\n",
    "        reranked = []\n",
    "        for result in response['results']:\n",
    "            idx = result['index']\n",
    "            doc_id, original_result = search_results[idx]\n",
    "            relevance_score = result['relevanceScore']\n",
    "            reranked.append((doc_id, original_result, relevance_score))\n",
    "        \n",
    "        return reranked\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback to hybrid scores\n",
    "        fallback = []\n",
    "        for i, (doc_id, result) in enumerate(search_results[:limit]):\n",
    "            score = result.get('hybrid_score', 0.5 - i * 0.05)\n",
    "            fallback.append((doc_id, result, score))\n",
    "        return fallback\n",
    "\n",
    "def search_pipeline(\n",
    "    query: str, \n",
    "    conn, \n",
    "    limit: int = 5,\n",
    "    persona_filter: Optional[str] = None,\n",
    "    severity_filter: Optional[str] = None\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Complete search pipeline with ML reranking\"\"\"\n",
    "    \n",
    "    # Step 1: Hybrid search\n",
    "    hybrid_results = hybrid_search(\n",
    "        query, conn, \n",
    "        limit=limit * 2,\n",
    "        persona_filter=persona_filter,\n",
    "        severity_filter=severity_filter\n",
    "    )\n",
    "    \n",
    "    # Step 2: ML Reranking\n",
    "    reranked = rerank_with_cohere(query, hybrid_results, limit)\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "# Test complete pipeline\n",
    "test_query = \"FATAL connection database error\"\n",
    "print(f\"Testing complete pipeline: '{test_query}'\\n\")\n",
    "\n",
    "results = search_pipeline(test_query, conn, limit=3, severity_filter='critical')\n",
    "\n",
    "print(f\"Final Results (ML-Reranked):\\n\")\n",
    "for i, (doc_id, data, relevance) in enumerate(results, 1):\n",
    "    emoji = {'critical': 'üî¥', 'warning': 'üü°', 'info': 'üîµ'}.get(data['severity'], '‚ö™')\n",
    "    print(f\"{i}. {emoji} [{data['persona']}] Relevance: {relevance:.3f}\")\n",
    "    print(f\"   {data['content'][:100]}...\\n\")\n",
    "\n",
    "print(\"‚úÖ Complete pipeline with ML reranking ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Interactive Search with Temporal Pattern Analysis\n",
    "\n",
    "Try different queries with time-based filtering to discover peak event patterns, team-specific insights, and severity correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Search Widget with Temporal Filter and Extended Score Interpretation\n",
    "print(\"üéÆ Interactive Hybrid Search Tester - Enhanced Version\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import time\n",
    "import hashlib\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import uuid\n",
    "\n",
    "# Clear any previous widgets\n",
    "clear_output(wait=True)\n",
    "\n",
    "print(\"üéÆ Interactive Hybrid Search Tester (with Temporal Filtering)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify dependencies\n",
    "if 'conn' not in globals():\n",
    "    print(\"‚ùå Database connection not found. Please run Module 3 first!\")\n",
    "    raise RuntimeError(\"Database connection required\")\n",
    "\n",
    "# SINGLETON PATTERN\n",
    "if '_widget_state' not in globals():\n",
    "    _widget_state = {'initialized': False, 'button': None, 'output': None}\n",
    "\n",
    "# Clean up existing widget state\n",
    "if _widget_state['initialized']:\n",
    "    if _widget_state['button'] is not None:\n",
    "        _widget_state['button'].close()\n",
    "    if _widget_state['output'] is not None:\n",
    "        _widget_state['output'].close()\n",
    "    _widget_state = {'initialized': False, 'button': None, 'output': None}\n",
    "\n",
    "# Enhanced search pipeline with temporal filtering\n",
    "def widget_search_pipeline_enhanced(\n",
    "    query: str, \n",
    "    conn, \n",
    "    limit: int = 5,\n",
    "    persona_filter: Optional[str] = None,\n",
    "    severity_filter: Optional[str] = None,\n",
    "    weights: Optional[Dict] = None,\n",
    "    start_date: Optional[datetime] = None,\n",
    "    end_date: Optional[datetime] = None\n",
    ") -> List[Tuple]:\n",
    "    \"\"\"Enhanced search pipeline with temporal filtering\"\"\"\n",
    "    \n",
    "    # Build filters including temporal\n",
    "    filter_conditions = []\n",
    "    filter_params = []\n",
    "    \n",
    "    if persona_filter:\n",
    "        filter_conditions.append(\"persona = %s\")\n",
    "        filter_params.append(persona_filter)\n",
    "    \n",
    "    if severity_filter:\n",
    "        filter_conditions.append(\"severity = %s\")\n",
    "        filter_params.append(severity_filter)\n",
    "    \n",
    "    if start_date:\n",
    "        filter_conditions.append(\"timestamp >= %s\")\n",
    "        filter_params.append(start_date.isoformat() + 'Z')\n",
    "    \n",
    "    if end_date:\n",
    "        filter_conditions.append(\"timestamp <= %s\")\n",
    "        filter_params.append(end_date.isoformat() + 'Z')\n",
    "    \n",
    "    where_clause = \" AND \" + \" AND \".join(filter_conditions) if filter_conditions else \"\"\n",
    "    \n",
    "    # Get results with temporal filtering\n",
    "    fetch_limit = limit * 3\n",
    "    \n",
    "    with conn.cursor() as cur:\n",
    "        # Trigram search with temporal filter\n",
    "        trigram_query = f\"\"\"\n",
    "            SELECT doc_id, content, persona, severity, timestamp,\n",
    "                   GREATEST(\n",
    "                       similarity(%s, content),\n",
    "                       word_similarity(%s, content),\n",
    "                       strict_word_similarity(%s, content)\n",
    "                   ) as score\n",
    "            FROM incident_logs\n",
    "            WHERE (similarity(%s, content) > 0.1\n",
    "                   OR word_similarity(%s, content) > 0.2\n",
    "                   OR %s <%% content) {where_clause}\n",
    "            ORDER BY score DESC\n",
    "            LIMIT %s;\n",
    "        \"\"\"\n",
    "        cur.execute(trigram_query,\n",
    "                   [query, query, query, query, query, query] + filter_params + [fetch_limit])\n",
    "        trgm_results = cur.fetchall()\n",
    "        \n",
    "        # Semantic search with temporal filter\n",
    "        query_embedding = generate_embedding(query, 'search_query')\n",
    "        semantic_query = f\"\"\"\n",
    "            SELECT doc_id, content, persona, severity, timestamp,\n",
    "                   1 - (content_embedding <=> %s::vector) as score\n",
    "            FROM incident_logs\n",
    "            WHERE content_embedding IS NOT NULL {where_clause}\n",
    "            ORDER BY content_embedding <=> %s::vector\n",
    "            LIMIT %s;\n",
    "        \"\"\"\n",
    "        cur.execute(semantic_query, \n",
    "                   [query_embedding] + filter_params + [query_embedding, fetch_limit])\n",
    "        sem_results = cur.fetchall()\n",
    "    \n",
    "    # Process results\n",
    "    all_results = {}\n",
    "    \n",
    "    # Process trigram\n",
    "    for i, row in enumerate(trgm_results):\n",
    "        doc_id, content, persona, severity, timestamp, score = row\n",
    "        all_results[doc_id] = {\n",
    "            'content': content, 'persona': persona, 'severity': severity,\n",
    "            'timestamp': timestamp,\n",
    "            'trigram_score': score, 'trigram_rank': 1 / (i + 1),\n",
    "            'semantic_score': 0, 'semantic_rank': 0,\n",
    "            'found_by': ['trigram']\n",
    "        }\n",
    "    \n",
    "    # Process semantic\n",
    "    for i, row in enumerate(sem_results):\n",
    "        doc_id, content, persona, severity, timestamp, score = row\n",
    "        if doc_id not in all_results:\n",
    "            all_results[doc_id] = {\n",
    "                'content': content, 'persona': persona, 'severity': severity,\n",
    "                'timestamp': timestamp,\n",
    "                'trigram_score': 0, 'trigram_rank': 0,\n",
    "                'semantic_score': 0, 'semantic_rank': 0,\n",
    "                'found_by': []\n",
    "            }\n",
    "        all_results[doc_id]['semantic_score'] = score\n",
    "        all_results[doc_id]['semantic_rank'] = 1 / (i + 1)\n",
    "        if 'semantic' not in all_results[doc_id]['found_by']:\n",
    "            all_results[doc_id]['found_by'].append('semantic')\n",
    "    \n",
    "    # Calculate hybrid scores\n",
    "    if weights is None:\n",
    "        weights = {'semantic': 0.5, 'trigram': 0.5}\n",
    "    \n",
    "    for doc_id, data in all_results.items():\n",
    "        score_component = (\n",
    "            weights['trigram'] * data['trigram_score'] + \n",
    "            weights['semantic'] * data['semantic_score']\n",
    "        )\n",
    "        rank_component = (\n",
    "            weights['trigram'] * data['trigram_rank'] + \n",
    "            weights['semantic'] * data['semantic_rank']\n",
    "        )\n",
    "        consensus_boost = 1.1 if len(data['found_by']) > 1 else 1.0\n",
    "        data['hybrid_score'] = (0.7 * score_component + 0.3 * rank_component) * consensus_boost\n",
    "    \n",
    "    # Sort and deduplicate\n",
    "    sorted_results = sorted(\n",
    "        all_results.items(),\n",
    "        key=lambda x: x[1]['hybrid_score'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # Deduplicate\n",
    "    deduplicated_results = []\n",
    "    seen_hashes = set()\n",
    "    \n",
    "    for doc_id, data in sorted_results:\n",
    "        normalized_content = ' '.join(data['content'][:200].lower().split())\n",
    "        content_hash = hashlib.md5(normalized_content.encode()).hexdigest()\n",
    "        \n",
    "        if content_hash not in seen_hashes:\n",
    "            deduplicated_results.append((doc_id, data))\n",
    "            seen_hashes.add(content_hash)\n",
    "            if len(deduplicated_results) >= limit:\n",
    "                break\n",
    "    \n",
    "    # Try ML reranking if available\n",
    "    if 'rerank_with_cohere' in globals() and deduplicated_results:\n",
    "        try:\n",
    "            reranked = rerank_with_cohere(query, deduplicated_results, limit)\n",
    "            return reranked\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Return with scores\n",
    "    reranked = []\n",
    "    for i, (doc_id, result) in enumerate(deduplicated_results[:limit]):\n",
    "        score = result.get('hybrid_score', 0.5)\n",
    "        reranked.append((doc_id, result, score))\n",
    "    \n",
    "    return reranked\n",
    "\n",
    "# Enhanced explanation\n",
    "explanation = \"\"\"\n",
    "<div style=\"background-color: #f0f8ff; padding: 15px; border-radius: 5px; margin-bottom: 15px;\">\n",
    "<h3>üéØ Why This Matters for Black Friday</h3>\n",
    "<p><b>The Challenge:</b> During peak events, different teams describe the same problem differently:</p>\n",
    "<ul>\n",
    "<li>üîß DBAs say: \"FATAL: connection slots reserved\"</li>\n",
    "<li>üíª Developers say: \"HikariPool timeout 30000ms\"</li>\n",
    "<li>üìä SREs say: \"CloudWatch DatabaseConnections > 990\"</li>\n",
    "</ul>\n",
    "<p><b>The Solution:</b> Hybrid search with temporal filtering helps you:</p>\n",
    "<ul>\n",
    "<li>‚úÖ Find similar incidents from specific time periods (e.g., last Black Friday)</li>\n",
    "<li>‚úÖ Identify seasonal patterns and recurring issues</li>\n",
    "<li>‚úÖ Handle typos and misspellings in urgent situations</li>\n",
    "<li>‚úÖ Discover patterns across team boundaries</li>\n",
    "<li>‚úÖ Build time-aware playbooks for peak events</li>\n",
    "</ul>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Detailed scenario instructions\n",
    "scenarios = \"\"\"\n",
    "<div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 15px;\">\n",
    "    <h4>üí° Detailed Search Scenarios & Instructions</h4>\n",
    "    \n",
    "    <div style=\"background-color: white; padding: 10px; margin: 10px 0; border-left: 4px solid #2196F3;\">\n",
    "        <h5>1. Typo-Tolerant Search (Trigram Power)</h5>\n",
    "        <p><b>Query:</b> \"conection pol exausted\" (intentional typos)</p>\n",
    "        <p><b>Settings:</b></p>\n",
    "        <ul>\n",
    "            <li>Team: All</li>\n",
    "            <li>Severity: Critical</li>\n",
    "            <li>Time Range: All time</li>\n",
    "            <li>Weights: Trigram 70%, Semantic 30%</li>\n",
    "        </ul>\n",
    "        <p><b>Expected:</b> Trigram will find \"connection pool exhausted\" despite typos</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background-color: white; padding: 10px; margin: 10px 0; border-left: 4px solid #4CAF50;\">\n",
    "        <h5>2. Error Code Search (Exact Match)</h5>\n",
    "        <p><b>Query:</b> \"PG-53300\"</p>\n",
    "        <p><b>Settings:</b></p>\n",
    "        <ul>\n",
    "            <li>Team: DBA (DBAs typically log error codes)</li>\n",
    "            <li>Severity: Critical</li>\n",
    "            <li>Time Range: Last 30 days</li>\n",
    "            <li>Weights: Trigram 80%, Semantic 20%</li>\n",
    "        </ul>\n",
    "        <p><b>Expected:</b> Exact error code matches with high precision</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background-color: white; padding: 10px; margin: 10px 0; border-left: 4px solid #FF9800;\">\n",
    "        <h5>3. Conceptual Pattern Search (Semantic Focus)</h5>\n",
    "        <p><b>Query:</b> \"resource exhaustion memory pressure\"</p>\n",
    "        <p><b>Settings:</b></p>\n",
    "        <ul>\n",
    "            <li>Team: All (different teams describe differently)</li>\n",
    "            <li>Severity: All</li>\n",
    "            <li>Time Range: November (Black Friday period)</li>\n",
    "            <li>Weights: Semantic 70%, Trigram 30%</li>\n",
    "        </ul>\n",
    "        <p><b>Expected:</b> Finds related concepts like OOM, swap usage, buffer cache</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background-color: white; padding: 10px; margin: 10px 0; border-left: 4px solid #9C27B0;\">\n",
    "        <h5>4. Historical Pattern Analysis (Temporal Focus)</h5>\n",
    "        <p><b>Query:</b> \"performance degradation slow queries\"</p>\n",
    "        <p><b>Settings:</b></p>\n",
    "        <ul>\n",
    "            <li>Team: All</li>\n",
    "            <li>Severity: Critical + Warning</li>\n",
    "            <li><b>Time Range: Nov 24-30 (Black Friday week)</b></li>\n",
    "            <li>Weights: Balanced 50/50</li>\n",
    "        </ul>\n",
    "        <p><b>Expected:</b> Reveals patterns specific to high-traffic periods</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background-color: white; padding: 10px; margin: 10px 0; border-left: 4px solid #F44336;\">\n",
    "        <h5>5. Cross-Team Correlation</h5>\n",
    "        <p><b>Query:</b> \"connection timeout\"</p>\n",
    "        <p><b>Settings:</b></p>\n",
    "        <ul>\n",
    "            <li><b>Team: Switch between DBA, Developer, SRE</b></li>\n",
    "            <li>Severity: All</li>\n",
    "            <li>Time Range: Last 7 days</li>\n",
    "            <li>Weights: Balanced 50/50</li>\n",
    "        </ul>\n",
    "        <p><b>Expected:</b> See how different teams describe the same issue</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background-color: #fff3cd; padding: 10px; margin-top: 15px; border-radius: 5px;\">\n",
    "        <h5>üéØ Pro Tips for Effective Searching:</h5>\n",
    "        <ul>\n",
    "            <li><b>Start broad, then narrow:</b> Begin with \"All\" filters, then refine based on initial results</li>\n",
    "            <li><b>Use time ranges strategically:</b> Compare peak vs. normal periods to identify patterns</li>\n",
    "            <li><b>Adjust weights based on query type:</b> Error codes ‚Üí high trigram, concepts ‚Üí high semantic</li>\n",
    "            <li><b>Combine filters:</b> Team + Severity + Time gives the most targeted results</li>\n",
    "            <li><b>Look for consensus:</b> Results found by BOTH methods (trigram + semantic) are highest confidence</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(widgets.HTML(explanation))\n",
    "display(widgets.HTML(scenarios))\n",
    "\n",
    "# Create widgets\n",
    "query_input = widgets.Text(\n",
    "    value='connection pool exhausted',\n",
    "    placeholder='Enter search query',\n",
    "    description='Query:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "sample_queries = widgets.Dropdown(\n",
    "    options=[\n",
    "        'connection pool exhausted',\n",
    "        'conection pol exausted',  # with typos\n",
    "        'PG-53300 FATAL',\n",
    "        'resource exhaustion OOM memory',\n",
    "        'deadlock detected',\n",
    "        'performance degradation slow queries',\n",
    "        'buffer cache hit ratio',\n",
    "        'autovacuum dead tuples',\n",
    "        'replication lag',\n",
    "        'disk space critical'\n",
    "    ],\n",
    "    value='connection pool exhausted',\n",
    "    description='Examples:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "persona_filter = widgets.Dropdown(\n",
    "    options=['All'] + ['dba', 'developer', 'sre', 'data_engineer'],\n",
    "    value='All',\n",
    "    description='Team:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "severity_filter = widgets.Dropdown(\n",
    "    options=['All', 'critical', 'warning', 'info'],\n",
    "    value='All',\n",
    "    description='Severity:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Temporal filter options\n",
    "temporal_presets = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('All Time', 'all'),\n",
    "        ('Last 7 Days', '7d'),\n",
    "        ('Last 30 Days', '30d'),\n",
    "        ('November (Black Friday)', 'november'),\n",
    "        ('Black Friday Week', 'bf_week'),\n",
    "        ('December (Holiday)', 'december'),\n",
    "        ('Q4 (Peak Season)', 'q4'),\n",
    "        ('Custom Range', 'custom')\n",
    "    ],\n",
    "    value='all',\n",
    "    description='Time Range:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Custom date pickers (initially hidden)\n",
    "start_date = widgets.DatePicker(\n",
    "    description='Start:',\n",
    "    value=datetime(2024, 1, 1),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "end_date = widgets.DatePicker(\n",
    "    description='End:',\n",
    "    value=datetime(2024, 12, 31),\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "custom_dates_box = widgets.HBox([start_date, end_date])\n",
    "custom_dates_box.layout.display = 'none'\n",
    "\n",
    "semantic_weight = widgets.FloatSlider(\n",
    "    value=0.5, min=0, max=1, step=0.1,\n",
    "    description='Semantic:',\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='.0%'\n",
    ")\n",
    "\n",
    "trigram_weight = widgets.FloatSlider(\n",
    "    value=0.5, min=0, max=1, step=0.1,\n",
    "    description='Trigram:',\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='.0%'\n",
    ")\n",
    "\n",
    "result_limit = widgets.IntSlider(\n",
    "    value=5, min=1, max=10,\n",
    "    description='Results:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='üîç Search',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "output = widgets.Output(layout={'border': '1px solid #ddd', 'padding': '10px', 'border-radius': '5px'})\n",
    "\n",
    "# Store in global state\n",
    "_widget_state['button'] = search_button\n",
    "_widget_state['output'] = output\n",
    "_widget_state['initialized'] = True\n",
    "\n",
    "# Event handlers\n",
    "def on_sample_change(change):\n",
    "    query_input.value = change['new']\n",
    "\n",
    "def on_temporal_change(change):\n",
    "    if change['new'] == 'custom':\n",
    "        custom_dates_box.layout.display = 'flex'\n",
    "    else:\n",
    "        custom_dates_box.layout.display = 'none'\n",
    "\n",
    "sample_queries.observe(on_sample_change, names='value')\n",
    "temporal_presets.observe(on_temporal_change, names='value')\n",
    "\n",
    "def on_semantic_change(change):\n",
    "    trigram_weight.value = 1 - change['new']\n",
    "\n",
    "def on_trigram_change(change):\n",
    "    semantic_weight.value = 1 - change['new']\n",
    "\n",
    "semantic_weight.observe(on_semantic_change, names='value')\n",
    "trigram_weight.observe(on_trigram_change, names='value')\n",
    "\n",
    "_search_lock = False\n",
    "\n",
    "def run_search(b):\n",
    "    \"\"\"Execute search with enhanced features\"\"\"\n",
    "    global _search_lock\n",
    "    \n",
    "    if _search_lock:\n",
    "        return\n",
    "    _search_lock = True\n",
    "    \n",
    "    output.clear_output(wait=True)\n",
    "    \n",
    "    try:\n",
    "        query = query_input.value\n",
    "        if not query:\n",
    "            with output:\n",
    "                display(widgets.HTML(\"<p style='color: red;'>‚ö†Ô∏è Please enter a search query</p>\"))\n",
    "            return\n",
    "        \n",
    "        # Prepare parameters\n",
    "        persona = None if persona_filter.value == 'All' else persona_filter.value\n",
    "        severity = None if severity_filter.value == 'All' else severity_filter.value\n",
    "        weights = {\n",
    "            'semantic': semantic_weight.value,\n",
    "            'trigram': trigram_weight.value\n",
    "        }\n",
    "        \n",
    "        # Handle temporal filtering\n",
    "        start_dt = None\n",
    "        end_dt = None\n",
    "        \n",
    "        if temporal_presets.value == '7d':\n",
    "            end_dt = datetime(2024, 11, 29)  # Black Friday 2024\n",
    "            start_dt = end_dt - timedelta(days=7)\n",
    "        elif temporal_presets.value == '30d':\n",
    "            end_dt = datetime(2024, 11, 29)\n",
    "            start_dt = end_dt - timedelta(days=30)\n",
    "        elif temporal_presets.value == 'november':\n",
    "            start_dt = datetime(2024, 11, 1)\n",
    "            end_dt = datetime(2024, 11, 30)\n",
    "        elif temporal_presets.value == 'bf_week':\n",
    "            start_dt = datetime(2024, 11, 24)\n",
    "            end_dt = datetime(2024, 11, 30)\n",
    "        elif temporal_presets.value == 'december':\n",
    "            start_dt = datetime(2024, 12, 1)\n",
    "            end_dt = datetime(2024, 12, 31)\n",
    "        elif temporal_presets.value == 'q4':\n",
    "            start_dt = datetime(2024, 10, 1)\n",
    "            end_dt = datetime(2024, 12, 31)\n",
    "        elif temporal_presets.value == 'custom':\n",
    "            start_dt = datetime.combine(start_date.value, datetime.min.time())\n",
    "            end_dt = datetime.combine(end_date.value, datetime.max.time())\n",
    "        \n",
    "        # Run search\n",
    "        start_time = time.time()\n",
    "        results = widget_search_pipeline_enhanced(\n",
    "            query, \n",
    "            conn, \n",
    "            limit=result_limit.value,\n",
    "            persona_filter=persona,\n",
    "            severity_filter=severity,\n",
    "            weights=weights,\n",
    "            start_date=start_dt,\n",
    "            end_date=end_dt\n",
    "        )\n",
    "        elapsed = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Build HTML\n",
    "        html_parts = []\n",
    "        \n",
    "        # Header with temporal info\n",
    "        temporal_desc = temporal_presets.label if temporal_presets.value != 'custom' else f\"{start_date.value} to {end_date.value}\"\n",
    "        \n",
    "        html_parts.append(f\"\"\"\n",
    "        <div style=\"background-color: #e8f4f8; padding: 10px; border-radius: 5px; margin-bottom: 10px;\">\n",
    "            <h3>üîç Search Results</h3>\n",
    "            <table style=\"width: 100%;\">\n",
    "                <tr><td><b>Query:</b></td><td>{query}</td></tr>\n",
    "                <tr><td><b>Time Range:</b></td><td>{temporal_desc}</td></tr>\n",
    "                <tr><td><b>Weights:</b></td><td>Semantic: {semantic_weight.value:.0%}, Trigram: {trigram_weight.value:.0%}</td></tr>\n",
    "                <tr><td><b>Filters:</b></td><td>Team: {persona_filter.value}, Severity: {severity_filter.value}</td></tr>\n",
    "            </table>\n",
    "        </div>\n",
    "        <p style='color: green;'>‚úÖ Found {len(results)} results in {elapsed:.1f}ms</p>\n",
    "        \"\"\")\n",
    "        \n",
    "        if results:\n",
    "            # Results table\n",
    "            html_parts.append(\"\"\"\n",
    "            <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px;\">\n",
    "            <thead>\n",
    "                <tr style=\"background-color: #f0f0f0;\">\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">#</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Severity</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Team</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Date</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Relevance</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Methods</th>\n",
    "                    <th style=\"padding: 8px; text-align: left; border: 1px solid #ddd;\">Content</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "            \"\"\")\n",
    "            \n",
    "            for i, (doc_id, data, relevance) in enumerate(results, 1):\n",
    "                emoji = {'critical': 'üî¥', 'warning': 'üü°', 'info': 'üîµ'}.get(data['severity'], '‚ö™')\n",
    "                methods = ', '.join(data['found_by'])\n",
    "                content_preview = data['content'][:100] + '...' if len(data['content']) > 100 else data['content']\n",
    "                row_color = '#ffffff' if i % 2 == 0 else '#f9f9f9'\n",
    "                \n",
    "                # Format timestamp if available\n",
    "                date_str = 'N/A'\n",
    "                if 'timestamp' in data and data['timestamp']:\n",
    "                    try:\n",
    "                        if isinstance(data['timestamp'], str):\n",
    "                            dt = datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00'))\n",
    "                        else:\n",
    "                            dt = data['timestamp']\n",
    "                        date_str = dt.strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                html_parts.append(f\"\"\"\n",
    "                <tr style=\"background-color: {row_color};\">\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd;\">{i}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd;\">{emoji} {data['severity']}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd;\">{data['persona']}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd; font-size: 0.9em;\">{date_str}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd;\">{relevance:.3f}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd; font-size: 0.9em;\">{methods}</td>\n",
    "                    <td style=\"padding: 8px; border: 1px solid #ddd; font-size: 0.9em;\">{content_preview}</td>\n",
    "                </tr>\n",
    "                \"\"\")\n",
    "            \n",
    "            html_parts.append(\"</tbody></table>\")\n",
    "            \n",
    "            # Enhanced score interpretation\n",
    "            html_parts.append(f\"\"\"\n",
    "            <div style=\"background-color: #e6f3ff; padding: 15px; border-radius: 5px; margin-top: 15px; border-left: 4px solid #1976d2;\">\n",
    "                <h4>üìä Understanding Your Relevance Scores</h4>\n",
    "                \n",
    "                <p><b>Score Interpretation Guide:</b></p>\n",
    "                <ul style=\"margin-bottom: 10px;\">\n",
    "                    <li><b>0.8-1.0 (Exact Match):</b> Nearly identical incidents - Study these first for direct pattern matches</li>\n",
    "                    <li><b>0.6-0.8 (High Relevance):</b> Same root cause described differently by teams - Critical for cross-team insights</li>\n",
    "                    <li><b>0.4-0.6 (Moderate Relevance):</b> Related concepts that may reveal cascade effects or dependencies</li>\n",
    "                    <li><b>0.2-0.4 (Low Relevance):</b> Peripheral matches - Often contain common keywords but different context. \n",
    "                        These are still valuable for understanding the broader incident landscape and may reveal unexpected correlations</li>\n",
    "                    <li><b>< 0.2 (Minimal Relevance):</b> Weak connection - May share only basic terms or be conceptually distant</li>\n",
    "                </ul>\n",
    "                \n",
    "                <div style=\"background-color: #fff3e0; padding: 10px; border-radius: 5px; margin: 10px 0;\">\n",
    "                    <p><b>üí° Why Lower Scores Matter:</b></p>\n",
    "                    <p>In your results, scores around 0.2-0.3 for \"memory exhaustion\" incidents indicate these are conceptually \n",
    "                    related but not direct matches to your query. They're valuable because:</p>\n",
    "                    <ul>\n",
    "                        <li>They show different failure modes that occurred during similar conditions</li>\n",
    "                        <li>Memory issues often precede or follow connection pool problems</li>\n",
    "                        <li>Understanding these correlations helps build comprehensive runbooks</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                \n",
    "                <p><b>Methods Column Insights:</b></p>\n",
    "                <ul>\n",
    "                    <li><b>\"trigram, semantic\":</b> Found by BOTH methods = highest confidence match</li>\n",
    "                    <li><b>\"trigram\" only:</b> Exact keyword/error code match - precise but may miss related issues</li>\n",
    "                    <li><b>\"semantic\" only:</b> Conceptually similar - reveals different terminology for same problem</li>\n",
    "                </ul>\n",
    "                \n",
    "                <p><b>Temporal Context:</b></p>\n",
    "                <p>Your selected time range <b>({temporal_desc})</b> helps identify:</p>\n",
    "                <ul>\n",
    "                    <li>Seasonal patterns specific to peak events</li>\n",
    "                    <li>Recurring issues that appear during high load</li>\n",
    "                    <li>Evolution of problems over time</li>\n",
    "                </ul>\n",
    "                \n",
    "                <p style=\"margin-top: 10px; font-weight: bold; color: #1976d2;\">\n",
    "                üéØ Action: Export these results grouped by score ranges to build a comprehensive incident response matrix!\n",
    "                </p>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            html_parts.append(\"\"\"\n",
    "            <div style=\"background-color: #fff0f0; padding: 10px; border-radius: 5px;\">\n",
    "                <p>‚ùå No results found. Try:</p>\n",
    "                <ul>\n",
    "                    <li>Expanding the time range</li>\n",
    "                    <li>Adjusting filter criteria</li>\n",
    "                    <li>Modifying search weights</li>\n",
    "                    <li>Using broader search terms</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \"\"\")\n",
    "        \n",
    "        # Display complete HTML\n",
    "        complete_html = ''.join(html_parts)\n",
    "        with output:\n",
    "            display(widgets.HTML(complete_html))\n",
    "            \n",
    "    except Exception as e:\n",
    "        with output:\n",
    "            display(widgets.HTML(f\"\"\"\n",
    "            <div style=\"background-color: #fff0f0; padding: 10px; border-radius: 5px;\">\n",
    "                <p style='color: red;'>‚ùå Error: {str(e)[:200]}</p>\n",
    "            </div>\n",
    "            \"\"\"))\n",
    "    finally:\n",
    "        time.sleep(0.1)\n",
    "        _search_lock = False\n",
    "\n",
    "# Attach handler\n",
    "if hasattr(search_button, '_click_handlers'):\n",
    "    search_button._click_handlers.callbacks.clear()\n",
    "search_button.on_click(run_search)\n",
    "\n",
    "# Layout\n",
    "query_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>üîç Search Query</h4>\"),\n",
    "    widgets.HBox([query_input, sample_queries])\n",
    "])\n",
    "\n",
    "filter_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>üîß Filters & Time Range</h4>\"),\n",
    "    widgets.HBox([persona_filter, severity_filter]),\n",
    "    widgets.HBox([temporal_presets, result_limit]),\n",
    "    custom_dates_box\n",
    "])\n",
    "\n",
    "weight_section = widgets.VBox([\n",
    "    widgets.HTML(\"<h4>‚öñÔ∏è Search Method Weights</h4>\"),\n",
    "    widgets.HBox([semantic_weight, trigram_weight]),\n",
    "    widgets.HTML(\"<p style='font-size: 0.9em; color: #666;'>Adjust to prioritize semantic understanding vs exact/typo matching</p>\")\n",
    "])\n",
    "\n",
    "control_panel = widgets.VBox([\n",
    "    query_section,\n",
    "    filter_section,\n",
    "    weight_section,\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    search_button\n",
    "], layout={'padding': '10px', 'border': '1px solid #ddd', 'border-radius': '5px'})\n",
    "\n",
    "# Display everything\n",
    "display(control_panel)\n",
    "display(widgets.HTML(\"<br>\"))\n",
    "display(output)\n",
    "\n",
    "print(\"\\n‚úÖ Enhanced widget with temporal filtering loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Workshop Complete!\n",
    "\n",
    "### ‚úÖ What You've Built\n",
    "\n",
    "You've successfully implemented a production-ready hybrid search system with:\n",
    "\n",
    "- **Trigram Search** - Handles typos and exact matches\n",
    "- **Semantic Search** - Understands conceptual relationships\n",
    "- **Hybrid Fusion** - Combines both methods optimally\n",
    "- **ML Reranking** - Uses Cohere for relevance optimization\n",
    "- **Interactive Testing** - Widget-based query exploration\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **Trigram search** (pg_trgm) handles both typos AND exact matches - no need for separate FTS\n",
    "2. **Semantic search** (pgvector) finds conceptually related content\n",
    "3. **Reciprocal rank fusion** effectively combines different scoring methods\n",
    "4. **ML reranking** provides the final optimization for relevance\n",
    "5. **Aurora PostgreSQL** provides all the tools needed for production hybrid search\n",
    "\n",
    "### üìö Next Steps\n",
    "\n",
    "1. **Deploy to Production**\n",
    "   - Use Aurora read replicas for search workloads\n",
    "   - Implement caching for frequent queries\n",
    "   - Monitor query performance with Performance Insights\n",
    "\n",
    "2. **Optimize for Your Use Case**\n",
    "   - Fine-tune weights based on user feedback\n",
    "   - Add more metadata filters as needed\n",
    "   - Consider implementing query expansion\n",
    "\n",
    "3. **Scale for Black Friday**\n",
    "   - Test with production-scale data\n",
    "   - Implement connection pooling\n",
    "   - Set up alerting for search latency\n",
    "\n",
    "### üôè Thank You!\n",
    "\n",
    "Thank you for participating in **DAT409**! You're now ready to implement hybrid search for MCP retrieval and prevent Black Friday incidents.\n",
    "\n",
    "---\n",
    "\n",
    "**Built with ‚ù§Ô∏è by AWS Database Specialists for re:Invent 2025**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
