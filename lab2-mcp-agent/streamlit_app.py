"""
DAT409: Aurora PostgreSQL Hybrid Search with MCP
Enhanced Streamlit Application - 400 Level (UI ENHANCED VERSION)

NEW UI FEATURES:
- Animated gradient backgrounds with floating particles
- Skeleton loading states for better UX
- Enhanced product cards with smooth animations
- Search bar with auto-suggestions
- Animated metrics with count-up effects
- Better empty states with actionable suggestions
- Toast notifications for user actions
- Collapsible sidebar sections
- Quick view modals for products
- Enhanced result cards with expand/collapse
"""

import streamlit as st
import os
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import psycopg
from pgvector.psycopg import register_vector
import boto3
from dotenv import load_dotenv
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

# MCP and Strands imports
from mcp import stdio_client, StdioServerParameters
from strands import Agent
from strands.tools.mcp import MCPClient

# Load environment
load_dotenv()
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger("dat409_app")
logger.setLevel(logging.INFO)
logging.getLogger("awslabs.postgres_mcp_server.server").setLevel(logging.WARNING)

# ============================================================================
# PAGE CONFIGURATION
# ============================================================================

st.set_page_config(
    page_title="DAT409: Hybrid Search with MCP",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# ENHANCED DARK THEME STYLING
# ============================================================================

st.markdown("""
<style>
    /* Main background with animated gradient */
    .stApp {
        background: linear-gradient(135deg, #000000 0%, #0a0a0a 50%, #000000 100%);
        background-size: 200% 200%;
        animation: gradientShift 15s ease infinite;
        color: #E0E0E0;
    }
    
    @keyframes gradientShift {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    
    /* Floating particles effect */
    .stApp::before {
        content: "";
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background-image: 
            radial-gradient(circle at 20% 80%, rgba(102, 126, 234, 0.03) 0%, transparent 50%),
            radial-gradient(circle at 80% 20%, rgba(118, 75, 162, 0.03) 0%, transparent 50%),
            radial-gradient(circle at 40% 40%, rgba(0, 217, 255, 0.02) 0%, transparent 50%);
        pointer-events: none;
        z-index: 0;
    }
    
    /* Sidebar styling with glass effect */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, rgba(10, 10, 10, 0.95) 0%, rgba(26, 26, 26, 0.95) 100%);
        backdrop-filter: blur(10px);
        border-right: 1px solid rgba(102, 126, 234, 0.2);
    }
    
    /* Headers with glow effect */
    h1, h2, h3, h4, h5, h6 {
        color: #FFFFFF !important;
        font-weight: 600;
        text-shadow: 0 0 20px rgba(102, 126, 234, 0.3);
    }
    
    /* Animated Metric cards */
    [data-testid="stMetricValue"] {
        color: #00D9FF !important;
        font-size: 2rem !important;
        animation: metricPulse 2s ease-in-out infinite;
    }
    
    @keyframes metricPulse {
        0%, 100% { transform: scale(1); }
        50% { transform: scale(1.05); }
    }
    
    [data-testid="stMetricLabel"] {
        color: #B0B0B0 !important;
    }
    
    /* Enhanced Buttons with ripple effect */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 8px;
        padding: 0.6rem 1.5rem;
        font-weight: 600;
        transition: all 0.3s ease;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
        position: relative;
        overflow: hidden;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(102, 126, 234, 0.5);
    }
    
    .stButton > button:active {
        transform: translateY(0px);
    }
    
    /* Text inputs with glow on focus */
    .stTextInput > div > div > input,
    .stTextArea > div > div > textarea {
        background-color: #1a1a1a;
        border: 1px solid #333333;
        border-radius: 8px;
        color: #E0E0E0;
        padding: 0.75rem;
        transition: all 0.3s ease;
    }
    
    .stTextInput > div > div > input:focus,
    .stTextArea > div > div > textarea:focus {
        border-color: #667eea;
        box-shadow: 0 0 0 2px rgba(102, 126, 234, 0.2), 0 0 20px rgba(102, 126, 234, 0.1);
    }
    
    /* Select boxes with better styling */
    .stSelectbox > div > div {
        background-color: #1a1a1a;
        border: 1px solid #333333;
        border-radius: 8px;
    }
    
    /* Enhanced Tabs with icons */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        background-color: #0a0a0a;
        padding: 0.5rem;
        border-radius: 8px;
    }
    
    .stTabs [data-baseweb="tab"] {
        background-color: #1a1a1a;
        border: 1px solid #333333;
        border-radius: 6px;
        color: #B0B0B0;
        padding: 0.5rem 1rem;
        transition: all 0.3s ease;
    }
    
    .stTabs [data-baseweb="tab"]:hover {
        border-color: #667eea;
        transform: translateY(-2px);
    }
    
    .stTabs [aria-selected="true"] {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-color: #667eea;
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.3);
    }
    
    /* Enhanced Product cards with better animations */
    .product-card {
        background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
        border: 1px solid #333333;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 0.5rem 0;
        display: flex;
        gap: 1rem;
        align-items: start;
        transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
        position: relative;
        overflow: hidden;
        min-height: 150px;
    }
    
    .product-details {
        flex: 1;
        min-width: 0;
    }
    
    .product-card::before {
        content: "";
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(90deg, transparent, rgba(102, 126, 234, 0.1), transparent);
        transition: left 0.5s ease;
    }
    
    .product-card:hover::before {
        left: 100%;
    }
    
    .product-card:hover {
        border-color: #667eea;
        box-shadow: 0 8px 24px rgba(102, 126, 234, 0.2);
        transform: translateY(-4px) scale(1.01);
    }
    
    .product-image {
        width: 120px;
        height: 120px;
        object-fit: contain;
        border: 1px solid #333333;
        border-radius: 8px;
        padding: 0.5rem;
        background: #0a0a0a;
        transition: transform 0.3s ease;
        flex-shrink: 0;
        display: block;
    }
    
    .product-image img {
        width: 100%;
        height: 100%;
        object-fit: contain;
    }
    
    .product-card:hover .product-image {
        transform: scale(1.05) rotate(2deg);
    }
    
    .product-title {
        color: #00D9FF;
        font-weight: 500;
        font-size: 1rem;
        margin-bottom: 0.5rem;
        cursor: pointer;
        transition: color 0.3s ease;
    }
    
    .product-title:hover {
        color: #667eea;
        text-decoration: underline;
    }
    
    .product-price {
        color: #10b981;
        font-size: 1.25rem;
        font-weight: 600;
        margin: 0.5rem 0;
    }
    
    /* Enhanced method badges */
    .method-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 20px;
        font-size: 0.75rem;
        font-weight: 600;
        text-transform: uppercase;
        margin-right: 0.5rem;
        animation: badgeSlideIn 0.5s ease;
    }
    
    @keyframes badgeSlideIn {
        from {
            opacity: 0;
            transform: translateX(-20px);
        }
        to {
            opacity: 1;
            transform: translateX(0);
        }
    }
    
    .badge-keyword { 
        background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%); 
        color: white; 
        box-shadow: 0 2px 8px rgba(59, 130, 246, 0.3);
    }
    .badge-semantic { 
        background: linear-gradient(135deg, #10b981 0%, #059669 100%); 
        color: white; 
        box-shadow: 0 2px 8px rgba(16, 185, 129, 0.3);
    }
    .badge-fuzzy { 
        background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); 
        color: white; 
        box-shadow: 0 2px 8px rgba(245, 158, 11, 0.3);
    }
    .badge-hybrid { 
        background: linear-gradient(135deg, #8b5cf6 0%, #7c3aed 100%); 
        color: white; 
        box-shadow: 0 2px 8px rgba(139, 92, 246, 0.3);
    }
    
    /* Animated score bar */
    .score-bar {
        height: 8px;
        background: #2a2a2a;
        border-radius: 4px;
        overflow: hidden;
        margin: 0.5rem 0;
    }
    
    .score-fill {
        height: 100%;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        transition: width 1s cubic-bezier(0.4, 0, 0.2, 1);
        animation: scoreFill 1s ease-out;
    }
    
    @keyframes scoreFill {
        from { width: 0 !important; }
    }
    
    /* Enhanced persona card */
    .persona-card {
        background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
        border-left: 4px solid #667eea;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        transition: all 0.3s ease;
    }
    
    .persona-card:hover {
        transform: translateX(5px);
        box-shadow: 0 4px 12px rgba(102, 126, 234, 0.2);
    }
    
    /* Stats panel with glassmorphism */
    .stats-panel {
        background: linear-gradient(135deg, rgba(102, 126, 234, 0.9) 0%, rgba(118, 75, 162, 0.9) 100%);
        backdrop-filter: blur(10px);
        border-radius: 10px;
        padding: 1.5rem;
        color: white;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(102, 126, 234, 0.3);
        animation: statsSlideUp 0.5s ease-out;
    }
    
    @keyframes statsSlideUp {
        from {
            opacity: 0;
            transform: translateY(20px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    /* Skeleton loader for loading states */
    .skeleton {
        background: linear-gradient(90deg, #1a1a1a 25%, #2a2a2a 50%, #1a1a1a 75%);
        background-size: 200% 100%;
        animation: shimmer 1.5s infinite;
        border-radius: 8px;
    }
    
    @keyframes shimmer {
        0% { background-position: 200% 0; }
        100% { background-position: -200% 0; }
    }
    
    .skeleton-card {
        height: 150px;
        margin: 0.5rem 0;
    }
    
    .skeleton-text {
        height: 20px;
        margin: 0.5rem 0;
    }
    
    /* Toast notification */
    .toast {
        position: fixed;
        bottom: 20px;
        right: 20px;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem 1.5rem;
        border-radius: 8px;
        box-shadow: 0 4px 20px rgba(102, 126, 234, 0.4);
        animation: toastSlideIn 0.3s ease-out;
        z-index: 1000;
    }
    
    @keyframes toastSlideIn {
        from {
            opacity: 0;
            transform: translateX(100px);
        }
        to {
            opacity: 1;
            transform: translateX(0);
        }
    }
    
    /* Empty state with better styling */
    .empty-state {
        text-align: center;
        padding: 3rem 2rem;
        background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
        border: 2px dashed #333333;
        border-radius: 12px;
        margin: 2rem 0;
    }
    
    .empty-state-icon {
        font-size: 4rem;
        margin-bottom: 1rem;
        opacity: 0.5;
    }
    
    /* Scrollbar styling */
    ::-webkit-scrollbar {
        width: 10px;
        height: 10px;
    }
    
    ::-webkit-scrollbar-track {
        background: #0a0a0a;
    }
    
    ::-webkit-scrollbar-thumb {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 5px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);
    }
    
    /* Divider with gradient */
    hr {
        border: none;
        height: 1px;
        background: linear-gradient(90deg, transparent, #667eea, transparent);
        margin: 2rem 0;
    }
    
    /* Expander with better animation */
    .streamlit-expanderHeader {
        background-color: #1a1a1a;
        border: 1px solid #333333;
        border-radius: 8px;
        color: #E0E0E0;
        transition: all 0.3s ease;
    }
    
    .streamlit-expanderHeader:hover {
        border-color: #667eea;
        background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);
    }
    
    /* Alert boxes with icons */
    .stAlert {
        background-color: #1a1a1a;
        border: 1px solid #333333;
        border-radius: 8px;
        padding: 1rem;
        animation: alertSlideIn 0.3s ease-out;
    }
    
    @keyframes alertSlideIn {
        from {
            opacity: 0;
            transform: translateY(-10px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
</style>
""", unsafe_allow_html=True)

# ============================================================================
# CONFIGURATION & CONSTANTS (Same as original)
# ============================================================================

# Database configuration
DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'port': os.getenv('DB_PORT', '5432'),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'dbname': os.getenv('DB_NAME', 'workshop_db')
}

# MCP configuration
MCP_CONFIG = {
    'cluster_arn': os.getenv('DATABASE_CLUSTER_ARN'),
    'secret_arn': os.getenv('DATABASE_SECRET_ARN'),
    'database': os.getenv('DB_NAME', 'workshop_db'),
    'region': os.getenv('AWS_REGION', 'us-west-2')
}

AWS_REGION = os.getenv('AWS_REGION', 'us-west-2')

# Persona definitions
PERSONAS = {
    'customer': {
        'icon': 'üë§',
        'name': 'Customer',
        'description': 'Public content only',
        'access_levels': ['product_faq'],
        'color': '#3b82f6',
        'db_user': 'customer_user',
        'db_password': 'customer123'
    },
    'support_agent': {
        'icon': 'üéß',
        'name': 'Support Agent',
        'description': 'Public + Internal content',
        'access_levels': ['product_faq', 'support_ticket', 'internal_note'],
        'color': '#10b981',
        'db_user': 'agent_user',
        'db_password': 'agent123'
    },
    'product_manager': {
        'icon': 'üëî',
        'name': 'Product Manager',
        'description': 'Full access including analytics',
        'access_levels': ['product_faq', 'support_ticket', 'internal_note', 'analytics'],
        'color': '#8b5cf6',
        'db_user': 'pm_user',
        'db_password': 'pm123'
    }
}

SAMPLE_QUERIES = {
    'customer': [
        'How do I set up my security camera?',
        'Troubleshooting bluetooth headphones',
        'Best robot vacuum for pet hair',
        'Smart doorbell features'
    ],
    'support_agent': [
        'Recent customer complaints about camera',
        'Common issues with vacuum cleaners',
        'Return requests for electronics',
        'Product defect patterns'
    ],
    'product_manager': [
        'Sales performance by category',
        'Customer satisfaction trends',
        'Product line profitability analysis',
        'Market segment analysis'
    ]
}

# ============================================================================
# HELPER FUNCTIONS FOR ALL ORIGINAL FUNCTIONALITY
# (Copy all the functions from original: get_bedrock_client, get_mcp_client, 
# get_db_connection, generate_embedding, all search functions, etc.)
# ============================================================================

@st.cache_resource
def get_bedrock_client():
    """Initialize Bedrock runtime client"""
    return boto3.client('bedrock-runtime', region_name=AWS_REGION)

bedrock_runtime = get_bedrock_client()

@st.cache_resource(ttl=60)
def get_mcp_client():
    """Initialize MCP client for Aurora PostgreSQL"""
    if not MCP_CONFIG['cluster_arn'] or not MCP_CONFIG['secret_arn']:
        logger.warning("MCP configuration incomplete.")
        return None
    
    import subprocess
    try:
        subprocess.run(
            ["uv", "tool", "install", "awslabs.postgres-mcp-server"],
            capture_output=True,
            timeout=30
        )
    except Exception as e:
        logger.warning(f"Could not pre-install MCP server: {e}")
    
    return MCPClient(lambda: stdio_client(
        StdioServerParameters(
            command="uv",
            args=[
                "tool", "run", "awslabs.postgres-mcp-server",
                "--resource_arn", MCP_CONFIG['cluster_arn'],
                "--secret_arn", MCP_CONFIG['secret_arn'],
                "--database", MCP_CONFIG['database'],
                "--region", MCP_CONFIG['region'],
                "--readonly", "True"
            ],
            env={
                "AWS_REGION": MCP_CONFIG['region'],
                "LOGURU_LEVEL": "SUCCESS",
                "LOGURU_FORMAT": "<level>{message}</level>",
                "POSTGRES_DEFAULT_SCHEMA": "bedrock_integration",
                "PYTHONUNBUFFERED": "1",
                "COLUMNS": "200"
            }
        )
    ))

def get_db_connection(persona: str = None):
    """Get database connection with optional persona-based credentials"""
    if persona and persona in PERSONAS:
        config = {
            **DB_CONFIG,
            'user': PERSONAS[persona]['db_user'],
            'password': PERSONAS[persona]['db_password']
        }
    else:
        config = DB_CONFIG
    
    conn = psycopg.connect(**config, autocommit=True)
    register_vector(conn)
    return conn

def generate_embedding(text: str, input_type: str = "search_query") -> Optional[List[float]]:
    """Generate Cohere embeddings via Bedrock"""
    if not text:
        return None
    
    try:
        body = json.dumps({
            "texts": [text],
            "input_type": input_type,
            "embedding_types": ["float"],
            "truncate": "END"
        })
        
        response = bedrock_runtime.invoke_model(
            modelId='cohere.embed-english-v3',
            body=body,
            accept='application/json',
            contentType='application/json'
        )
        
        response_body = json.loads(response['body'].read())
        if 'embeddings' in response_body and 'float' in response_body['embeddings']:
            return response_body['embeddings']['float'][0]
        elif 'embeddings' in response_body:
            return response_body['embeddings'][0]
    except Exception as e:
        logger.error(f"Embedding generation failed: {e}")
    
    return None

def keyword_search(query: str, limit: int = 10, persona: str = None) -> List[Dict]:
    """PostgreSQL Full-Text Search"""
    conn = get_db_connection(persona)
    
    try:
        results = conn.execute("""
            SELECT 
                p."productId",
                p.product_description,
                p.category_name,
                p.price,
                p.stars,
                p.reviews,
                p.imgurl,
                p.producturl,
                ts_rank_cd(
                    to_tsvector('english', p.product_description), 
                    plainto_tsquery('english', %s)
                ) as rank
            FROM bedrock_integration.product_catalog p
            WHERE to_tsvector('english', p.product_description) 
                  @@ plainto_tsquery('english', %s)
            ORDER BY rank DESC
            LIMIT %s;
        """, (query, query, limit)).fetchall()
        
        return [{
            'productId': r[0],
            'description': r[1][:200] + '...' if len(r[1]) > 200 else r[1],
            'category': r[2],
            'price': float(r[3]) if r[3] else 0,
            'stars': float(r[4]) if r[4] else 0,
            'reviews': int(r[5]) if r[5] else 0,
            'imgUrl': r[6],
            'productUrl': r[7],
            'score': float(r[8]) if r[8] else 0,
            'method': 'Keyword'
        } for r in results]
    finally:
        conn.close()

def fuzzy_search(query: str, limit: int = 10, persona: str = None) -> List[Dict]:
    """PostgreSQL Trigram Search"""
    conn = get_db_connection(persona)
    
    try:
        conn.execute("SET pg_trgm.similarity_threshold = 0.1;")
        
        results = conn.execute("""
            SELECT 
                "productId",
                product_description,
                category_name,
                price,
                stars,
                reviews,
                imgurl,
                producturl,
                similarity(lower(product_description), lower(%s)) as sim
            FROM bedrock_integration.product_catalog
            WHERE lower(product_description) %% lower(%s)
            ORDER BY sim DESC
            LIMIT %s;
        """, (query, query, limit)).fetchall()
        
        return [{
            'productId': r[0],
            'description': r[1][:200] + '...' if len(r[1]) > 200 else r[1],
            'category': r[2],
            'price': float(r[3]) if r[3] else 0,
            'stars': float(r[4]) if r[4] else 0,
            'reviews': int(r[5]) if r[5] else 0,
            'imgUrl': r[6],
            'productUrl': r[7],
            'score': float(r[8]) if r[8] else 0,
            'method': 'Fuzzy'
        } for r in results]
    finally:
        conn.close()

def semantic_search(query: str, limit: int = 10, persona: str = None) -> List[Dict]:
    """Semantic Search using Cohere embeddings"""
    query_embedding = generate_embedding(query, "search_query")
    if not query_embedding:
        return []
    
    conn = get_db_connection(persona)
    
    try:
        results = conn.execute("""
            SELECT 
                "productId",
                product_description,
                category_name,
                price,
                stars,
                reviews,
                imgurl,
                producturl,
                1 - (embedding <=> %s::vector) as similarity
            FROM bedrock_integration.product_catalog
            WHERE embedding IS NOT NULL
            ORDER BY embedding <=> %s::vector
            LIMIT %s;
        """, (query_embedding, query_embedding, limit)).fetchall()
        
        return [{
            'productId': r[0],
            'description': r[1][:200] + '...' if len(r[1]) > 200 else r[1],
            'category': r[2],
            'price': float(r[3]) if r[3] else 0,
            'stars': float(r[4]) if r[4] else 0,
            'reviews': int(r[5]) if r[5] else 0,
            'imgUrl': r[6],
            'productUrl': r[7],
            'score': float(r[8]) if r[8] else 0,
            'method': 'Semantic'
        } for r in results]
    finally:
        conn.close()

def hybrid_search(
    query: str,
    semantic_weight: float = 0.7,
    keyword_weight: float = 0.3,
    limit: int = 10,
    persona: str = None
) -> List[Dict]:
    """Hybrid Search combining semantic and keyword"""
    total = semantic_weight + keyword_weight
    semantic_weight = semantic_weight / total
    keyword_weight = keyword_weight / total
    
    semantic_results = semantic_search(query, limit * 2, persona)
    keyword_results = keyword_search(query, limit * 2, persona)
    
    product_scores = {}
    product_data = {}
    
    for result in semantic_results:
        pid = result['productId']
        product_scores[pid] = result['score'] * semantic_weight
        product_data[pid] = result
    
    for result in keyword_results:
        pid = result['productId']
        if pid in product_scores:
            product_scores[pid] += result['score'] * keyword_weight
        else:
            product_scores[pid] = result['score'] * keyword_weight
            product_data[pid] = result
    
    sorted_products = sorted(product_scores.items(), key=lambda x: x[1], reverse=True)[:limit]
    
    results = []
    for pid, score in sorted_products:
        product = product_data[pid].copy()
        product['score'] = score
        product['method'] = 'Hybrid'
        results.append(product)
    
    return results

def search_with_mcp_context(
    query: str,
    persona: str,
    time_window: Optional[str] = None,
    limit: int = 10
) -> List[Dict]:
    """Search with MCP context and RLS policies"""
    conn = get_db_connection(persona)
    
    try:
        query_embedding = generate_embedding(query, "search_query")
        
        time_filter = ""
        if time_window:
            if time_window == "24h":
                time_filter = "AND k.created_at >= NOW() - INTERVAL '24 hours'"
            elif time_window == "7d":
                time_filter = "AND k.created_at >= NOW() - INTERVAL '7 days'"
            elif time_window == "30d":
                time_filter = "AND k.created_at >= NOW() - INTERVAL '30 days'"
        
        if query_embedding:
            results = conn.execute(f"""
                SELECT 
                    k.id,
                    k.content,
                    k.content_type,
                    k.severity,
                    k.created_at,
                    k.product_id,
                    p.product_description,
                    p.price,
                    p.stars,
                    p.reviews,
                    p.imgurl,
                    CASE 
                        WHEN p.embedding IS NOT NULL THEN
                            1 - (p.embedding <=> %s::vector)
                        ELSE 0.5
                    END as semantic_score,
                    ts_rank(to_tsvector('english', k.content), plainto_tsquery('english', %s)) as text_score
                FROM bedrock_integration.knowledge_base k
                LEFT JOIN bedrock_integration.product_catalog p ON k.product_id = p."productId"
                WHERE (
                    k.content ILIKE %s
                    OR p.product_description ILIKE %s
                )
                {time_filter}
                ORDER BY semantic_score DESC, text_score DESC
                LIMIT %s;
            """, (query_embedding, query, f'%{query}%', f'%{query}%', limit)).fetchall()
        else:
            results = conn.execute(f"""
                SELECT 
                    k.id,
                    k.content,
                    k.content_type,
                    k.severity,
                    k.created_at,
                    k.product_id,
                    p.product_description,
                    p.price,
                    p.stars,
                    p.reviews,
                    p.imgurl,
                    0.5 as semantic_score,
                    ts_rank(to_tsvector('english', k.content), plainto_tsquery('english', %s)) as text_score
                FROM bedrock_integration.knowledge_base k
                LEFT JOIN bedrock_integration.product_catalog p ON k.product_id = p."productId"
                WHERE (
                    k.content ILIKE %s
                    OR p.product_description ILIKE %s
                )
                {time_filter}
                ORDER BY text_score DESC
                LIMIT %s;
            """, (query, f'%{query}%', f'%{query}%', limit)).fetchall()
        
        return [{
            'id': r[0],
            'content': r[1],
            'content_type': r[2],
            'severity': r[3],
            'created_at': r[4].isoformat() if r[4] else None,
            'product_id': r[5],
            'product_description': r[6],
            'price': float(r[7]) if r[7] else 0,
            'stars': float(r[8]) if r[8] else 0,
            'reviews': int(r[9]) if r[9] else 0,
            'imgUrl': r[10],
            'semantic_score': float(r[11]) if r[11] else 0,
            'text_score': float(r[12]) if r[12] else 0,
            'combined_score': (float(r[11]) if r[11] else 0) * 0.7 + (float(r[12]) if r[12] else 0) * 0.3
        } for r in results]
    finally:
        conn.close()

def strands_agent_search(
    query: str,
    persona: str = None,
    use_mcp: bool = True
) -> Dict[str, Any]:
    """Use Strands Agent with MCP tools"""
    if not use_mcp:
        return {
            'response': 'MCP not available. Using direct search.',
            'method': 'direct',
            'error': 'MCP client not configured'
        }
    
    mcp_client = get_mcp_client()
    
    if not mcp_client:
        return {
            'response': 'MCP client not configured.',
            'method': 'error',
            'error': 'Missing MCP configuration'
        }
    
    try:
        try:
            mcp_client.start()
        except:
            pass
        
        tools = mcp_client.list_tools_sync()
            
        agent = Agent(
            tools=tools,
            model="us.anthropic.claude-sonnet-4-20250514-v1:0",
            system_prompt=f"""You are a helpful database assistant with access to Aurora PostgreSQL through MCP tools.

IMPORTANT SCHEMA:
- Main: bedrock_integration.product_catalog ("productId", product_description, category_name, price, stars, reviews, imgurl, embedding)
- Knowledge: bedrock_integration.knowledge_base (id, product_id, content, content_type, persona_access VARCHAR[], severity, created_at)

Current persona: {persona} (simulating {PERSONAS[persona]['db_user']})

SECURITY - CRITICAL: Always filter knowledge_base queries with:
WHERE '{persona}' = ANY(persona_access) OR persona_access IS NULL

Access levels for {persona}:
{', '.join(PERSONAS[persona]['access_levels'])}

NOTE: persona_access is a PostgreSQL array. Use ARRAY syntax for comparisons.

Provide clear responses based on filtered query results."""
        )
        
        start_time = time.time()
        response = agent(query)
        elapsed = time.time() - start_time
        
        # Extract response text
        if hasattr(response, 'message') and isinstance(response.message, dict):
            content = response.message.get('content', [])
            response_text = content[0].get('text', str(content)) if content else str(response)
        else:
            response_text = str(response)
        
        tools_used = []
        
        # Try multiple ways to extract tool calls
        # Method 1: Check response.tool_calls
        if hasattr(response, 'tool_calls') and response.tool_calls:
            for tool in response.tool_calls:
                tool_name = getattr(tool, 'name', str(tool))
                tools_used.append(tool_name)
                

        
        # Method 2: Check response.message for tool_use blocks
        if hasattr(response, 'message') and isinstance(response.message, dict):
            content = response.message.get('content', [])
            for block in content:
                if isinstance(block, dict):
                    if block.get('type') == 'tool_use':
                        tool_name = block.get('name', 'unknown')
                        tools_used.append(tool_name)
        
        # Method 3: Check response.state (it's a dict)
        if hasattr(response, 'state') and isinstance(response.state, dict):
            # Check for messages in state
            if 'messages' in response.state:
                messages = response.state['messages']
                for msg in messages:
                    if isinstance(msg, dict) and 'content' in msg:
                        content = msg['content']
                        if isinstance(content, list):
                            for block in content:
                                if isinstance(block, dict) and block.get('type') == 'tool_use':
                                    tool_name = block.get('name', 'unknown')
                                    tools_used.append(tool_name)

        
        available_tool_names = []
        if tools:
            for tool in tools:
                if isinstance(tool, str):
                    available_tool_names.append(tool)
                elif hasattr(tool, 'mcp_tool') and hasattr(tool.mcp_tool, 'name'):
                    available_tool_names.append(tool.mcp_tool.name)
                elif hasattr(tool, 'name'):
                    available_tool_names.append(tool.name)
        
        tools_used = list(dict.fromkeys(tools_used))
        
        return {
            'response': response_text,
            'method': 'strands_mcp',
            'elapsed_time': elapsed,
            'available_tools': available_tool_names,
            'error': None
        }
            
    except Exception as e:
        logger.error(f"Strands Agent error: {e}")
        return {
            'response': f"Agent execution failed: {str(e)}",
            'method': 'error',
            'error': str(e)
        }

def rerank_results(query: str, results: List[Dict], top_k: int = 5) -> List[Dict]:
    """Re-rank search results using Cohere"""
    if not results:
        return []
    
    try:
        documents = [r.get('description', r.get('content', '')) for r in results]
        
        body = json.dumps({
            "api_version": 2,
            "query": query,
            "documents": documents,
            "top_n": min(top_k, len(documents))
        })
        
        response = bedrock_runtime.invoke_model(
            modelId='cohere.rerank-v3-5:0',
            body=body,
            accept='application/json',
            contentType='application/json'
        )
        
        response_body = json.loads(response['body'].read())
        
        reranked = []
        for item in response_body.get('results', []):
            idx = item['index']
            result = results[idx].copy()
            result['rerank_score'] = item['relevance_score']
            reranked.append(result)
        
        return reranked
    except Exception as e:
        logger.error(f"Reranking failed: {e}")
        return results[:top_k]

# ============================================================================
# ENHANCED UI COMPONENTS
# ============================================================================

def render_skeleton_card():
    """Render a skeleton loading card"""
    st.markdown("""
    <div class="skeleton skeleton-card"></div>
    """, unsafe_allow_html=True)

def render_product_card(product: Dict, show_score: bool = True):
    """Render an enhanced product card with animations"""
    method = product.get('method', 'Unknown')
    badge_class = f"badge-{method.lower()}"
    
    score = product.get('rerank_score', product.get('score', 0))
    score_percent = min(score * 100, 100)
    
    product_url = product.get('productUrl', '')
    img_url = product.get('imgUrl', '')
    description = product.get('description', 'No description')
    
    # Make title and image clickable if URL exists
    if product_url:
        img_html = f'<a href="{product_url}" target="_blank"><img src="{img_url}" class="product-image" alt="Product" loading="lazy"></a>'
        title_html = f'<a href="{product_url}" target="_blank" class="product-title">{description}</a>'
    else:
        img_html = f'<img src="{img_url}" class="product-image" alt="Product" loading="lazy">'
        title_html = f'<div class="product-title">{description}</div>'
    
    st.markdown(f"""
    <div class="product-card">
        {img_html}
        <div class="product-details">
            <div>
                <span class="method-badge {badge_class}">{method}</span>
                {f'<span style="color: #B0B0B0; font-size: 0.75rem;">Score: {score:.3f}</span>' if show_score else ''}
            </div>
            {title_html}
            <div class="product-price">${product.get('price', 0):.2f}</div>
            <div class="product-meta">
                ‚≠ê {product.get('stars', 0):.1f} | 
                {product.get('reviews', 0):,} reviews | 
                {product.get('category', 'Unknown')}
            </div>
            {f'<div class="score-bar"><div class="score-fill" style="width: {score_percent}%"></div></div>' if show_score else ''}
        </div>
    </div>
    """, unsafe_allow_html=True)

def render_knowledge_card(item: Dict, show_persona: bool = False):
    """Render a knowledge base card with enhanced styling"""
    content_type = item.get('content_type', 'unknown') or 'unknown'
    severity = item.get('severity', 'low') or 'low'
    
    severity_colors = {
        'low': '#10b981',
        'medium': '#f59e0b',
        'high': '#ef4444',
        'critical': '#dc2626'
    }
    
    severity_descriptions = {
        'low': '‚ÑπÔ∏è Informational - General FAQs and routine information',
        'medium': '‚ö†Ô∏è Moderate - Customer complaints requiring attention',
        'high': 'üö® Urgent - Product defects or warranty claims',
        'critical': 'üî• Critical - Widespread defects requiring immediate action'
    }
    
    content_type_display = content_type.replace('_', ' ').title()
    if content_type == 'product_faq':
        content_type_display = 'Product FAQ'
    elif content_type == 'support_ticket':
        content_type_display = 'Support Ticket'
    elif content_type == 'internal_note':
        content_type_display = 'Internal Note'
    
    col1, col2, col3 = st.columns([2, 2, 1])
    with col1:
        st.markdown(f"""
        <span class="method-badge" style="background: {severity_colors.get(severity, '#666')};">
            {(severity or 'low').upper()}
        </span>
        """, unsafe_allow_html=True)
    with col2:
        st.markdown(f'<span style="color: #B0B0B0; font-size: 0.875rem;">{content_type_display}</span>', unsafe_allow_html=True)
        st.caption(severity_descriptions.get(severity, ''))
    with col3:
        st.caption(item.get('created_at', 'N/A')[:10] if item.get('created_at') else 'N/A')
    
    st.markdown(f"**{item.get('content', 'No content')}**")
    
    if item.get('product_description'):
        st.markdown("---")
        st.caption("üîó Related Product")
        st.markdown(f"{item.get('product_description', 'N/A')[:100]}...")
        st.caption(f"${item.get('price', 0):.2f} ‚Ä¢ ‚≠ê {item.get('stars', 0):.1f} ‚Ä¢ {item.get('reviews', 0):,} reviews")
    
    st.markdown("<br>", unsafe_allow_html=True)

def show_empty_state(message: str, icon: str = "üîç"):
    """Show an enhanced empty state"""
    st.markdown(f"""
    <div class="empty-state">
        <div class="empty-state-icon">{icon}</div>
        <h3 style="color: #E0E0E0; margin-bottom: 1rem;">{message}</h3>
        <p style="color: #B0B0B0;">Try adjusting your search query or filters</p>
    </div>
    """, unsafe_allow_html=True)

# ============================================================================
# SESSION STATE
# ============================================================================

if 'search_history' not in st.session_state:
    st.session_state.search_history = []
if 'performance_metrics' not in st.session_state:
    st.session_state.performance_metrics = []

# ============================================================================
# MAIN APPLICATION
# ============================================================================

# Enhanced Header with gradient effect
st.markdown("""
<div style="text-align: center; padding: 2rem 0;">
    <h1 style="font-size: 3rem; margin-bottom: 0.5rem; color: #FFFFFF; font-weight: 600;">
        Hybrid Search with Aurora PostgreSQL for MCP Retrieval
    </h1>
    <p style="color: #00D9FF; font-size: 1.1rem; margin-top: 0.5rem; font-weight: 500;">
        ‚ö° Powering AI Agents with Enterprise-Grade Search
    </p>
    <p style="color: #B0B0B0; font-size: 1rem; margin-top: 0.5rem;">
        Aurora PostgreSQL ‚Ä¢ pgvector ‚Ä¢ Cohere ‚Ä¢ Model Context Protocol
    </p>
</div>
""", unsafe_allow_html=True)

# ============================================================================
# SIDEBAR WITH COLLAPSIBLE SECTIONS
# ============================================================================

with st.sidebar:
    st.markdown("## ‚öôÔ∏è Configuration")
    
    # Persona selection with enhanced styling
    st.markdown("### üë§ Persona (RLS)")
    st.caption("‚ö†Ô∏è Used in MCP Context Search (Tab 2) only")
    selected_persona = st.selectbox(
        "Select Role",
        options=list(PERSONAS.keys()),
        format_func=lambda x: f"{PERSONAS[x]['icon']} {PERSONAS[x]['name']}",
        key='persona'
    )
    
    persona_info = PERSONAS[selected_persona]
    st.markdown(f"""
    <div class="persona-card">
        <div style="font-size: 2rem; margin-bottom: 0.5rem;">{persona_info['icon']}</div>
        <div style="font-weight: 600; margin-bottom: 0.5rem;">{persona_info['name']}</div>
        <div style="color: #B0B0B0; font-size: 0.875rem; margin-bottom: 1rem;">
            {persona_info['description']}
        </div>
        <div style="font-size: 0.75rem; color: #B0B0B0;">
            <strong>Access Levels:</strong><br>
            {'<br>'.join([f"‚úÖ {level.replace('_', ' ').title().replace('Product Faq', 'Product FAQ')}" for level in persona_info['access_levels']])}
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Database status with enhanced visuals
    st.markdown("### üìä Database Status")
    
    if 'db_connected' not in st.session_state:
        st.session_state.db_connected = False
    
    try:
        conn = get_db_connection()
        
        result = conn.execute(
            "SELECT COUNT(*) FROM bedrock_integration.product_catalog"
        ).fetchone()
        product_count = result[0]
        
        result = conn.execute(
            "SELECT COUNT(*) FROM bedrock_integration.product_catalog WHERE embedding IS NOT NULL"
        ).fetchone()
        embedding_count = result[0]
        
        result = conn.execute(
            "SELECT COUNT(*) FROM bedrock_integration.knowledge_base"
        ).fetchone()
        kb_count = result[0]
        
        conn.close()
        st.session_state.db_connected = True
        
        st.success("‚úÖ Connected")
        
        # Show metrics with animation
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Products", f"{product_count:,}")
            st.metric("KB Items", f"{kb_count:,}")
        with col2:
            st.metric("Embeddings", f"{embedding_count:,}")
            st.metric("Status", "üü¢ Online")
        
    except Exception as e:
        st.session_state.db_connected = False
        st.error("‚ùå Connection Failed")
        if st.button("üîÑ Retry Connection", key="retry_db"):
            st.rerun()
    
    st.markdown("---")
    
    # Hybrid weights with visual feedback
    with st.expander("‚öñÔ∏è Hybrid Search Weights", expanded=False):
        semantic_weight = st.slider(
            "Semantic",
            min_value=0.0,
            max_value=1.0,
            value=0.7,
            step=0.1,
            key='semantic_weight'
        )
        
        keyword_weight = st.slider(
            "Keyword",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            key='keyword_weight'
        )
        
        total_weight = semantic_weight + keyword_weight
        if total_weight > 0:
            st.caption(f"üìä Normalized: {semantic_weight/total_weight:.1%} / {keyword_weight/total_weight:.1%}")
    
    # Search options in collapsible section
    with st.expander("üîß Search Options", expanded=False):
        results_limit = st.slider("Results per method", 1, 20, 5, key='results_limit')
        
        time_filter = st.selectbox(
            "üìÖ Time Window",
            options=['All Time', 'Last 24 Hours', 'Last 7 Days', 'Last 30 Days'],
            key='time_filter'
        )
    
    # Advanced Index Information (400-level)
    with st.expander("üîß Index Performance (Advanced)", expanded=False):
        st.caption("PostgreSQL index configuration and performance")
        
        try:
            conn = get_db_connection()
            
            # Get HNSW index info
            st.markdown("**Vector Index (HNSW):**")
            st.code("""CREATE INDEX ON product_catalog 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);""")
            st.caption("‚Ä¢ m=16: Max connections per layer (higher = better recall, more memory)")
            st.caption("‚Ä¢ ef_construction=64: Build-time search depth (higher = better quality)")
            
            st.markdown("**Full-Text Index (GIN):**")
            st.code("""CREATE INDEX ON product_catalog 
USING gin(to_tsvector('english', product_description));""")
            st.caption("‚Ä¢ GIN index for fast full-text search with stemming")
            
            st.markdown("**Trigram Index (GIN):**")
            st.code("""CREATE INDEX ON product_catalog 
USING gin(product_description gin_trgm_ops);""")
            st.caption("‚Ä¢ Trigram index for fuzzy matching (similarity threshold: 0.1)")
            
            conn.close()
        except Exception as e:
            st.caption("Index information unavailable")
    
    # Search History with better formatting
    if st.session_state.search_history:
        st.markdown("---")
        with st.expander("üïí Recent Searches", expanded=False):
            for i, search in enumerate(st.session_state.search_history[-5:][::-1]):
                if st.button(f"üîç {search['query'][:25]}...", key=f"history_{i}"):
                    st.session_state.quick_search = search['query']
                    st.rerun()

# ============================================================================
# MAIN TABS
# ============================================================================

tab1, tab2, tab3 = st.tabs([
    "üîç Search Comparison",
    "üéØ MCP Context Search",
    "üî¨ Advanced Analysis (OPTIONAL)"
])

# TAB 1: Enhanced Search Comparison
with tab1:
    st.markdown("### Compare Search Methods Side-by-Side")
    st.caption("üöÄ See how different search algorithms perform on the same query")
    
    st.markdown("---")
    
    # Quick action buttons with better layout
    st.markdown("**‚ö° Quick Try:**")
    quick_cols = st.columns(5)
    quick_queries = ["wireless headphones", "security camera", "robot vacuum", "smart doorbell", "laptop"]
    for idx, q in enumerate(quick_queries):
        with quick_cols[idx]:
            if st.button(f"üí° {q}", key=f"quick_{idx}"):
                st.session_state.quick_search = q
                st.rerun()
    
    st.markdown("---")
    
    # Options row
    col1, col2 = st.columns([3, 1])
    with col1:
        use_rerank = st.checkbox(
            "‚ú® Use Cohere Rerank",
            value=False,
            key='use_rerank',
            help="Apply Cohere's reranking model for better relevance"
        )
    with col2:
        show_all = st.checkbox("üìä Show All", value=False, key='show_all')
    
    # Initialize comparison_query if needed
    if 'comparison_query' not in st.session_state:
        st.session_state.comparison_query = ''
    
    # Handle quick search button clicks
    if 'quick_search' in st.session_state:
        st.session_state.comparison_query = st.session_state.quick_search
        del st.session_state.quick_search
    
    search_query = st.text_input(
        "Search Query",
        placeholder="Enter your search query (e.g., wireless headphones, security camera...)",
        key='comparison_query'
    )
    
    search_button = st.button("üîç Search All Methods", type="primary")
    
    with st.expander("üí° Understanding Search Scores", expanded=False):
        st.markdown("""
        **Why Semantic Search Often Has Higher Scores:**
        
        - **Semantic (0.0-1.0)**: Cosine similarity between embeddings - naturally produces scores closer to 1.0 for relevant matches
        - **Keyword (0.0-1.0)**: PostgreSQL ts_rank_cd scores - typically lower values even for good matches
        - **Fuzzy (0.0-1.0)**: Trigram similarity - requires very close character matches to score high
        - **Hybrid**: Weighted combination of Semantic + Keyword scores
        
        **Key Insight:** Higher scores don't always mean better results! Each method excels at different query types:
        - Use **Semantic** for conceptual/meaning-based searches
        - Use **Keyword** for exact term matching
        - Use **Fuzzy** for typo-tolerant searches
        - Use **Hybrid** for balanced results
        
        ---
        
        **üí° Understanding Hybrid Search Approaches:**
        
        **Challenge:** Different search methods produce vastly different score ranges (semantic: 0.7-1.0, keyword: 0.01-0.1), causing one method to dominate weighted combinations.
        
        **Solutions Demonstrated:**
        - ‚úÖ **Hybrid (70/30)** - Weighted score fusion (simple but requires tuning)
        - ‚úÖ **Hybrid-RRF** (Tab 3) - Rank-based fusion (robust, no normalization needed) ‚ú®
        - ‚úÖ **Cohere Rerank** (checkbox above) - ML-based re-ranking (most sophisticated)
        
        **Try it:** Enable Cohere Rerank or check out RRF in Tab 3!
        """)
    
    if search_button and search_query:
        # Create columns first
        cols = st.columns(4)
        
        # Perform actual search with spinner
        with st.spinner("üîç Searching across all methods..."):
            results_data = {}
            methods = [
                ('Keyword', lambda q: keyword_search(q, results_limit, selected_persona)),
                ('Fuzzy', lambda q: fuzzy_search(q, results_limit, selected_persona)),
                ('Semantic', lambda q: semantic_search(q, results_limit, selected_persona)),
                ('Hybrid', lambda q: hybrid_search(q, semantic_weight, keyword_weight, results_limit, selected_persona))
            ]
        
        for idx, (method_name, method_func) in enumerate(methods):
            with cols[idx]:
                st.markdown(f"#### {method_name}")
                
                start_time = time.time()
                try:
                    results = method_func(search_query)
                    elapsed = time.time() - start_time
                    
                    if use_rerank and results:
                        rerank_start = time.time()
                        results = rerank_results(search_query, results, len(results))
                        rerank_time = time.time() - rerank_start
                        total_time = elapsed + rerank_time
                        st.caption(f"‚è±Ô∏è {elapsed*1000:.0f}ms + {rerank_time*1000:.0f}ms rerank")
                    else:
                        total_time = elapsed
                        st.caption(f"‚è±Ô∏è {elapsed*1000:.0f}ms")
                    
                    if results:
                        st.caption(f"‚úÖ {len(results)} results")
                        
                        display_count = len(results) if show_all else 3
                        for result in results[:display_count]:
                            with st.container():
                                render_product_card(result, show_score=True)
                        
                        if f'results_{method_name}' not in st.session_state:
                            st.session_state[f'results_{method_name}'] = results
                    else:
                        show_empty_state("No results found", "üîç")
                    
                    results_data[method_name] = {
                        'count': len(results),
                        'time': total_time,
                        'avg_score': sum(r.get('score', 0) for r in results) / len(results) if results else 0
                    }
                    
                except Exception as e:
                    st.error(f"Error: {str(e)}")
        
        # Add to search history
        if search_query:
            st.session_state.search_history.append({
                'query': search_query,
                'timestamp': datetime.now().isoformat(),
                'persona': selected_persona
            })
            st.session_state.search_history = st.session_state.search_history[-10:]
        
        # Performance charts
        if results_data:
            st.markdown("---")
            st.markdown("### üìä Performance Metrics")
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_time = go.Figure(data=[
                    go.Bar(
                        x=list(results_data.keys()),
                        y=[v['time'] * 1000 for v in results_data.values()],
                        marker_color=['#3b82f6', '#f59e0b', '#10b981', '#8b5cf6'],
                        text=[f"{v['time']*1000:.0f}ms" for v in results_data.values()],
                        textposition='auto',
                    )
                ])
                fig_time.update_layout(
                    title="Response Time",
                    yaxis_title="Milliseconds",
                    paper_bgcolor='#0a0a0a',
                    plot_bgcolor='#1a1a1a',
                    font=dict(color='#E0E0E0'),
                    height=300
                )
                st.plotly_chart(fig_time, use_container_width=True)
            
            with col2:
                fig_score = go.Figure(data=[
                    go.Bar(
                        x=list(results_data.keys()),
                        y=[v['avg_score'] for v in results_data.values()],
                        marker_color=['#3b82f6', '#f59e0b', '#10b981', '#8b5cf6'],
                        text=[f"{v['avg_score']:.3f}" for v in results_data.values()],
                        textposition='auto',
                    )
                ])
                fig_score.update_layout(
                    title="Average Relevance Score",
                    yaxis_title="Score",
                    paper_bgcolor='#0a0a0a',
                    plot_bgcolor='#1a1a1a',
                    font=dict(color='#E0E0E0'),
                    height=300
                )
                st.plotly_chart(fig_score, use_container_width=True)

# TAB 2: MCP Context Search (Keep all original functionality)
with tab2:
    st.markdown("### MCP Context Search with RLS Policies")
    st.caption(f"Currently viewing as: **{PERSONAS[selected_persona]['name']}** {PERSONAS[selected_persona]['icon']}")
    
    with st.expander("üîí About Row-Level Security (RLS)", expanded=False):
        st.markdown("""
        **What is RLS?**  
        Row-Level Security in PostgreSQL automatically filters results based on your persona.
        
        **Implementation Approaches:**
        - üß† **With Strands Agent**: Application-level filtering via system prompt (WHERE '{persona}' = ANY(persona_access))
          - Agent uses admin access via MCP Data API
          - Security enforced through AI agent instructions
          - Standard pattern for AI agents with database access
        - üîí **Without Strands Agent**: Database-level RLS with persona-specific users
          - Traditional PostgreSQL RLS policies
          - Enforced at database connection level
        
        **Why Application-Level for MCP?**
        - Data API uses IAM authentication (not database users)
        - MCP server connects as single admin user
        - AI agent intelligently applies filtering based on persona context
        """)
    
    with st.expander("üîç Search Strategy (Direct Search)", expanded=False):
        st.markdown("""
        **When NOT using Strands Agent, the search uses:**
        
        1. **Hybrid Search Approach**:
           - Semantic search (70%): Vector similarity using Cohere embeddings
           - Keyword search (30%): PostgreSQL full-text search with ts_rank
        
        2. **Multi-Table Query**:
           - Searches `knowledge_base` table (FAQs, tickets, notes, analytics)
           - Joins with `product_catalog` for related product information
        
        3. **RLS Filtering**:
           - Automatically filters results based on your selected persona
           - Uses persona-specific database credentials
        
        4. **Time Window** (if selected):
           - Filters results by `created_at` timestamp
           - Options: 24 hours, 7 days, 30 days, or all time
        
        **Note:** This demonstrates traditional database access patterns with RLS enforcement.
        """)
    
    # Quick queries
    st.markdown("**‚ö° Quick Try:**")
    mcp_quick_queries_by_persona = {
        'customer': [
            ("warranty", "‚úÖ FAQ"),
            ("return policy", "‚úÖ FAQ"),
            ("headphones", "‚úÖ FAQ"),
            ("setup guide", "‚úÖ FAQ"),
            ("support ticket", "üîí Restricted")
        ],
        'support_agent': [
            ("connectivity", "‚úÖ Tickets"),
            ("firmware", "‚úÖ Tickets"),
            ("maintenance", "‚úÖ Internal"),
            ("defect", "‚úÖ Tickets"),
            ("analytics", "üîí Restricted")
        ],
        'product_manager': [
            ("growth", "‚úÖ Analytics"),
            ("sales", "‚úÖ Analytics"),
            ("product launch", "‚úÖ Internal"),
            ("warranty", "‚úÖ All"),
            ("revenue", "‚úÖ Analytics")
        ]
    }
    
    mcp_quick_queries = mcp_quick_queries_by_persona.get(selected_persona, [])
    mcp_quick_cols = st.columns(5)
    for idx, (q, status) in enumerate(mcp_quick_queries):
        with mcp_quick_cols[idx]:
            is_restricted = "üîí" in status
            if st.button(f"{'üîí' if is_restricted else 'üí°'} {q}", key=f"mcp_quick_{idx}", type="secondary" if is_restricted else "primary"):
                st.session_state.mcp_quick_search = q
                st.rerun()
    
    st.markdown("---")
    
    use_strands_agent = st.checkbox(
        "üß† Use Strands Agent with MCP Tools",
        value=False,
        help="AI agent with MCP tools for intelligent database querying"
    )
    
    if use_strands_agent:
        st.caption("üí° MCP Agent uses admin access (via Data API) with application-level security filtering enforced through system prompt. This is the standard production pattern for AI agents - security is maintained through intelligent query construction rather than database-level RLS.")
    
    time_window_map = {
        'All Time': None,
        'Last 24 Hours': '24h',
        'Last 7 Days': '7d',
        'Last 30 Days': '30d'
    }
    
    # Initialize mcp_query if needed
    if 'mcp_query' not in st.session_state:
        st.session_state.mcp_query = ''
    
    # Handle quick search button clicks
    if 'mcp_quick_search' in st.session_state:
        st.session_state.mcp_query = st.session_state.mcp_quick_search
        del st.session_state.mcp_quick_search
    
    mcp_query = st.text_input(
        "Search Query",
        placeholder="Enter your search query or question...",
        key='mcp_query'
    )
    
    mcp_search_button = st.button("üîç Search MCP Context", type="primary")
    
    if mcp_search_button and mcp_query:
        if use_strands_agent:
            st.markdown("#### üß† Strands Agent Response")
            
            with st.spinner("Agent is thinking..."):
                try:
                    start_time = time.time()
                    agent_result = strands_agent_search(mcp_query, selected_persona, use_mcp=True)
                    elapsed = time.time() - start_time
                    
                    if agent_result['error']:
                        st.error(f"‚ùå {agent_result['error']}")
                    else:
                        # Enhanced stats panel
                        st.markdown(f"""
                        <div class="stats-panel">
                            <div style="display: flex; justify-content: space-between; flex-wrap: wrap; gap: 1rem;">
                                <div>
                                    <div style="font-size: 1.5rem; font-weight: 600;">üß† Strands Agent</div>
                                    <div style="font-size: 0.875rem; opacity: 0.9;">Claude Sonnet 4 + MCP</div>
                                </div>
                                <div>
                                    <div style="font-size: 1.5rem; font-weight: 600;">{elapsed*1000:.0f}ms</div>
                                    <div style="font-size: 0.875rem; opacity: 0.9;">Response Time</div>
                                </div>
                                <div>
                                    <div style="font-size: 1.5rem; font-weight: 600;">‚úÖ</div>
                                    <div style="font-size: 0.875rem; opacity: 0.9;">Database Query</div>
                                </div>
                            </div>
                        </div>
                        """, unsafe_allow_html=True)
                        
                        if agent_result.get('available_tools'):
                            with st.expander("üîó MCP Tools Available", expanded=False):
                                for tool in agent_result['available_tools']:
                                    st.markdown(f"- `{tool}`")
                        
                        # Explain how the agent works
                        with st.expander("üîç How It Works", expanded=False):
                            st.markdown("""
                            **Agent Architecture:**
                            
                            1. üß† **Strands Agent** receives your natural language query
                            2. ü§ñ **Claude Sonnet 4** analyzes the query and decides which MCP tools to use
                            3. üîß **MCP Tools** execute SQL queries against Aurora PostgreSQL via Data API
                            4. üìä **Agent synthesizes** the database results into a natural language response
                            
                            **Note:** The Strands framework abstracts away tool call details, so SQL queries are not exposed in the response object. However, the agent successfully queries the database to provide accurate answers.
                            """)
                        
                        # Display response
                        st.markdown("**Response:**")
                        st.markdown(agent_result['response'])
                        
                        st.caption("üí° **Note:** This demo shows a single query response. The MCP architecture can be extended to support multi-turn conversations with chat history and follow-up questions (out of scope for this workshop).")
                        
                except Exception as e:
                    st.error(f"Agent error: {str(e)}")
        else:
            # Direct MCP search
            st.caption("üí° Using hybrid search (semantic + keyword) with RLS filtering. See 'Search Strategy' expander above for details.")
            with st.spinner(f"Searching as {selected_persona}..."):
                try:
                    start_time = time.time()
                    results = search_with_mcp_context(
                        mcp_query,
                        selected_persona,
                        time_window_map[time_filter],
                        results_limit * 2
                    )
                    elapsed = time.time() - start_time
                    
                    st.markdown(f"""
                    <div class="stats-panel">
                        <div style="display: flex; justify-content: space-between;">
                            <div>
                                <div style="font-size: 2rem; font-weight: 600;">{len(results)}</div>
                                <div style="font-size: 0.875rem;">Results Found</div>
                            </div>
                            <div>
                                <div style="font-size: 2rem; font-weight: 600;">{elapsed*1000:.0f}ms</div>
                                <div style="font-size: 0.875rem;">Response Time</div>
                            </div>
                            <div>
                                <div style="font-size: 2rem; font-weight: 600;">{PERSONAS[selected_persona]['icon']}</div>
                                <div style="font-size: 0.875rem;">{PERSONAS[selected_persona]['name']}</div>
                            </div>
                        </div>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if results:
                        by_type = {}
                        for r in results:
                            content_type = r.get('content_type', 'unknown')
                            if content_type not in by_type:
                                by_type[content_type] = []
                            by_type[content_type].append(r)
                        
                        for content_type, items in by_type.items():
                            with st.expander(f"üìÅ {content_type.replace('_', ' ').title()} ({len(items)})", expanded=True):
                                for item in items:
                                    render_knowledge_card(item, show_persona=True)
                    else:
                        show_empty_state(f"No results found for '{mcp_query}'", "üîç")
                        
                except Exception as e:
                    st.error(f"Search error: {str(e)}")

# TAB 3: Advanced Analysis
with tab3:
    st.markdown("### üî¨ Advanced Analysis & Optimization")
    st.caption("üéØ Deep dive into query analysis, result overlap, and index configuration")
    
    st.info("‚ÑπÔ∏è **Note:** This tab is optional and not required for completing the workshop labs. It provides additional technical insights for those interested in production optimization and algorithm internals.")
    
    st.markdown("---")
    
    # Section 0: Reranking comparison (moved to top)
    st.markdown("## üéØ Reranking: Cohere vs Reciprocal Rank Fusion (RRF)")
    st.caption("Understanding different reranking strategies for hybrid search")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Cohere Rerank (ML-Based)")
        st.markdown("""
        **Approach:**
        - Uses transformer-based neural network
        - Trained on query-document relevance pairs
        - Understands semantic relationships
        
        **Pros:**
        - ‚úÖ Superior relevance accuracy
        - ‚úÖ Handles complex queries well
        - ‚úÖ Cross-lingual capabilities
        - ‚úÖ Continuous model improvements
        
        **Cons:**
        - ‚ùå API latency (~50-200ms)
        - ‚ùå Cost per request
        - ‚ùå External dependency
        
        **Best For:**
        - Production search applications
        - User-facing search experiences
        - When accuracy is critical
        """)
    
    with col2:
        st.markdown("### Reciprocal Rank Fusion (RRF)")
        st.markdown("""
        **Approach:**
        - Mathematical formula: `score = Œ£(1/(k + rank))`
        - Combines rankings from multiple methods
        - Pure PostgreSQL implementation
        
        **Pros:**
        - ‚úÖ Zero latency (in-database)
        - ‚úÖ No external dependencies
        - ‚úÖ No additional cost
        - ‚úÖ Deterministic results
        
        **Cons:**
        - ‚ùå Less accurate than ML models
        - ‚ùå Doesn't understand semantics
        - ‚ùå Fixed algorithm (no learning)
        
        **Best For:**
        - Cost-sensitive applications
        - Low-latency requirements
        - Internal tools/dashboards
        """)
    
    st.markdown("### PostgreSQL RRF Implementation Example")
    st.code("""
-- Reciprocal Rank Fusion in PostgreSQL
WITH semantic_results AS (
    SELECT product_id, ROW_NUMBER() OVER (ORDER BY embedding <=> query_vector) as rank
    FROM products
),
keyword_results AS (
    SELECT product_id, ROW_NUMBER() OVER (ORDER BY ts_rank DESC) as rank
    FROM products
)
SELECT 
    COALESCE(s.product_id, k.product_id) as product_id,
    (1.0 / (60 + COALESCE(s.rank, 1000))) + 
    (1.0 / (60 + COALESCE(k.rank, 1000))) as rrf_score
FROM semantic_results s
FULL OUTER JOIN keyword_results k USING (product_id)
ORDER BY rrf_score DESC
LIMIT 10;
""", language="sql")
    
    st.info("üí° **Recommendation:** Use Cohere Rerank for user-facing search (better accuracy), and RRF for internal tools or when latency/cost is a concern.")
    
    # Section 1: Query Analysis
    st.markdown("---")
    st.markdown("## üß† Query Analysis")
    
    if 'comparison_query' in st.session_state and st.session_state.comparison_query:
        search_query = st.session_state.comparison_query
        
        col1, col2, col3, col4 = st.columns(4)
        
        # Query characteristics
        word_count = len(search_query.split())
        char_count = len(search_query)
        has_special = any(c in search_query for c in ['-', '_', '/', '&'])
        is_phrase = word_count > 3
        
        with col1:
            st.metric("Words", word_count)
        with col2:
            st.metric("Characters", char_count)
        with col3:
            st.metric("Type", "Phrase" if is_phrase else "Keywords")
        with col4:
            st.metric("Special Chars", "‚úÖ" if has_special else "‚ùå")
        
        # Recommended search method
        st.markdown("### üéØ Recommended Search Method")
        if word_count == 1:
            recommendation = "**Fuzzy Search** - Single word queries benefit from typo tolerance"
            reason = "Fuzzy matching handles misspellings and variations effectively for single terms."
        elif word_count <= 3 and not has_special:
            recommendation = "**Keyword Search** - Short queries work well with full-text search"
            reason = "PostgreSQL full-text search excels at exact term matching with stemming."
        elif is_phrase:
            recommendation = "**Semantic Search** - Long phrases capture intent better with embeddings"
            reason = "Vector embeddings understand context and meaning in longer queries."
        else:
            recommendation = "**Hybrid Search** - Balanced approach for mixed queries"
            reason = "Combines semantic understanding with keyword precision for optimal results."
        
        st.info(f"{recommendation}\n\n{reason}")
        
        # Query preprocessing insights
        st.markdown("### üîß Preprocessing Pipeline")
        preprocessing_cols = st.columns(3)
        with preprocessing_cols[0]:
            st.markdown("""
            **Text Normalization**
            - ‚úÖ Lowercase conversion
            - ‚úÖ Whitespace trimming
            - ‚úÖ Special char handling
            """)
        with preprocessing_cols[1]:
            st.markdown("""
            **Embedding Generation**
            - ‚úÖ Cohere Embed v3
            - ‚úÖ 1024 dimensions
            - ‚úÖ Cosine similarity
            """)
        with preprocessing_cols[2]:
            st.markdown("""
            **Tokenization**
            - ‚úÖ Trigram generation
            - ‚úÖ English stemming
            - ‚úÖ Stop word filtering
            """)
    else:
        st.info("üëâ Run a search in Tab 1 to see query analysis")
    
    # Section 2: Result Overlap Analysis
    st.markdown("---")
    st.markdown("## üìä Result Overlap Analysis")
    st.caption("Understanding how different search methods agree on relevant results")
    
    # Get product IDs from each method
    method_results = {}
    for method_name in ['Keyword', 'Fuzzy', 'Semantic', 'Hybrid']:
        if f'results_{method_name}' in st.session_state:
            method_results[method_name] = set(
                r['productId'] for r in st.session_state[f'results_{method_name}']
            )
    
    if len(method_results) >= 2:
        # Calculate overlaps
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Total Unique Products", len(set().union(*method_results.values())))
        
        with col2:
            # Products in all methods
            common_all = set.intersection(*method_results.values()) if method_results else set()
            st.metric("In All Methods", len(common_all))
        
        with col3:
            # Average overlap
            overlaps = []
            methods = list(method_results.keys())
            for i in range(len(methods)):
                for j in range(i+1, len(methods)):
                    overlap = len(method_results[methods[i]] & method_results[methods[j]])
                    overlaps.append(overlap)
            avg_overlap = sum(overlaps) / len(overlaps) if overlaps else 0
            st.metric("Avg Pairwise Overlap", f"{avg_overlap:.1f}")
        
        # Method-specific unique results
        st.markdown("### Unique Results per Method")
        unique_cols = st.columns(4)
        for idx, (method, ids) in enumerate(method_results.items()):
            other_ids = set().union(*[v for k, v in method_results.items() if k != method])
            unique = ids - other_ids
            with unique_cols[idx]:
                st.metric(method, len(unique), delta=f"{len(unique)} unique")
        
        st.info("üí° **Interpretation:** High overlap indicates methods agree on relevance. Low overlap means methods find different aspects of the query.")
        
        # Pairwise overlap matrix
        st.markdown("### Pairwise Overlap Matrix")
        overlap_data = []
        for m1 in methods:
            row = []
            for m2 in methods:
                if m1 == m2:
                    row.append(len(method_results[m1]))
                else:
                    row.append(len(method_results[m1] & method_results[m2]))
            overlap_data.append(row)
        
        df_overlap = pd.DataFrame(overlap_data, columns=methods, index=methods)
        st.dataframe(df_overlap, width='stretch')
        
    else:
        st.info("üëâ Run a search in Tab 1 to see result overlap analysis")
    
    # Section 3: Index Performance
    st.markdown("---")
    st.markdown("## üîß Index Configuration & Performance")
    st.caption("PostgreSQL index setup and tuning parameters for production")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("### Vector Index (HNSW)")
        st.code("""CREATE INDEX ON product_catalog 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);""")
        st.markdown("""
        **Parameters:**
        - `m=16`: Max connections per layer
          - Higher = better recall, more memory
          - Typical range: 8-64
        - `ef_construction=64`: Build-time search
          - Higher = better quality, slower build
          - Typical range: 32-512
        """)
    
    with col2:
        st.markdown("### Full-Text Index (GIN)")
        st.code("""CREATE INDEX ON product_catalog 
USING gin(
  to_tsvector('english', 
              product_description)
);""")
        st.markdown("""
        **Features:**
        - English language stemming
        - Stop word removal
        - Fast phrase matching
        - Supports ranking (ts_rank)
        """)
    
    with col3:
        st.markdown("### Trigram Index (GIN)")
        st.code("""CREATE INDEX ON product_catalog 
USING gin(
  product_description 
  gin_trgm_ops
);""")
        st.markdown("""
        **Configuration:**
        - Similarity threshold: 0.1
        - Trigram tokenization
        - Fuzzy matching support
        - Typo tolerance
        """)
    
    # Performance recommendations
    st.markdown("---")
    st.markdown("## üöÄ Production Tuning Recommendations")
    
    rec_col1, rec_col2 = st.columns(2)
    
    with rec_col1:
        st.markdown("""
        ### Query Optimization
        - ‚úÖ Use HNSW for semantic search (10-100x faster than IVFFlat)
        - ‚úÖ Set appropriate `ef_search` for recall/speed tradeoff
        - ‚úÖ Combine with WHERE clauses for filtered search
        - ‚úÖ Use EXPLAIN ANALYZE to profile queries
        - ‚úÖ Consider query result caching for common searches
        """)
    
    with rec_col2:
        st.markdown("""
        ### Index Maintenance
        - ‚úÖ Monitor index bloat with pg_stat_user_indexes
        - ‚úÖ VACUUM ANALYZE after bulk updates
        - ‚úÖ Adjust work_mem for large index builds
        - ‚úÖ Use parallel index creation for large tables
        - ‚úÖ Consider partitioning for very large datasets (>10M rows)
        """)
    
    st.markdown("---")
    st.markdown("## üìö Additional Resources")
    
    resource_cols = st.columns(3)
    with resource_cols[0]:
        st.markdown("""
        **pgvector Documentation**
        - [GitHub Repository](https://github.com/pgvector/pgvector)
        - [HNSW Algorithm](https://arxiv.org/abs/1603.09320)
        - [Performance Tuning](https://github.com/pgvector/pgvector#performance)
        """)
    
    with resource_cols[1]:
        st.markdown("""
        **PostgreSQL Full-Text Search**
        - [Official Docs](https://www.postgresql.org/docs/current/textsearch.html)
        - [GIN Indexes](https://www.postgresql.org/docs/current/gin.html)
        - [pg_trgm Extension](https://www.postgresql.org/docs/current/pgtrgm.html)
        """)
    
    with resource_cols[2]:
        st.markdown("""
        **Aurora PostgreSQL**
        - [User Guide](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/)
        - [Best Practices](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.BestPractices.html)
        - [Performance Insights](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_PerfInsights.html)
        """)

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; padding: 2rem 0;">
    <p style="font-size: 1.1rem;">DAT409: Hybrid Search with Aurora PostgreSQL</p>
    <p style="font-size: 0.875rem;">Built with Streamlit, pgvector, Cohere, and MCP</p>
    <p style="font-size: 0.75rem; margin-top: 1rem;">Amazon Web Services ‚Ä¢ re:Invent 2025</p>
</div>
""", unsafe_allow_html=True)