[
  {
    "doc_id": "681f84a3_data_engineer_e6cc16c2",
    "content": "Batch job performance degraded - cache hit 60%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-02T04:25:00Z",
      "severity": "critical",
      "incident_id": "681f84a3",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 87,
        "work_mem_mb": 41,
        "wait_count": 14,
        "swap_usage_gb": 15.4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "681f84a3_dba_2af38efd",
    "content": "Memory exhaustion: 87% RAM, 15.4GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-02T04:31:00Z",
      "severity": "critical",
      "incident_id": "681f84a3",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 14,
        "buffer_cache_hit": 60,
        "work_mem_mb": 41,
        "oom_kills": 8
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "681f84a3_sre_c8720d1e",
    "content": "AWS monitoring: Memory 87%, swap 15.4GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-02T04:35:00Z",
      "severity": "critical",
      "incident_id": "681f84a3",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 41,
        "buffer_cache_hit": 60,
        "swap_usage_gb": 15.4,
        "memory_usage_percent": 87,
        "oom_kills": 8
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "681f84a3_developer_6d7ebf09",
    "content": "Database swapping 15.4GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-02T04:35:00Z",
      "severity": "warning",
      "incident_id": "681f84a3",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 87,
        "work_mem_mb": 41,
        "oom_kills": 8,
        "wait_count": 14
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "bd1f3480_dba_4e38c91e",
    "content": "Standby lag alert: 63075929 bytes behind, 958MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-02T15:00:00Z",
      "severity": "warning",
      "incident_id": "bd1f3480",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 63075929,
        "wait_count": 9,
        "replica_count": 4,
        "lag_seconds": 46,
        "wal_size_mb": 958
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "bd1f3480_data_engineer_4b1c019e",
    "content": "Pipeline data quality affected by 46s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-02T15:01:00Z",
      "severity": "warning",
      "incident_id": "bd1f3480",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_bytes": 63075929,
        "lag_seconds": 46,
        "wal_size_mb": 958
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a3cb3650_dba_79b0a2be",
    "content": "Deadlock cascade on 'orders' - 47/min for 3min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-02T15:27:00Z",
      "severity": "warning",
      "incident_id": "a3cb3650",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "cascade_duration_min": 3,
        "lock_wait_ms": 6835,
        "blocked_queries": 22,
        "wait_count": 32
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a3cb3650_sre_5f0c8368",
    "content": "Incident: Lock contention causing 22 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-02T15:34:00Z",
      "severity": "warning",
      "incident_id": "a3cb3650",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 6835,
        "deadlocks_per_minute": 47,
        "transaction_rollback": 176
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c20a34c8_developer_19986f63",
    "content": "Connection pool exhausted - 35 threads waiting, 984 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-03T16:36:00Z",
      "severity": "warning",
      "incident_id": "c20a34c8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 95,
        "wait_ms": 9345
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c20a34c8_sre_eaaacf8b",
    "content": "AWS RDS Alert: Connection count 984, wait time 9345ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-03T16:40:00Z",
      "severity": "critical",
      "incident_id": "c20a34c8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 9345,
        "wait_time_ms": 9345.742102663822,
        "error_code": "PG-53300"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c20a34c8_data_engineer_41af336d",
    "content": "Spark job waiting 9345ms for database connection - 35 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-03T16:44:00Z",
      "severity": "critical",
      "incident_id": "c20a34c8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_ms": 9345,
        "wait_count": 35,
        "active": 984,
        "threshold": 95
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1cd841d597f243c7",
    "content": "WAL archiving on schedule - 42 segments archived",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-03T21:54:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "98a5ab41_data_engineer_bb962667",
    "content": "Data freshness issue: 69340690 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-03T23:57:00Z",
      "severity": "info",
      "incident_id": "98a5ab41",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 233,
        "wal_size_mb": 2035,
        "network_latency_ms": 15,
        "lag_bytes": 69340690,
        "wait_count": 19
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "98a5ab41_developer_c47a99eb",
    "content": "Data consistency issue: replicas lagging by 69340690 bytes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-04T00:02:00Z",
      "severity": "info",
      "incident_id": "98a5ab41",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "wait_count": 19,
        "lag_bytes": 69340690
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "98a5ab41_sre_0fee46e0",
    "content": "Critical: Replication lag exceeding SLA (233s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-04T00:05:00Z",
      "severity": "info",
      "incident_id": "98a5ab41",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 69340690,
        "wal_size_mb": 2035,
        "replica_count": 2,
        "network_latency_ms": 15
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1e4039bc_developer_ad1d3cb4",
    "content": "Database swapping 6.1GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-05T00:20:00Z",
      "severity": "warning",
      "incident_id": "1e4039bc",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 37,
        "swap_usage_gb": 6.1,
        "buffer_cache_hit": 67
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1e4039bc_sre_9d8cd5f2",
    "content": "Resource exhaustion: 3 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-05T00:23:00Z",
      "severity": "warning",
      "incident_id": "1e4039bc",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 6.1,
        "buffer_cache_hit": 67,
        "work_mem_mb": 57
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1e4039bc_dba_f81849b6",
    "content": "Memory exhaustion: 87% RAM, 6.1GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-05T00:24:00Z",
      "severity": "warning",
      "incident_id": "1e4039bc",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 67,
        "work_mem_mb": 57,
        "swap_usage_gb": 6.1,
        "memory_usage_percent": 87,
        "oom_kills": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_d428d860e4a444e7",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-06T06:17:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_4fe0144c29124c5e",
    "content": "Memory usage normal - 12.3GB of 128GB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-06T19:03:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "792dbb07_dba_0612bf34",
    "content": "FATAL: out of memory - 8 processes killed, 92% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-07T01:45:00Z",
      "severity": "warning",
      "incident_id": "792dbb07",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 8,
        "buffer_cache_hit": 37,
        "memory_usage_percent": 92
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "792dbb07_sre_cee99cb7",
    "content": "Critical: Buffer cache efficiency 37% (memory pressure)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-07T01:54:00Z",
      "severity": "warning",
      "incident_id": "792dbb07",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 92,
        "oom_kills": 8,
        "buffer_cache_hit": 37,
        "swap_usage_gb": 5.8,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "792dbb07_data_engineer_0fc47e45",
    "content": "Data processing failed - 8 tasks killed, 5.8GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-07T01:57:00Z",
      "severity": "warning",
      "incident_id": "792dbb07",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 37,
        "memory_usage_percent": 92,
        "oom_kills": 8,
        "work_mem_mb": 40,
        "swap_usage_gb": 5.8
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_6aa7fdc08c444391",
    "content": "Checkpoint completed: 28202 buffers written in 9.4 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-07T11:22:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_74bca98aa3de4ec0",
    "content": "Buffer cache warmed - 18.6GB loaded into memory",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-07T11:57:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ac2b0757_dba_2f749401",
    "content": "Index scan taking 256x longer on 'idx_users_email' - 42901960 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-07T18:12:00Z",
      "severity": "info",
      "incident_id": "ac2b0757",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 77,
        "affected_query": "user_dashboard",
        "wait_count": 33,
        "rows_scanned": 42901960,
        "query_time_after_ms": 40889.764139882434
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ac2b0757_sre_b931a078",
    "content": "Service alert: user_dashboard queries 256x slower, CPU 77%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-07T18:17:00Z",
      "severity": "info",
      "incident_id": "ac2b0757",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 40889.764139882434,
        "affected_query": "user_dashboard",
        "wait_count": 33,
        "query_time_before_ms": 89.20171220655851,
        "slowdown_factor": 256
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ac2b0757_developer_83d4bbc1",
    "content": "API timeout: 'user_dashboard' taking 40889ms (was 89.2ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-07T18:21:00Z",
      "severity": "info",
      "incident_id": "ac2b0757",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 256,
        "wait_count": 33,
        "affected_query": "user_dashboard",
        "rows_scanned": 42901960,
        "query_time_after_ms": 40889.764139882434
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d4ca5023_sre_826da548",
    "content": "Incident: Lock contention causing 64 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-08T02:53:00Z",
      "severity": "info",
      "incident_id": "d4ca5023",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 38,
        "blocked_queries": 64,
        "deadlocks_per_minute": 41
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "d4ca5023_developer_6c710872",
    "content": "Deadlock rate 41/min causing 64 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-08T03:02:00Z",
      "severity": "info",
      "incident_id": "d4ca5023",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 64,
        "cascade_duration_min": 12,
        "deadlocks_per_minute": 41,
        "lock_wait_ms": 6465
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "46528c80_dba_460200b6",
    "content": "Connection accumulation: 12.7 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-08T17:04:00Z",
      "severity": "critical",
      "incident_id": "46528c80",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 58,
        "active": 581,
        "max_connections": 1000,
        "wait_count": 29
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46528c80_data_engineer_68c7a6eb",
    "content": "ETL connection pool: 167 idle connections leaking at 13/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-08T17:05:00Z",
      "severity": "critical",
      "incident_id": "46528c80",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 167,
        "leak_duration_hours": 14,
        "max_connections": 1000,
        "wait_count": 29,
        "active_connections": 581.2037569884684
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46528c80_developer_535cbcef",
    "content": "Connection pool: 167 idle, 581 active - possible 454MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-08T17:09:00Z",
      "severity": "critical",
      "incident_id": "46528c80",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 58,
        "memory_usage_mb": 454,
        "leak_rate_per_hour": 12.7,
        "active_connections": 581.2037569884684
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46528c80_sre_35740120",
    "content": "Linear growth: 167 idle connections, 454MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-08T17:10:00Z",
      "severity": "critical",
      "incident_id": "46528c80",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 29,
        "max_connections": 1000,
        "leak_duration_hours": 14,
        "leak_rate_per_hour": 12.7
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_735450c1481c4994",
    "content": "Connection recycling: 44 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-09T01:54:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_603ebd0a4fd34c5f",
    "content": "Network latency optimal - 2.8ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-11T02:56:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1c74d5c8_dba_d148c9ed",
    "content": "Memory exhaustion: 92% RAM, 15.5GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-12T04:55:00Z",
      "severity": "critical",
      "incident_id": "1c74d5c8",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 19,
        "work_mem_mb": 26,
        "memory_usage_percent": 92,
        "swap_usage_gb": 15.5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1c74d5c8_sre_c454d14f",
    "content": "Resource exhaustion: 7 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-12T04:55:00Z",
      "severity": "critical",
      "incident_id": "1c74d5c8",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 26,
        "wait_count": 19,
        "oom_kills": 7,
        "swap_usage_gb": 15.5,
        "memory_usage_percent": 92
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "7d7fedae_sre_aee11e1e",
    "content": "Monitoring: Connection pool 93% full, 35 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-12T07:15:00Z",
      "severity": "info",
      "incident_id": "7d7fedae",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "threshold": 93,
        "wait_count": 35,
        "active_connections": 951.7154474467363
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7d7fedae_dba_f095796f",
    "content": "ERROR: too many connections for role 'app_user' (current: 951, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-12T07:16:00Z",
      "severity": "info",
      "incident_id": "7d7fedae",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 93,
        "active_connections": 951.7154474467363,
        "active": 951,
        "wait_ms": 22646
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7d7fedae_developer_6dbdf636",
    "content": "JDBC connection failed after 22646ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-12T07:19:00Z",
      "severity": "info",
      "incident_id": "7d7fedae",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 22646,
        "max_connections": 1000,
        "spike_duration_min": 21.6,
        "threshold": 93
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "41ec9dbb_data_engineer_6c858e52",
    "content": "ETL bottleneck: 'report_aggregate' step taking 41638ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-13T03:41:00Z",
      "severity": "warning",
      "incident_id": "41ec9dbb",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 34007841,
        "query_time_before_ms": 174.55082741734486,
        "cpu_usage": 74,
        "query_time_after_ms": 41638.59833282178
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "41ec9dbb_developer_eca7663c",
    "content": "Application query 'report_aggregate' timeout after 41638ms at 74% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-13T03:47:00Z",
      "severity": "warning",
      "incident_id": "41ec9dbb",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "query_time_after_ms": 41638.59833282178,
        "query_time_before_ms": 174.55082741734486,
        "cpu_usage": 74
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "41ec9dbb_sre_0ff8e49f",
    "content": "Latency alarm: 'report_aggregate' breached 41638ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-13T03:52:00Z",
      "severity": "warning",
      "incident_id": "41ec9dbb",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 74,
        "wait_count": 24,
        "rows_scanned": 34007841,
        "affected_query": "report_aggregate",
        "query_time_before_ms": 174.55082741734486
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ed0420a3_dba_40077f79",
    "content": "Standby lag alert: 42608396 bytes behind, 945MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-13T07:17:00Z",
      "severity": "info",
      "incident_id": "ed0420a3",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 257,
        "wal_size_mb": 945,
        "lag_bytes": 42608396
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ed0420a3_developer_70463ff2",
    "content": "Read replica delay causing user-visible inconsistencies",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-13T07:22:00Z",
      "severity": "info",
      "incident_id": "ed0420a3",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_seconds": 257,
        "wal_size_mb": 945,
        "wait_count": 37
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f2f7559ec75947f7",
    "content": "Connection recycling: 100 connections refreshed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-13T13:44:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7086055b27d04cf0",
    "content": "Memory usage normal - 64.0GB of 128GB used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-13T17:47:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d830dbd359b44ed5",
    "content": "Backup successful: 15.5GB completed in 15 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-15T15:33:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "43c73f17_data_engineer_3e997ff7",
    "content": "Batch process deadlocked - 8min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-15T18:23:00Z",
      "severity": "warning",
      "incident_id": "43c73f17",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 74,
        "transaction_rollback": 67,
        "cascade_duration_min": 8,
        "lock_wait_ms": 3095,
        "wait_count": 15
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "43c73f17_sre_3bc173a5",
    "content": "System alert: 3095ms lock wait impacting 67 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-15T18:27:00Z",
      "severity": "info",
      "incident_id": "43c73f17",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 17,
        "affected_tables": "orders",
        "transaction_rollback": 67
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "43c73f17_developer_0276c9f4",
    "content": "Transaction retry exhausted - 17 deadlocks/min on 'orders'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-15T18:37:00Z",
      "severity": "info",
      "incident_id": "43c73f17",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "deadlocks_per_minute": 17,
        "wait_count": 15
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_dd75e7497f634462",
    "content": "Network latency optimal - 2.6ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-16T00:00:17Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_1e7e0bfac1c44b99",
    "content": "Connection recycling: 97 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-16T02:47:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_af013be2033b4d4e",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-16T07:28:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_8253abe125a442aa",
    "content": "Autovacuum completed on table 'sessions' - 376130 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-16T08:59:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_0db0327db3f34ca0",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-16T17:39:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1c91c1e1daaf47ab",
    "content": "Disk I/O normal - 4687 read IOPS, 2404 write IOPS",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-17T22:52:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "252d5060_dba_6e3963b9",
    "content": "Connection accumulation: 10.4 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-18T17:32:00Z",
      "severity": "info",
      "incident_id": "252d5060",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 179,
        "leak_rate_per_hour": 10.4,
        "active_connections": 648.4030747139363
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "252d5060_developer_c22a21fa",
    "content": "Connection pool: 179 idle, 648 active - possible 129MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-18T17:32:00Z",
      "severity": "info",
      "incident_id": "252d5060",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 64,
        "active": 648,
        "leak_duration_hours": 15,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "252d5060_data_engineer_26705d66",
    "content": "ETL connection pool: 179 idle connections leaking at 10/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-18T17:47:00Z",
      "severity": "warning",
      "incident_id": "252d5060",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 179,
        "leak_duration_hours": 15,
        "active": 648,
        "wait_count": 33
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "252d5060_sre_d7724c27",
    "content": "Linear growth: 179 idle connections, 129MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-18T17:52:00Z",
      "severity": "info",
      "incident_id": "252d5060",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 33,
        "leak_duration_hours": 15,
        "threshold": 64
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e864197e_dba_99aaa014",
    "content": "Vacuum freeze on 'logs' - 89% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-18T23:24:00Z",
      "severity": "info",
      "incident_id": "e864197e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 33486325,
        "tables_affected": "logs",
        "wait_count": 48,
        "io_usage_percent": 89,
        "vacuum_duration_min": 159
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e864197e_developer_8691f2c3",
    "content": "Application slowdown during 159min vacuum on 'logs'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-18T23:26:00Z",
      "severity": "info",
      "incident_id": "e864197e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "wait_count": 48,
        "io_usage_percent": 89,
        "dead_tuples": 33486325
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e864197e_sre_7f94dc54",
    "content": "Performance impact: 159min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-18T23:30:00Z",
      "severity": "info",
      "incident_id": "e864197e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 29,
        "vacuum_duration_min": 159,
        "io_usage_percent": 89,
        "tables_affected": "logs",
        "wait_count": 48
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e864197e_data_engineer_a30f14b8",
    "content": "ETL performance impacted by vacuum on 'logs'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-18T23:42:00Z",
      "severity": "info",
      "incident_id": "e864197e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 89,
        "vacuum_duration_min": 159,
        "wait_count": 48
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c889077aae8e4161",
    "content": "WAL archiving on schedule - 36 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-19T01:52:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_331bcbbb2405487e",
    "content": "Connection recycling: 60 connections refreshed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-19T16:49:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_22eeffa877944ca6",
    "content": "Checkpoint completed: 9039 buffers written in 4.0 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-19T20:38:27Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "958fa9fc_data_engineer_0c4d82d6",
    "content": "ETL reading outdated data from replica lagging 269s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-20T19:53:00Z",
      "severity": "warning",
      "incident_id": "958fa9fc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 36,
        "wal_size_mb": 1769,
        "lag_seconds": 269
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "958fa9fc_developer_25a21f25",
    "content": "Application seeing outdated data - 269s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-20T19:56:00Z",
      "severity": "warning",
      "incident_id": "958fa9fc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 1769,
        "replica_count": 4,
        "lag_seconds": 269,
        "network_latency_ms": 36
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_b10873e1f7184743",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-21T06:57:46Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_6f479c97442849bd",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-21T17:57:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_fd3917f1fc124e3b",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-21T19:52:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_a9c272424603424a",
    "content": "WAL archiving on schedule - 41 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-22T17:32:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9defb556_dba_b35acfc6",
    "content": "Deadlock cascade on 'payments' - 34/min for 2min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-24T09:42:00Z",
      "severity": "warning",
      "incident_id": "9defb556",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 26,
        "transaction_rollback": 138,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9defb556_data_engineer_99dc24b2",
    "content": "Data load failing: 138 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-24T09:57:00Z",
      "severity": "warning",
      "incident_id": "9defb556",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 138,
        "blocked_queries": 26,
        "wait_count": 9,
        "lock_wait_ms": 5157,
        "deadlocks_per_minute": 34
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_efbdf5f1b83443f8",
    "content": "Autovacuum completed on table 'sessions' - 828273 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-24T22:51:36Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0f1d0d281d734a23",
    "content": "Disk I/O normal - 3749 read IOPS, 1505 write IOPS",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-25T17:10:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_291fbea137b94ec4",
    "content": "Buffer cache warmed - 14.8GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-25T18:36:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3886f48b_data_engineer_a0afdc30",
    "content": "ETL jobs at risk - disk 93% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-26T21:02:00Z",
      "severity": "critical",
      "incident_id": "3886f48b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 6,
        "free_space_gb": 43,
        "wal_size_gb": 32
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3886f48b_dba_f08bebc2",
    "content": "WAL accumulation: 32GB of logs, disk 93% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-26T21:13:00Z",
      "severity": "critical",
      "incident_id": "3886f48b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 6,
        "disk_usage_percent": 93,
        "free_space_gb": 43,
        "growth_rate_gb_per_day": 83,
        "wait_count": 11
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3886f48b_developer_c6145f06",
    "content": "Critical: Database disk 93% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-26T21:15:00Z",
      "severity": "warning",
      "incident_id": "3886f48b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 83,
        "wal_size_gb": 32,
        "free_space_gb": 43,
        "wait_count": 11,
        "days_until_full": 6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_ab579804b4074f8e",
    "content": "Backup successful: 86.8GB completed in 8 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-26T23:00:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "58503186_developer_3cb5ca90",
    "content": "User queries slow - autovacuum processing 64406279 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-27T10:01:00Z",
      "severity": "info",
      "incident_id": "58503186",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 144,
        "dead_tuples": 64406279,
        "io_usage_percent": 81,
        "tables_affected": "sessions",
        "table_bloat_percent": 76
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "58503186_dba_d8c9fd34",
    "content": "Table bloat: 'sessions' at 76% with 64406279 dead rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-27T10:13:00Z",
      "severity": "info",
      "incident_id": "58503186",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "sessions",
        "table_bloat_percent": 76,
        "wait_count": 43
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "5d6e8b20_sre_9d62f490",
    "content": "Memory alert: 94% utilized, 3 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-27T15:43:00Z",
      "severity": "critical",
      "incident_id": "5d6e8b20",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 94,
        "oom_kills": 3,
        "buffer_cache_hit": 55,
        "work_mem_mb": 59,
        "wait_count": 46
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5d6e8b20_data_engineer_c33e0516",
    "content": "Memory pressure: 94% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-27T15:45:00Z",
      "severity": "critical",
      "incident_id": "5d6e8b20",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 55,
        "work_mem_mb": 59,
        "oom_kills": 3
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5d6e8b20_dba_7f4a454c",
    "content": "work_mem reduced to 59MB - 3 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-27T15:47:00Z",
      "severity": "critical",
      "incident_id": "5d6e8b20",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 3,
        "wait_count": 46,
        "memory_usage_percent": 94,
        "swap_usage_gb": 11.5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "08339ebc_dba_b586fbdd",
    "content": "WAL accumulation: 24GB of logs, disk 97% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-28T18:04:00Z",
      "severity": "info",
      "incident_id": "08339ebc",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 49,
        "disk_usage_percent": 97,
        "free_space_gb": 42,
        "days_until_full": 6
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "08339ebc_sre_eedcfed5",
    "content": "Emergency response needed: 97% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-28T18:19:00Z",
      "severity": "info",
      "incident_id": "08339ebc",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 42,
        "disk_usage_percent": 97,
        "days_until_full": 6,
        "wait_count": 49,
        "growth_rate_gb_per_day": 64
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_2e750cbd95c84a59",
    "content": "Buffer cache warmed - 21.3GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-28T22:37:57Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "7f9fc990_developer_1013719f",
    "content": "Application at risk: only 37GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-29T11:04:00Z",
      "severity": "warning",
      "incident_id": "7f9fc990",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 39,
        "growth_rate_gb_per_day": 39,
        "wal_size_gb": 38,
        "days_until_full": 1,
        "disk_usage_percent": 94
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7f9fc990_data_engineer_8d522f48",
    "content": "Storage crisis: 1 days until pipeline failure",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-29T11:09:00Z",
      "severity": "critical",
      "incident_id": "7f9fc990",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 38,
        "free_space_gb": 37,
        "days_until_full": 1
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7f9fc990_sre_92791c1d",
    "content": "Emergency response needed: 94% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-29T11:16:00Z",
      "severity": "critical",
      "incident_id": "7f9fc990",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 39,
        "growth_rate_gb_per_day": 39,
        "wal_size_gb": 38
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7f9fc990_dba_2a29169e",
    "content": "WAL accumulation: 38GB of logs, disk 94% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-29T11:17:00Z",
      "severity": "critical",
      "incident_id": "7f9fc990",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 39,
        "disk_usage_percent": 94,
        "free_space_gb": 37,
        "days_until_full": 1,
        "wait_count": 39
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_4b2d0ff5e5334f77",
    "content": "Memory usage normal - 27.1GB of 128GB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-29T14:29:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_84cbb995c76c441b",
    "content": "Connection pool stable at 496 connections (40% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-29T18:34:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_5f2d4a8f7e334291",
    "content": "Memory usage normal - 70.7GB of 128GB used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-29T19:22:47Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c27b2d5c23474772",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-29T23:41:46Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_f1b57959de7d47f6",
    "content": "Connection recycling: 32 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-30T03:48:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "662102d0_data_engineer_f6d38e0f",
    "content": "Data job resource leak: 13/hour accumulation rate",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-30T20:25:00Z",
      "severity": "critical",
      "incident_id": "662102d0",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 509.62681355156775,
        "memory_usage_mb": 260,
        "idle_connections": 279,
        "active": 509
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "662102d0_developer_d927e53f",
    "content": "Connection pool growing: 509 active + 279 idle = potential leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-06-30T20:29:00Z",
      "severity": "warning",
      "incident_id": "662102d0",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "leak_duration_hours": 23,
        "threshold": 50
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "662102d0_sre_6346774b",
    "content": "Grafana: Connection leak - 13.0 connections/hour over 23h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-06-30T20:32:00Z",
      "severity": "critical",
      "incident_id": "662102d0",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 23,
        "active_connections": 509.62681355156775,
        "active": 509,
        "memory_usage_mb": 260
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "662102d0_dba_bbdc917b",
    "content": "Resource leak: 279 connections idle, 509 active, total memory 260MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-06-30T20:34:00Z",
      "severity": "critical",
      "incident_id": "662102d0",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 279,
        "wait_count": 23,
        "leak_rate_per_hour": 13.0
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_54f4e4f27ea3490b",
    "content": "Connection recycling: 16 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-06-30T22:12:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_b446f8aa2c3b40c7",
    "content": "Connection pool stable at 297 connections (66% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-01T01:22:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "808120d7_data_engineer_ee67f7b8",
    "content": "Table 'events' bloat (61%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-02T02:53:00Z",
      "severity": "warning",
      "incident_id": "808120d7",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "events",
        "vacuum_duration_min": 104,
        "io_usage_percent": 62,
        "dead_tuples": 50250129,
        "table_bloat_percent": 61
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "808120d7_dba_8e9f8b21",
    "content": "Autovacuum running 104min on 'events', IO at 62%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-02T02:59:00Z",
      "severity": "info",
      "incident_id": "808120d7",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 61,
        "dead_tuples": 50250129,
        "io_usage_percent": 62,
        "wait_count": 30
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_6551c86357a14aac",
    "content": "Database health check passed - response time 113ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-02T05:18:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_162f539954af46dc",
    "content": "Statistics updated for 49 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-02T08:38:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_166132deb6e14500",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-02T12:25:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_dc101e118b754b4b",
    "content": "Autovacuum completed on table 'events' - 538364 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-02T19:12:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "62f96f2b_developer_6a0ea38e",
    "content": "Connection pool exhausted - 40 threads waiting, 959 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-04T16:10:00Z",
      "severity": "warning",
      "incident_id": "62f96f2b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 40,
        "threshold": 95,
        "error_code": "PG-53300",
        "active": 959,
        "wait_ms": 25716
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "62f96f2b_data_engineer_669956bc",
    "content": "Spark job waiting 25716ms for database connection - 40 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-04T16:16:00Z",
      "severity": "warning",
      "incident_id": "62f96f2b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 95,
        "max_connections": 1000,
        "spike_duration_min": 7.1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "62f96f2b_dba_cccb2f92",
    "content": "PANIC: connection limit exceeded - 959/1000 after 7.1min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-04T16:17:00Z",
      "severity": "warning",
      "incident_id": "62f96f2b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "wait_ms": 25716,
        "active_connections": 959.9921151948739
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_743b36af8aa64b69",
    "content": "WAL archiving on schedule - 61 segments archived",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-05T06:09:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "077fffb2_dba_76872e10",
    "content": "Sequential scan on 24536533 rows - index 'idx_products_sku' not used, CPU at 84%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-05T16:11:00Z",
      "severity": "info",
      "incident_id": "077fffb2",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 178.20142207436166,
        "affected_query": "order_search",
        "query_time_after_ms": 15008.915585041974,
        "rows_scanned": 24536533
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "077fffb2_sre_23bcbff7",
    "content": "Latency alarm: 'order_search' breached 15008ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-05T16:13:00Z",
      "severity": "info",
      "incident_id": "077fffb2",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 24536533,
        "wait_count": 21,
        "query_time_before_ms": 178.20142207436166,
        "query_time_after_ms": 15008.915585041974
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "077fffb2_developer_379bb90e",
    "content": "Application query 'order_search' timeout after 15008ms at 84% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-05T16:13:00Z",
      "severity": "info",
      "incident_id": "077fffb2",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 15008.915585041974,
        "affected_query": "order_search",
        "query_time_before_ms": 178.20142207436166
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ac063dd23e734ed9",
    "content": "Backup successful: 23.0GB completed in 40 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-07T01:59:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_348af76f1cbd45dd",
    "content": "Connection pool stable at 167 connections (19% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-07T04:32:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "f3c7279a_dba_7b58f884",
    "content": "Memory exhaustion: 94% RAM, 6.2GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-07T06:57:00Z",
      "severity": "warning",
      "incident_id": "f3c7279a",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 4,
        "buffer_cache_hit": 47,
        "swap_usage_gb": 6.2,
        "memory_usage_percent": 94
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f3c7279a_sre_6fae8e13",
    "content": "Resource exhaustion: 8 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-07T07:07:00Z",
      "severity": "warning",
      "incident_id": "f3c7279a",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 94,
        "work_mem_mb": 4,
        "swap_usage_gb": 6.2,
        "wait_count": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f3c7279a_data_engineer_a8baf1fd",
    "content": "Batch job performance degraded - cache hit 47%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-07T07:10:00Z",
      "severity": "warning",
      "incident_id": "f3c7279a",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 4,
        "swap_usage_gb": 6.2,
        "wait_count": 7,
        "buffer_cache_hit": 47
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "3a3cfa47_dba_37425bc0",
    "content": "CRITICAL: Disk 91% full, only 46GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-08T18:52:00Z",
      "severity": "info",
      "incident_id": "3a3cfa47",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 88,
        "wait_count": 37,
        "growth_rate_gb_per_day": 67,
        "days_until_full": 2
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3a3cfa47_developer_f0cb0e55",
    "content": "Application at risk: only 46GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-08T18:55:00Z",
      "severity": "info",
      "incident_id": "3a3cfa47",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 46,
        "wait_count": 37,
        "days_until_full": 2,
        "disk_usage_percent": 91
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3a3cfa47_data_engineer_c1c00ef6",
    "content": "Data pipeline may fail: only 46GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-08T18:58:00Z",
      "severity": "warning",
      "incident_id": "3a3cfa47",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 88,
        "days_until_full": 2,
        "disk_usage_percent": 91,
        "growth_rate_gb_per_day": 67,
        "wait_count": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "985f4f23_developer_a7d8a905",
    "content": "User queries slow - autovacuum processing 98857383 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-09T01:41:00Z",
      "severity": "warning",
      "incident_id": "985f4f23",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 50,
        "vacuum_duration_min": 330,
        "wait_count": 14
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "985f4f23_data_engineer_f9e272d0",
    "content": "Pipeline slowdown - 50% IO used by maintenance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-09T01:46:00Z",
      "severity": "warning",
      "incident_id": "985f4f23",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 330,
        "io_usage_percent": 50,
        "wait_count": 14,
        "tables_affected": "logs"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ab9d3d35_dba_22886c18",
    "content": "CRITICAL: Disk 87% full, only 6GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-09T01:48:00Z",
      "severity": "critical",
      "incident_id": "ab9d3d35",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 18,
        "disk_usage_percent": 87,
        "days_until_full": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "985f4f23_dba_502ee17f",
    "content": "Table bloat: 'logs' at 73% with 98857383 dead rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-09T01:48:00Z",
      "severity": "warning",
      "incident_id": "985f4f23",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 14,
        "tables_affected": "logs",
        "vacuum_duration_min": 330,
        "io_usage_percent": 50,
        "table_bloat_percent": 73
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ab9d3d35_sre_00d3e8df",
    "content": "Critical infrastructure alert: WAL using 13GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-09T01:51:00Z",
      "severity": "critical",
      "incident_id": "ab9d3d35",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 53,
        "wait_count": 18,
        "disk_usage_percent": 87
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ea83f9f4_dba_0f3d82a1",
    "content": "Critical memory pressure: cache hit 31%, swap 13.5GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-09T04:53:00Z",
      "severity": "critical",
      "incident_id": "ea83f9f4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "wait_count": 38,
        "swap_usage_gb": 13.5,
        "buffer_cache_hit": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ea83f9f4_developer_9ddc46b2",
    "content": "Database OOM: 5 connections terminated at 91% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-09T04:58:00Z",
      "severity": "warning",
      "incident_id": "ea83f9f4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 38,
        "work_mem_mb": 9,
        "memory_usage_percent": 91,
        "oom_kills": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ea83f9f4_data_engineer_ab6c00e5",
    "content": "Pipeline memory issues: work_mem only 9MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-09T05:03:00Z",
      "severity": "critical",
      "incident_id": "ea83f9f4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 13.5,
        "oom_kills": 5,
        "memory_usage_percent": 91,
        "wait_count": 38,
        "buffer_cache_hit": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_08a6add112cd4af0",
    "content": "Query performance within SLA - P99 latency 65ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-09T16:52:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5fc8ff2d_developer_027f5abb",
    "content": "Database unavailable after 27070ms wait - circuit breaker opened at 91%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-10T20:46:00Z",
      "severity": "info",
      "incident_id": "5fc8ff2d",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 13,
        "wait_time_ms": 27070.8949175877,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5fc8ff2d_data_engineer_e8cfdf5c",
    "content": "Data pipeline blocked: connection pool at 91% capacity",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-10T20:48:00Z",
      "severity": "warning",
      "incident_id": "5fc8ff2d",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 27070.8949175877,
        "active": 998,
        "spike_duration_min": 31.2,
        "error_code": "PG-53300",
        "wait_count": 13
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_80a3b38d52d14594",
    "content": "Statistics updated for 8 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-11T19:56:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a5cd4e57_dba_33aa94a3",
    "content": "Long-running vacuum: 255min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-12T08:59:00Z",
      "severity": "warning",
      "incident_id": "a5cd4e57",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 96373952,
        "table_bloat_percent": 73,
        "tables_affected": "sessions"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a5cd4e57_developer_e8e235d7",
    "content": "User queries slow - autovacuum processing 96373952 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-12T09:00:00Z",
      "severity": "warning",
      "incident_id": "a5cd4e57",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 255,
        "wait_count": 48,
        "dead_tuples": 96373952,
        "tables_affected": "sessions",
        "table_bloat_percent": 73
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a5cd4e57_data_engineer_ec1ea5ef",
    "content": "Batch job delayed by autovacuum processing 96373952 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-12T09:01:00Z",
      "severity": "warning",
      "incident_id": "a5cd4e57",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 65,
        "wait_count": 48,
        "tables_affected": "sessions"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_bcd567920c4d4f5e",
    "content": "Connection recycling: 12 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-13T01:07:21Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "b09baa59_data_engineer_3ec2cd89",
    "content": "ETL bottleneck: 'order_search' step taking 11339ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-13T16:53:00Z",
      "severity": "critical",
      "incident_id": "b09baa59",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 78,
        "slowdown_factor": 431,
        "query_time_before_ms": 126.48904078340267,
        "rows_scanned": 13159563
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b09baa59_dba_2ec43b1b",
    "content": "Sequential scan on 13159563 rows - index 'idx_products_sku' not used, CPU at 78%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-13T17:09:00Z",
      "severity": "critical",
      "incident_id": "b09baa59",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 78,
        "wait_count": 13,
        "affected_query": "order_search"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b09baa59_sre_dc2b64a7",
    "content": "Latency alarm: 'order_search' breached 11339ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-13T17:12:00Z",
      "severity": "critical",
      "incident_id": "b09baa59",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 126.48904078340267,
        "cpu_usage": 78,
        "query_time_after_ms": 11339.756720357655
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d47720b7_dba_143117b3",
    "content": "Storage alert: Growing 25GB/day, full in 5 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-13T18:35:00Z",
      "severity": "info",
      "incident_id": "d47720b7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 90,
        "growth_rate_gb_per_day": 25,
        "free_space_gb": 8,
        "days_until_full": 5,
        "wait_count": 43
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d47720b7_sre_18b76784",
    "content": "Emergency response needed: 90% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-13T18:40:00Z",
      "severity": "info",
      "incident_id": "d47720b7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 90,
        "free_space_gb": 8,
        "growth_rate_gb_per_day": 25,
        "wal_size_gb": 39,
        "days_until_full": 5
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d47720b7_data_engineer_7f013f8c",
    "content": "Batch processing consuming 25GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-13T18:44:00Z",
      "severity": "info",
      "incident_id": "d47720b7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 43,
        "growth_rate_gb_per_day": 25,
        "free_space_gb": 8,
        "days_until_full": 5
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_7217f12455354910",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-14T12:57:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_4948569c2bdd4f39",
    "content": "Checkpoint completed: 31406 buffers written in 7.6 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-14T19:27:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "da4407d4_sre_764762fa",
    "content": "Grafana: Connection leak - 18.5 connections/hour over 18h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-15T08:34:00Z",
      "severity": "critical",
      "incident_id": "da4407d4",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 458,
        "threshold": 69,
        "leak_duration_hours": 18,
        "max_connections": 1000,
        "active_connections": 696.238584058922
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "da4407d4_data_engineer_680b71ab",
    "content": "Data job resource leak: 18/hour accumulation rate",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-15T08:41:00Z",
      "severity": "critical",
      "incident_id": "da4407d4",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 696.238584058922,
        "leak_duration_hours": 18,
        "memory_usage_mb": 458,
        "idle_connections": 176
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "93901cb4_dba_12b1c204",
    "content": "Disk space critical: 90% used, growth rate 20GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-16T09:34:00Z",
      "severity": "warning",
      "incident_id": "93901cb4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 25,
        "disk_usage_percent": 90,
        "free_space_gb": 11,
        "days_until_full": 1,
        "growth_rate_gb_per_day": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "93901cb4_sre_5f2c3ff3",
    "content": "Projection: Storage full in 1 days (20GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-16T09:38:00Z",
      "severity": "warning",
      "incident_id": "93901cb4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 26,
        "free_space_gb": 11,
        "growth_rate_gb_per_day": 20,
        "wait_count": 25
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "93901cb4_data_engineer_b97d49cc",
    "content": "ETL jobs at risk - disk 90% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-16T09:44:00Z",
      "severity": "warning",
      "incident_id": "93901cb4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 20,
        "days_until_full": 1,
        "wait_count": 25
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7c67a474d82b453c",
    "content": "Index 'idx_users_id' rebuild completed - 120MB size reduction",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-17T05:34:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "d83d654b_sre_32029960",
    "content": "Grafana: Connection leak - 10.1 connections/hour over 15h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-17T08:11:00Z",
      "severity": "critical",
      "incident_id": "d83d654b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 10.1,
        "memory_usage_mb": 372,
        "leak_duration_hours": 15,
        "active": 611,
        "active_connections": 611.5709339372743
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d83d654b_developer_f137d33e",
    "content": "Connection pool growing: 611 active + 313 idle = potential leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-17T08:16:00Z",
      "severity": "critical",
      "incident_id": "d83d654b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 61,
        "active_connections": 611.5709339372743,
        "memory_usage_mb": 372
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_435dacb804734a50",
    "content": "Buffer cache warmed - 1.9GB loaded into memory",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-17T13:27:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f79f2ec9_data_engineer_c2af3487",
    "content": "Storage crisis: 5 days until pipeline failure",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-17T15:06:00Z",
      "severity": "critical",
      "incident_id": "f79f2ec9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "growth_rate_gb_per_day": 63,
        "wait_count": 5,
        "disk_usage_percent": 96
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f79f2ec9_developer_dd953ce2",
    "content": "Application at risk: only 29GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-17T15:14:00Z",
      "severity": "critical",
      "incident_id": "f79f2ec9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 96,
        "growth_rate_gb_per_day": 63,
        "free_space_gb": 29,
        "wait_count": 5,
        "days_until_full": 5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f92ab5e5_data_engineer_88203a1b",
    "content": "Data retention issue: 49GB logs, 85% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-18T08:15:00Z",
      "severity": "warning",
      "incident_id": "f92ab5e5",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 2,
        "growth_rate_gb_per_day": 76,
        "free_space_gb": 15,
        "wait_count": 48
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f92ab5e5_dba_d741a4eb",
    "content": "Emergency: 15GB free, database will halt in 2 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-18T08:17:00Z",
      "severity": "warning",
      "incident_id": "f92ab5e5",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 49,
        "growth_rate_gb_per_day": 76,
        "wait_count": 48,
        "disk_usage_percent": 85,
        "days_until_full": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f92ab5e5_developer_4456d57d",
    "content": "Write failures imminent - 15GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-18T08:31:00Z",
      "severity": "warning",
      "incident_id": "f92ab5e5",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 15,
        "days_until_full": 2,
        "wal_size_gb": 49,
        "wait_count": 48
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "c36d9380_sre_d51c5246",
    "content": "Connection leak: 5.6/hr growth rate detected over 19h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-18T23:53:00Z",
      "severity": "info",
      "incident_id": "c36d9380",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 5.6,
        "threshold": 68,
        "idle_connections": 284,
        "active_connections": 685.8335495995225,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c36d9380_dba_e543d1f7",
    "content": "pg_stat_activity shows 284 idle connections consuming 423MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-19T00:11:00Z",
      "severity": "info",
      "incident_id": "c36d9380",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 685,
        "wait_count": 7,
        "idle_connections": 284,
        "memory_usage_mb": 423,
        "leak_duration_hours": 19
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "9de91984_data_engineer_4ab25d32",
    "content": "ETL performance impacted by vacuum on 'sessions'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-19T04:33:00Z",
      "severity": "warning",
      "incident_id": "9de91984",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 29301102,
        "vacuum_duration_min": 335,
        "table_bloat_percent": 32,
        "tables_affected": "sessions",
        "wait_count": 13
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "9de91984_dba_ea0cfd47",
    "content": "Vacuum freeze on 'sessions' - 52% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-19T04:34:00Z",
      "severity": "warning",
      "incident_id": "9de91984",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 32,
        "tables_affected": "sessions",
        "wait_count": 13,
        "vacuum_duration_min": 335
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "9de91984_developer_4b54a8d3",
    "content": "Application slowdown during 335min vacuum on 'sessions'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-19T04:43:00Z",
      "severity": "warning",
      "incident_id": "9de91984",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 52,
        "vacuum_duration_min": 335,
        "table_bloat_percent": 32,
        "dead_tuples": 29301102
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "55cd0477_developer_c5c57d7f",
    "content": "Storage exhaustion in 2 days at current growth rate",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-19T14:02:00Z",
      "severity": "warning",
      "incident_id": "55cd0477",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 84,
        "growth_rate_gb_per_day": 20,
        "disk_usage_percent": 95,
        "days_until_full": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "55cd0477_data_engineer_db02f458",
    "content": "Data retention issue: 84GB logs, 95% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-19T14:10:00Z",
      "severity": "warning",
      "incident_id": "55cd0477",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 84,
        "disk_usage_percent": 95,
        "free_space_gb": 1,
        "wait_count": 14
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4b8ccd6d_dba_c60a81ac",
    "content": "CRITICAL: Disk 88% full, only 6GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-20T01:36:00Z",
      "severity": "critical",
      "incident_id": "4b8ccd6d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 6,
        "wait_count": 28,
        "wal_size_gb": 78,
        "disk_usage_percent": 88,
        "growth_rate_gb_per_day": 65
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4b8ccd6d_developer_2f492261",
    "content": "Application at risk: only 6GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-20T01:45:00Z",
      "severity": "warning",
      "incident_id": "4b8ccd6d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 6,
        "disk_usage_percent": 88,
        "wal_size_gb": 78,
        "growth_rate_gb_per_day": 65
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4b8ccd6d_data_engineer_0af57133",
    "content": "Data pipeline may fail: only 6GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-20T01:47:00Z",
      "severity": "critical",
      "incident_id": "4b8ccd6d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 65,
        "days_until_full": 1,
        "disk_usage_percent": 88
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_cefa15fed6f941fd",
    "content": "Query cache hit ratio: 98% over last 7 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-20T08:13:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d1f031a4_data_engineer_19a4d4d5",
    "content": "Table 'events' bloat (50%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-21T09:17:00Z",
      "severity": "warning",
      "incident_id": "d1f031a4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 90,
        "tables_affected": "events",
        "vacuum_duration_min": 157,
        "dead_tuples": 93194485
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d1f031a4_sre_dedd9619",
    "content": "Monitoring: Autovacuum causing 90% IO saturation",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-21T09:25:00Z",
      "severity": "warning",
      "incident_id": "d1f031a4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 49,
        "io_usage_percent": 90,
        "table_bloat_percent": 50
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d1f031a4_developer_c4c69803",
    "content": "Database maintenance causing 90% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-21T09:26:00Z",
      "severity": "warning",
      "incident_id": "d1f031a4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 90,
        "tables_affected": "events",
        "table_bloat_percent": 50
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d1f031a4_dba_a126e2ff",
    "content": "Autovacuum running 157min on 'events', IO at 90%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-21T09:30:00Z",
      "severity": "warning",
      "incident_id": "d1f031a4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 93194485,
        "io_usage_percent": 90,
        "vacuum_duration_min": 157
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9a7656e7_data_engineer_cdec64e9",
    "content": "Data load failing: 171 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-21T10:35:00Z",
      "severity": "warning",
      "incident_id": "9a7656e7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "lock_wait_ms": 9384,
        "blocked_queries": 21,
        "deadlocks_per_minute": 30
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9a7656e7_dba_5347c8ec",
    "content": "Deadlock cascade on 'orders' - 30/min for 6min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-21T10:39:00Z",
      "severity": "warning",
      "incident_id": "9a7656e7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 6,
        "wait_count": 22,
        "affected_tables": "orders",
        "blocked_queries": 21,
        "lock_wait_ms": 9384
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9a7656e7_sre_98cd324f",
    "content": "Critical: 30 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-21T10:41:00Z",
      "severity": "warning",
      "incident_id": "9a7656e7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 22,
        "transaction_rollback": 171,
        "affected_tables": "orders",
        "cascade_duration_min": 6,
        "deadlocks_per_minute": 30
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9a7656e7_developer_786ba39d",
    "content": "Application errors: 171 transactions failed in 6min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-21T10:42:00Z",
      "severity": "warning",
      "incident_id": "9a7656e7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 21,
        "deadlocks_per_minute": 30,
        "transaction_rollback": 171,
        "lock_wait_ms": 9384,
        "cascade_duration_min": 6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_a1694ce7909f4dd6",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-22T09:31:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_43243bf5e2514f9d",
    "content": "SSL certificate valid for 284 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-23T01:56:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_5f8999ffecaf438f",
    "content": "Index 'idx_users_id' rebuild completed - 25MB size reduction",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-23T07:31:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "4646fffb_sre_81e5d3f7",
    "content": "Monitoring: Connection pool 96% full, 20 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-23T12:43:00Z",
      "severity": "warning",
      "incident_id": "4646fffb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 998,
        "active_connections": 998.7634764069859,
        "wait_ms": 13889,
        "threshold": 96,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4646fffb_data_engineer_6064450b",
    "content": "DBT run failed: waited 13889ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-23T12:48:00Z",
      "severity": "warning",
      "incident_id": "4646fffb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 58.0,
        "threshold": 96,
        "wait_count": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_af25b10e4a864deb",
    "content": "Query performance within SLA - P99 latency 462ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-23T21:35:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "1cf0cbff_sre_47f7a8f1",
    "content": "Incident: Lock contention causing 52 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-24T01:50:00Z",
      "severity": "critical",
      "incident_id": "1cf0cbff",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 31,
        "transaction_rollback": 177,
        "lock_wait_ms": 4297
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1cf0cbff_dba_17db86e3",
    "content": "CRITICAL: 52 queries waiting, deadlock rate: 30/min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-24T01:51:00Z",
      "severity": "critical",
      "incident_id": "1cf0cbff",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 30,
        "affected_tables": "users",
        "lock_wait_ms": 4297,
        "cascade_duration_min": 3,
        "wait_count": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1cf0cbff_developer_316469cf",
    "content": "Deadlock rate 30/min causing 52 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-24T01:54:00Z",
      "severity": "critical",
      "incident_id": "1cf0cbff",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 31,
        "deadlocks_per_minute": 30,
        "blocked_queries": 52,
        "affected_tables": "users"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1cf0cbff_data_engineer_8d6c198d",
    "content": "Pipeline stalled: 52 queries waiting for 4297ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-24T02:04:00Z",
      "severity": "critical",
      "incident_id": "1cf0cbff",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 3,
        "affected_tables": "users",
        "blocked_queries": 52
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "d5d7c71e_developer_25248961",
    "content": "Application query 'report_aggregate' timeout after 41805ms at 71% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-24T08:17:00Z",
      "severity": "warning",
      "incident_id": "d5d7c71e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 48454736,
        "cpu_usage": 71,
        "wait_count": 5,
        "query_time_after_ms": 41805.878206146444,
        "slowdown_factor": 13
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d5d7c71e_sre_0f42392d",
    "content": "Latency alarm: 'report_aggregate' breached 41805ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-24T08:25:00Z",
      "severity": "critical",
      "incident_id": "d5d7c71e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "cpu_usage": 71,
        "query_time_before_ms": 114.05856890420141,
        "wait_count": 5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d5d7c71e_data_engineer_2bd93af8",
    "content": "ETL bottleneck: 'report_aggregate' step taking 41805ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-24T08:26:00Z",
      "severity": "critical",
      "incident_id": "d5d7c71e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 48454736,
        "affected_query": "report_aggregate",
        "slowdown_factor": 13,
        "cpu_usage": 71
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d5d7c71e_dba_f8a564d3",
    "content": "Sequential scan on 48454736 rows - index 'idx_orders_date' not used, CPU at 71%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-24T08:26:00Z",
      "severity": "critical",
      "incident_id": "d5d7c71e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 48454736,
        "wait_count": 5,
        "query_time_after_ms": 41805.878206146444,
        "query_time_before_ms": 114.05856890420141
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f3a01fe8_data_engineer_909f41ea",
    "content": "Batch job performance degraded - cache hit 37%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-24T12:47:00Z",
      "severity": "critical",
      "incident_id": "f3a01fe8",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 15,
        "buffer_cache_hit": 37,
        "memory_usage_percent": 96
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f3a01fe8_developer_c5b31070",
    "content": "Database swapping 15.8GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-24T12:49:00Z",
      "severity": "warning",
      "incident_id": "f3a01fe8",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 37,
        "swap_usage_gb": 15.8,
        "oom_kills": 4,
        "wait_count": 15
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f3a01fe8_dba_90bdc144",
    "content": "Memory exhaustion: 96% RAM, 15.8GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-24T12:53:00Z",
      "severity": "critical",
      "incident_id": "f3a01fe8",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 15,
        "oom_kills": 4,
        "buffer_cache_hit": 37
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_dd3e1db8e66c4979",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-24T18:57:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_a07311833d364fc5",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-26T17:26:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_11f0828089ea4124",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-26T23:25:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2b195299_developer_df837e32",
    "content": "Read-after-write inconsistency - replica 81s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-28T11:00:00Z",
      "severity": "warning",
      "incident_id": "2b195299",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 1861,
        "lag_seconds": 81,
        "network_latency_ms": 15,
        "replica_count": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "2b195299_dba_9fb41af6",
    "content": "Read replica out of sync by 81 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-28T11:07:00Z",
      "severity": "warning",
      "incident_id": "2b195299",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 1861,
        "replica_count": 4,
        "network_latency_ms": 15,
        "lag_seconds": 81
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "2b195299_data_engineer_03d16c50",
    "content": "Analytics queries on stale data - replica 81s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-28T11:09:00Z",
      "severity": "warning",
      "incident_id": "2b195299",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 81,
        "network_latency_ms": 15,
        "wal_size_mb": 1861
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_fe46e4829a79475a",
    "content": "Index 'idx_products_id' rebuild completed - 74MB size reduction",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-28T12:47:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_aee8d35d9fc240eb",
    "content": "Query cache hit ratio: 85% over last 42 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-28T20:57:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b7df6845_sre_043d021f",
    "content": "Disk usage alert: 'logs' table 60% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-28T22:41:00Z",
      "severity": "warning",
      "incident_id": "b7df6845",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "io_usage_percent": 60,
        "dead_tuples": 11688181,
        "table_bloat_percent": 60,
        "wait_count": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b7df6845_data_engineer_4133c28d",
    "content": "Data load competing with 134min vacuum operation",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-28T22:43:00Z",
      "severity": "warning",
      "incident_id": "b7df6845",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 11688181,
        "table_bloat_percent": 60,
        "io_usage_percent": 60,
        "vacuum_duration_min": 134,
        "tables_affected": "logs"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "bb3bdd50_developer_dfe48c28",
    "content": "TransactionRollback: deadlock after 6151ms on 'payments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-29T03:07:00Z",
      "severity": "warning",
      "incident_id": "bb3bdd50",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 58,
        "affected_tables": "payments",
        "wait_count": 19,
        "lock_wait_ms": 6151
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "bb3bdd50_dba_afb073b2",
    "content": "Circular wait on 'payments' - 6151ms average wait time",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-29T03:09:00Z",
      "severity": "warning",
      "incident_id": "bb3bdd50",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 14,
        "blocked_queries": 58,
        "lock_wait_ms": 6151,
        "wait_count": 19,
        "affected_tables": "payments"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "bb3bdd50_sre_9da6294d",
    "content": "Service disruption: 65 failed transactions in 14min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-29T03:24:00Z",
      "severity": "warning",
      "incident_id": "bb3bdd50",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "lock_wait_ms": 6151,
        "blocked_queries": 58,
        "cascade_duration_min": 14,
        "deadlocks_per_minute": 38
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "bb3bdd50_data_engineer_aafe674d",
    "content": "ETL blocked by deadlocks on 'payments' (38/min)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-29T03:24:00Z",
      "severity": "warning",
      "incident_id": "bb3bdd50",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "deadlocks_per_minute": 38,
        "lock_wait_ms": 6151,
        "transaction_rollback": 65
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_ec4e28d3be6f426b",
    "content": "Buffer cache warmed - 25.1GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-29T04:12:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_9027f2fb77d44607",
    "content": "CPU utilization stable at 55%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-30T09:53:37Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_34e5eded9d694391",
    "content": "Index 'idx_products_date' rebuild completed - 308MB size reduction",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-30T10:42:26Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_1f4212b0bb7040cb",
    "content": "Connection recycling: 80 connections refreshed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-30T13:21:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f0bffb67_dba_c552a537",
    "content": "Connection leak: idle_in_transaction growing at 9/hour for 5h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-30T18:27:00Z",
      "severity": "warning",
      "incident_id": "f0bffb67",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 323,
        "leak_rate_per_hour": 9.4,
        "active_connections": 558.5006105796742
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f0bffb67_sre_228c67f2",
    "content": "Resource alert: 323 idle connections using 148MB RAM",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-30T18:42:00Z",
      "severity": "warning",
      "incident_id": "f0bffb67",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 13,
        "leak_rate_per_hour": 9.4,
        "memory_usage_mb": 148,
        "idle_connections": 323
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "30d7eb58_sre_68874a1e",
    "content": "Emergency response needed: 95% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-31T00:29:00Z",
      "severity": "info",
      "incident_id": "30d7eb58",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 98,
        "wait_count": 23,
        "disk_usage_percent": 95,
        "free_space_gb": 36,
        "growth_rate_gb_per_day": 64
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30d7eb58_dba_3584db1d",
    "content": "Storage alert: Growing 64GB/day, full in 5 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-31T00:31:00Z",
      "severity": "info",
      "incident_id": "30d7eb58",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "free_space_gb": 36,
        "disk_usage_percent": 95,
        "wal_size_gb": 98,
        "growth_rate_gb_per_day": 64
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30d7eb58_developer_d13291a0",
    "content": "Critical: Database disk 95% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-31T00:41:00Z",
      "severity": "info",
      "incident_id": "30d7eb58",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "wait_count": 23,
        "growth_rate_gb_per_day": 64,
        "free_space_gb": 36,
        "wal_size_gb": 98
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30d7eb58_data_engineer_4e6437cc",
    "content": "Batch processing consuming 64GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-31T00:44:00Z",
      "severity": "warning",
      "incident_id": "30d7eb58",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "free_space_gb": 36,
        "wait_count": 23,
        "wal_size_gb": 98,
        "disk_usage_percent": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30160229_dba_2eec3593",
    "content": "Vacuum freeze on 'metrics' - 89% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-31T04:07:00Z",
      "severity": "warning",
      "incident_id": "30160229",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 77168673,
        "vacuum_duration_min": 270,
        "tables_affected": "metrics",
        "io_usage_percent": 89
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30160229_data_engineer_c1896fce",
    "content": "Batch job delayed by autovacuum processing 77168673 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-31T04:08:00Z",
      "severity": "warning",
      "incident_id": "30160229",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 270,
        "tables_affected": "metrics",
        "dead_tuples": 77168673,
        "wait_count": 37,
        "table_bloat_percent": 63
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30160229_developer_7bed68c7",
    "content": "Application slowdown during 270min vacuum on 'metrics'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-31T04:15:00Z",
      "severity": "warning",
      "incident_id": "30160229",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 63,
        "vacuum_duration_min": 270,
        "wait_count": 37,
        "dead_tuples": 77168673,
        "io_usage_percent": 89
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "30160229_sre_9bb0b21b",
    "content": "Performance impact: 270min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-31T04:17:00Z",
      "severity": "warning",
      "incident_id": "30160229",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 270,
        "table_bloat_percent": 63,
        "dead_tuples": 77168673
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ed325db4_dba_3093a1b5",
    "content": "Memory exhaustion: 88% RAM, 12.1GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-07-31T06:43:00Z",
      "severity": "warning",
      "incident_id": "ed325db4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 12.1,
        "wait_count": 38,
        "buffer_cache_hit": 57,
        "memory_usage_percent": 88,
        "work_mem_mb": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ed325db4_developer_cd99eb38",
    "content": "Database swapping 12.1GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-07-31T06:45:00Z",
      "severity": "warning",
      "incident_id": "ed325db4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 8,
        "work_mem_mb": 10,
        "swap_usage_gb": 12.1,
        "buffer_cache_hit": 57
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ed325db4_sre_37c15fde",
    "content": "Critical: Buffer cache efficiency 57% (memory pressure)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-07-31T06:51:00Z",
      "severity": "warning",
      "incident_id": "ed325db4",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 8,
        "buffer_cache_hit": 57,
        "memory_usage_percent": 88,
        "swap_usage_gb": 12.1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_fd41b690e1044f03",
    "content": "Connection recycling: 73 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-07-31T14:40:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_bb12b30217ab41bd",
    "content": "Network latency optimal - 1.9ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-01T02:00:36Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_65e25bf982304d39",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-01T09:30:37Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_9833bf3e5c8149ae",
    "content": "Backup successful: 78.7GB completed in 52 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-01T09:56:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0f9796b8_data_engineer_676a2b59",
    "content": "Data freshness issue: 52730086 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-01T19:06:00Z",
      "severity": "warning",
      "incident_id": "0f9796b8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 34,
        "lag_bytes": 52730086,
        "replica_count": 3,
        "lag_seconds": 72,
        "network_latency_ms": 11
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0f9796b8_sre_b6141bcb",
    "content": "Critical: Replication lag exceeding SLA (72s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-01T19:20:00Z",
      "severity": "warning",
      "incident_id": "0f9796b8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 11,
        "wait_count": 34,
        "wal_size_mb": 671
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fcc27dfe_dba_69350014",
    "content": "pg_stat_activity shows 125 idle connections consuming 256MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-02T14:29:00Z",
      "severity": "critical",
      "incident_id": "fcc27dfe",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 125,
        "max_connections": 1000,
        "active": 635,
        "leak_duration_hours": 2,
        "memory_usage_mb": 256
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "fcc27dfe_data_engineer_eb36500e",
    "content": "Pipeline issue: 125 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-02T14:41:00Z",
      "severity": "critical",
      "incident_id": "fcc27dfe",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 41,
        "max_connections": 1000,
        "active_connections": 635.1253183104837,
        "threshold": 63,
        "leak_duration_hours": 2
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_bf59421f01b74f3f",
    "content": "Database health check passed - response time 52ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-03T09:47:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3669ade430c14955",
    "content": "Backup successful: 40.6GB completed in 53 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-05T03:00:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8bf14657_sre_aef81972",
    "content": "Grafana: Connection leak - 16.0 connections/hour over 22h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-06T04:40:00Z",
      "severity": "critical",
      "incident_id": "8bf14657",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 16.0,
        "idle_connections": 382,
        "wait_count": 16,
        "active": 574
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8bf14657_developer_66f5d3e5",
    "content": "Connection pool growing: 574 active + 382 idle = potential leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-06T04:46:00Z",
      "severity": "warning",
      "incident_id": "8bf14657",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 16.0,
        "threshold": 57,
        "leak_duration_hours": 22,
        "memory_usage_mb": 106
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8bf14657_data_engineer_ce95439e",
    "content": "Batch job leak: 16.0 connections/hour over 22h runtime",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-06T04:54:00Z",
      "severity": "critical",
      "incident_id": "8bf14657",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 574,
        "threshold": 57,
        "max_connections": 1000,
        "memory_usage_mb": 106,
        "leak_rate_per_hour": 16.0
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "17ab0cac_data_engineer_75db187d",
    "content": "DBT run failed: waited 6807ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-06T06:05:00Z",
      "severity": "info",
      "incident_id": "17ab0cac",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 37,
        "threshold": 86,
        "spike_duration_min": 19.5
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "17ab0cac_sre_65e35cf9",
    "content": "Monitoring: Connection pool 86% full, 37 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-06T06:05:00Z",
      "severity": "info",
      "incident_id": "17ab0cac",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 37,
        "spike_duration_min": 19.5,
        "error_code": "PG-53300",
        "wait_ms": 6807
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "17ab0cac_dba_91015142",
    "content": "ERROR: too many connections for role 'app_user' (current: 974, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-06T06:07:00Z",
      "severity": "info",
      "incident_id": "17ab0cac",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "wait_count": 37,
        "active_connections": 974.2143470896892,
        "threshold": 86
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f15193f00c77434b",
    "content": "Backup successful: 41.4GB completed in 10 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-06T06:23:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "caadd56b_dba_02f7c6d0",
    "content": "ERROR: too many connections for role 'app_user' (current: 968, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-06T15:05:00Z",
      "severity": "info",
      "incident_id": "caadd56b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 36.4,
        "active_connections": 968.2052300454128,
        "max_connections": 1000,
        "active": 968,
        "threshold": 89
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "caadd56b_data_engineer_cbb2408c",
    "content": "DBT run failed: waited 10743ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-06T15:12:00Z",
      "severity": "info",
      "incident_id": "caadd56b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 10743,
        "spike_duration_min": 36.4,
        "active": 968,
        "wait_count": 36,
        "wait_time_ms": 10743.169953667857
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "caadd56b_developer_a53b3229",
    "content": "JDBC connection failed after 10743ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-06T15:23:00Z",
      "severity": "info",
      "incident_id": "caadd56b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 10743.169953667857,
        "error_code": "PG-53300",
        "max_connections": 1000,
        "active": 968
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cf9e31bf_dba_84107ee2",
    "content": "Autovacuum running 125min on 'metrics', IO at 50%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-06T19:10:00Z",
      "severity": "info",
      "incident_id": "cf9e31bf",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 67,
        "io_usage_percent": 50,
        "tables_affected": "metrics",
        "wait_count": 33
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "cf9e31bf_developer_2552b4c7",
    "content": "Database maintenance causing 50% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-06T19:14:00Z",
      "severity": "info",
      "incident_id": "cf9e31bf",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 33,
        "vacuum_duration_min": 125,
        "io_usage_percent": 50
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "cf9e31bf_data_engineer_c0ba7550",
    "content": "Table 'metrics' bloat (67%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-06T19:22:00Z",
      "severity": "warning",
      "incident_id": "cf9e31bf",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 67,
        "tables_affected": "metrics",
        "vacuum_duration_min": 125,
        "io_usage_percent": 50
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_50e104e03545466a",
    "content": "Autovacuum completed on table 'users' - 821066 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-07T01:49:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_44a071f65bc249bb",
    "content": "Connection pool stable at 329 connections (53% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-07T17:10:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_8b3aff7ed4254bf5",
    "content": "Autovacuum completed on table 'products' - 285036 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-07T23:53:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_2c52e140cc3b4ad6",
    "content": "Checkpoint completed: 9969 buffers written in 6.9 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-08T01:30:57Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_653136b7b1524cec",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-08T06:20:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_bb77475aa6fe4344",
    "content": "SSL certificate valid for 92 more days",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-08T18:35:24Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_e295d3cf67d14358",
    "content": "WAL archiving on schedule - 77 segments archived",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-09T10:31:46Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee1b8c88_developer_580c995a",
    "content": "Application errors: 167 transactions failed in 9min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-09T11:40:00Z",
      "severity": "info",
      "incident_id": "ee1b8c88",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "inventory",
        "deadlocks_per_minute": 15,
        "blocked_queries": 61,
        "wait_count": 15,
        "cascade_duration_min": 9
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee1b8c88_sre_ae98a0c0",
    "content": "Critical: 15 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-09T11:49:00Z",
      "severity": "info",
      "incident_id": "ee1b8c88",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 167,
        "affected_tables": "inventory",
        "lock_wait_ms": 6466
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_ac77b79290794242",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-09T13:28:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a9e2cfe4_sre_efdc4fa7",
    "content": "Monitoring: Autovacuum causing 57% IO saturation",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-10T01:38:00Z",
      "severity": "info",
      "incident_id": "a9e2cfe4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 26,
        "io_usage_percent": 57,
        "dead_tuples": 59320287,
        "tables_affected": "sessions"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "a9e2cfe4_developer_7f08fb19",
    "content": "Database maintenance causing 57% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-10T01:44:00Z",
      "severity": "info",
      "incident_id": "a9e2cfe4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 322,
        "table_bloat_percent": 26,
        "dead_tuples": 59320287,
        "wait_count": 31
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_207fef0479d54bd6",
    "content": "Backup successful: 6.8GB completed in 48 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-10T13:01:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_a3540cbbd790438a",
    "content": "Buffer cache warmed - 31.4GB loaded into memory",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-10T22:08:54Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_441e0fbc05824bd1",
    "content": "Connection recycling: 58 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-10T22:21:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0786792c_data_engineer_82ef982e",
    "content": "Data processing failed - 9 tasks killed, 8.9GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-11T09:38:00Z",
      "severity": "warning",
      "incident_id": "0786792c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 9,
        "memory_usage_percent": 96,
        "buffer_cache_hit": 48,
        "swap_usage_gb": 8.9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0786792c_dba_f219a88a",
    "content": "FATAL: out of memory - 9 processes killed, 96% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-11T09:43:00Z",
      "severity": "warning",
      "incident_id": "0786792c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 8.9,
        "wait_count": 9,
        "buffer_cache_hit": 48,
        "memory_usage_percent": 96,
        "work_mem_mb": 51
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0786792c_developer_7525a26b",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 48%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-11T09:46:00Z",
      "severity": "warning",
      "incident_id": "0786792c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 9,
        "buffer_cache_hit": 48,
        "work_mem_mb": 51
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_52584ce15e13458c",
    "content": "Disk I/O normal - 5363 read IOPS, 3653 write IOPS",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-12T03:13:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_32d54222c3494c3c",
    "content": "Replication healthy - all 2 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-12T16:55:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94c5952a_sre_fd16fdcd",
    "content": "System alert: 8375ms lock wait impacting 83 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-13T05:00:00Z",
      "severity": "critical",
      "incident_id": "94c5952a",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 8375,
        "deadlocks_per_minute": 31,
        "wait_count": 35,
        "blocked_queries": 41
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "94c5952a_developer_17b5120e",
    "content": "Transaction retry exhausted - 31 deadlocks/min on 'payments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-13T05:07:00Z",
      "severity": "critical",
      "incident_id": "94c5952a",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 14,
        "affected_tables": "payments",
        "transaction_rollback": 83,
        "lock_wait_ms": 8375,
        "deadlocks_per_minute": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "94c5952a_dba_f503a625",
    "content": "Lock escalation: 83 transactions rolled back on 'payments'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-13T05:15:00Z",
      "severity": "critical",
      "incident_id": "94c5952a",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "transaction_rollback": 83,
        "deadlocks_per_minute": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_00623f35e9cb4636",
    "content": "Network latency optimal - 1.7ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-16T03:38:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_aeefe8b72aa14905",
    "content": "SSL certificate valid for 312 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-17T02:28:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "d526005f_data_engineer_d9022698",
    "content": "Data processing failed - 6 tasks killed, 10.0GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-17T14:38:00Z",
      "severity": "critical",
      "incident_id": "d526005f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 85,
        "swap_usage_gb": 10.0,
        "oom_kills": 6,
        "buffer_cache_hit": 47
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d526005f_dba_16bf2fe4",
    "content": "FATAL: out of memory - 6 processes killed, 85% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-17T14:39:00Z",
      "severity": "critical",
      "incident_id": "d526005f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 6,
        "swap_usage_gb": 10.0,
        "memory_usage_percent": 85,
        "work_mem_mb": 36,
        "wait_count": 45
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_9464b3dbbabc4708",
    "content": "Backup successful: 97.4GB completed in 44 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-17T15:01:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_95e47136ccf1424c",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-17T19:08:21Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_21e9bc9f790b438b",
    "content": "Autovacuum completed on table 'events' - 617817 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-17T23:30:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f6766773_sre_b3a6a70a",
    "content": "Critical: Replication lag exceeding SLA (87s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-18T14:26:00Z",
      "severity": "critical",
      "incident_id": "f6766773",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 87,
        "lag_bytes": 65999096,
        "wait_count": 36,
        "wal_size_mb": 4611,
        "replica_count": 2
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f6766773_data_engineer_71464387",
    "content": "Data freshness issue: 65999096 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-18T14:31:00Z",
      "severity": "critical",
      "incident_id": "f6766773",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 65999096,
        "replica_count": 2,
        "wal_size_mb": 4611,
        "wait_count": 36,
        "network_latency_ms": 49
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f6766773_dba_4b122b3e",
    "content": "WARNING: Replica 2 lagging 87s, WAL 4611MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-18T14:39:00Z",
      "severity": "critical",
      "incident_id": "f6766773",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 36,
        "replica_count": 2,
        "lag_bytes": 65999096
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_4791b283b0074f60",
    "content": "SSL certificate valid for 332 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-19T21:25:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_908fe3353403411c",
    "content": "Network latency optimal - 3.8ms average RTT",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-19T21:37:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "91a4d15f_data_engineer_1870fff5",
    "content": "Analytics queries on stale data - replica 18s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-20T08:44:00Z",
      "severity": "warning",
      "incident_id": "91a4d15f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 58341448,
        "replica_count": 1,
        "wal_size_mb": 3835,
        "network_latency_ms": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "91a4d15f_sre_67b2100a",
    "content": "Alert: WAL accumulation 3835MB, lag 18s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-20T08:49:00Z",
      "severity": "info",
      "incident_id": "91a4d15f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 25,
        "replica_count": 1,
        "network_latency_ms": 7,
        "lag_seconds": 18,
        "wal_size_mb": 3835
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "91a4d15f_dba_a5247543",
    "content": "Read replica out of sync by 18 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-20T08:49:00Z",
      "severity": "info",
      "incident_id": "91a4d15f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 25,
        "network_latency_ms": 7,
        "lag_seconds": 18,
        "wal_size_mb": 3835,
        "replica_count": 1
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "91a4d15f_developer_7a3652bd",
    "content": "Read-after-write inconsistency - replica 18s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-20T08:54:00Z",
      "severity": "info",
      "incident_id": "91a4d15f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 25,
        "network_latency_ms": 7,
        "replica_count": 1,
        "wal_size_mb": 3835,
        "lag_bytes": 58341448
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "66e89858_dba_465a68fa",
    "content": "Disk space critical: 85% used, growth rate 63GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-20T19:27:00Z",
      "severity": "warning",
      "incident_id": "66e89858",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 91,
        "wait_count": 27,
        "free_space_gb": 17,
        "growth_rate_gb_per_day": 63,
        "days_until_full": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "66e89858_data_engineer_da21f406",
    "content": "ETL jobs at risk - disk 85% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-20T19:28:00Z",
      "severity": "warning",
      "incident_id": "66e89858",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 4,
        "free_space_gb": 17,
        "growth_rate_gb_per_day": 63,
        "disk_usage_percent": 85,
        "wait_count": 27
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_da7f3842bf884b17",
    "content": "Database health check passed - response time 39ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-20T23:40:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0fa388c2_sre_cdb851df",
    "content": "Alert: WAL accumulation 4852MB, lag 294s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-22T10:00:00Z",
      "severity": "info",
      "incident_id": "0fa388c2",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 43838859,
        "wait_count": 13,
        "lag_seconds": 294,
        "wal_size_mb": 4852
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0fa388c2_developer_68e5c458",
    "content": "Read-after-write inconsistency - replica 294s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-22T10:06:00Z",
      "severity": "info",
      "incident_id": "0fa388c2",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "wal_size_mb": 4852,
        "lag_bytes": 43838859
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0fa388c2_dba_89822970",
    "content": "Read replica out of sync by 294 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-22T10:09:00Z",
      "severity": "info",
      "incident_id": "0fa388c2",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 4852,
        "lag_bytes": 43838859,
        "network_latency_ms": 45
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_93972bb55e744241",
    "content": "Connection recycling: 56 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-24T00:10:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_38105f036ecd4497",
    "content": "Database health check passed - response time 133ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-24T05:59:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_b4c6d90be5c94d10",
    "content": "Connection recycling: 56 connections refreshed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-24T08:49:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_31100e660e9e4b2e",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-24T10:46:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_19b9e4e5e9964484",
    "content": "Autovacuum completed on table 'events' - 591153 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-24T23:04:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_98ddd712b97c46cf",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-25T13:05:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_20451a9894e441bb",
    "content": "Index 'idx_orders_id' rebuild completed - 25MB size reduction",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-27T05:19:29Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_9f96b4d91cf947b9",
    "content": "Query performance within SLA - P99 latency 246ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-08-27T08:31:29Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f83c472a38904070",
    "content": "Network latency optimal - 4.5ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-27T21:06:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_61f096a43a1b4248",
    "content": "WAL archiving on schedule - 20 segments archived",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-28T08:09:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_6b0c9f70bced4b51",
    "content": "Connection recycling: 49 connections refreshed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-29T09:00:07Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_b895d78db5d14df9",
    "content": "Checkpoint completed: 1527 buffers written in 5.4 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-29T12:02:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_dd8b6c83687c4a25",
    "content": "Checkpoint completed: 23493 buffers written in 7.7 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-29T21:13:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e315e7fe_dba_6a837a73",
    "content": "Circular wait on 'shipments' - 8477ms average wait time",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-08-30T13:01:00Z",
      "severity": "info",
      "incident_id": "e315e7fe",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "transaction_rollback": 122,
        "lock_wait_ms": 8477
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e315e7fe_sre_f587f950",
    "content": "Service disruption: 122 failed transactions in 13min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-08-30T13:10:00Z",
      "severity": "info",
      "incident_id": "e315e7fe",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 8477,
        "blocked_queries": 82,
        "deadlocks_per_minute": 29,
        "affected_tables": "shipments"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_5ff19f1373ca4278",
    "content": "Connection pool stable at 275 connections (55% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-08-30T14:25:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_fe1defcdf12847f8",
    "content": "Index 'idx_orders_date' rebuild completed - 373MB size reduction",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-01T11:00:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d200faad_dba_f82cac27",
    "content": "PANIC: connection limit exceeded - 981/1000 after 22.2min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-02T18:24:00Z",
      "severity": "critical",
      "incident_id": "d200faad",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 26,
        "wait_ms": 13287,
        "threshold": 89,
        "wait_time_ms": 13287.689099012425,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d200faad_sre_30b62ff9",
    "content": "AWS RDS Alert: Connection count 981, wait time 13287ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-02T18:33:00Z",
      "severity": "critical",
      "incident_id": "d200faad",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 26,
        "max_connections": 1000,
        "active_connections": 981.9420284383278,
        "threshold": 89
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d200faad_developer_14cf7d53",
    "content": "Connection pool exhausted - 26 threads waiting, 981 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-02T18:40:00Z",
      "severity": "critical",
      "incident_id": "d200faad",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 22.2,
        "wait_ms": 13287,
        "error_code": "PG-53300",
        "max_connections": 1000,
        "wait_count": 26
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8516308e_developer_b8b2f017",
    "content": "JDBC connection failed after 13167ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-02T22:02:00Z",
      "severity": "warning",
      "incident_id": "8516308e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 22.4,
        "max_connections": 1000,
        "error_code": "PG-53300",
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8516308e_dba_6b7b4581",
    "content": "ERROR: too many connections for role 'app_user' (current: 963, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-02T22:10:00Z",
      "severity": "warning",
      "incident_id": "8516308e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 963,
        "wait_time_ms": 13167.101517435953,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8516308e_sre_8f888057",
    "content": "Monitoring: Connection pool 88% full, 9 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-02T22:14:00Z",
      "severity": "warning",
      "incident_id": "8516308e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 963.1496970485205,
        "wait_count": 9,
        "wait_ms": 13167,
        "active": 963
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8516308e_data_engineer_c8bb7751",
    "content": "DBT run failed: waited 13167ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-02T22:15:00Z",
      "severity": "warning",
      "incident_id": "8516308e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 963,
        "threshold": 88,
        "spike_duration_min": 22.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "14d30e0a_developer_bb1d9b1f",
    "content": "Database storage 94% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-03T19:59:00Z",
      "severity": "critical",
      "incident_id": "14d30e0a",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 63,
        "wait_count": 25,
        "wal_size_gb": 61
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "14d30e0a_dba_cb806dd5",
    "content": "Disk space critical: 94% used, growth rate 63GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-03T20:02:00Z",
      "severity": "critical",
      "incident_id": "14d30e0a",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 94,
        "days_until_full": 4,
        "free_space_gb": 46,
        "wait_count": 25,
        "growth_rate_gb_per_day": 63
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "14d30e0a_data_engineer_cf63752a",
    "content": "Data retention issue: 61GB logs, 94% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-03T20:07:00Z",
      "severity": "critical",
      "incident_id": "14d30e0a",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 94,
        "days_until_full": 4,
        "wait_count": 25,
        "free_space_gb": 46
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "14d30e0a_sre_3db6821b",
    "content": "Projection: Storage full in 4 days (63GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-03T20:11:00Z",
      "severity": "critical",
      "incident_id": "14d30e0a",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 4,
        "wal_size_gb": 61,
        "disk_usage_percent": 94,
        "free_space_gb": 46,
        "growth_rate_gb_per_day": 63
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "77c391ed_developer_f88b8395",
    "content": "Application slowdown during 228min vacuum on 'metrics'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-04T04:44:00Z",
      "severity": "critical",
      "incident_id": "77c391ed",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "metrics",
        "io_usage_percent": 73,
        "table_bloat_percent": 26
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "77c391ed_data_engineer_b5e8fea4",
    "content": "Batch job delayed by autovacuum processing 50624863 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-04T04:49:00Z",
      "severity": "critical",
      "incident_id": "77c391ed",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 34,
        "vacuum_duration_min": 228,
        "tables_affected": "metrics"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "3cf3d248_data_engineer_bde2ed60",
    "content": "Data freshness issue: 44243062 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-04T16:45:00Z",
      "severity": "warning",
      "incident_id": "3cf3d248",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 44243062,
        "wal_size_mb": 2692,
        "wait_count": 45,
        "lag_seconds": 36,
        "replica_count": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3cf3d248_sre_bbf781e5",
    "content": "Critical: Replication lag exceeding SLA (36s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-04T16:55:00Z",
      "severity": "warning",
      "incident_id": "3cf3d248",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 45,
        "network_latency_ms": 36,
        "lag_bytes": 44243062,
        "replica_count": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3cf3d248_dba_a5a4f2b6",
    "content": "WARNING: Replica 1 lagging 36s, WAL 2692MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-04T16:59:00Z",
      "severity": "warning",
      "incident_id": "3cf3d248",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 2692,
        "lag_seconds": 36,
        "network_latency_ms": 36,
        "replica_count": 1,
        "lag_bytes": 44243062
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1b78190cab1e4e67",
    "content": "Query cache hit ratio: 90% over last 6 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-05T23:03:05Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_f667a87eea994ec1",
    "content": "Memory usage normal - 87.2GB of 128GB used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-06T11:25:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_945d8273064e42c5",
    "content": "WAL archiving on schedule - 98 segments archived",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-07T03:50:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_55107a94ce344f98",
    "content": "Checkpoint completed: 4761 buffers written in 4.6 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-07T04:37:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_13efa0d5cab54c23",
    "content": "Memory usage normal - 40.7GB of 128GB used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-07T13:53:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d9862664ac5b4f47",
    "content": "Replication healthy - all 2 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-07T16:30:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d8eee681_developer_67566b32",
    "content": "Application query 'order_search' timeout after 45752ms at 92% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-07T18:41:00Z",
      "severity": "info",
      "incident_id": "d8eee681",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 92,
        "query_time_after_ms": 45752.8319755041,
        "affected_query": "order_search",
        "slowdown_factor": 123
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d8eee681_data_engineer_5be8760a",
    "content": "ETL bottleneck: 'order_search' step taking 45752ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-07T18:51:00Z",
      "severity": "info",
      "incident_id": "d8eee681",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 92,
        "query_time_before_ms": 64.27241414626434,
        "slowdown_factor": 123
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d8eee681_dba_be549ed9",
    "content": "Sequential scan on 32182590 rows - index 'idx_orders_date' not used, CPU at 92%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-07T18:54:00Z",
      "severity": "info",
      "incident_id": "d8eee681",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 64.27241414626434,
        "wait_count": 18,
        "cpu_usage": 92,
        "rows_scanned": 32182590,
        "affected_query": "order_search"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_b6620ab4621845c7",
    "content": "Buffer cache warmed - 3.9GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-08T00:28:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_b4762e02514b49e9",
    "content": "Autovacuum completed on table 'users' - 341184 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-08T03:36:20Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_f04a059166c847de",
    "content": "Backup successful: 38.6GB completed in 18 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-08T06:02:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "bcda3933_developer_08f31fd8",
    "content": "Write failures imminent - 19GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-08T23:14:00Z",
      "severity": "info",
      "incident_id": "bcda3933",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 89,
        "wait_count": 50,
        "growth_rate_gb_per_day": 32,
        "free_space_gb": 19
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "bcda3933_sre_093df8bc",
    "content": "Disk space alarm: 88% used, 19GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-08T23:17:00Z",
      "severity": "info",
      "incident_id": "bcda3933",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 32,
        "free_space_gb": 19,
        "wait_count": 50
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0aceb59a50724448",
    "content": "Query performance within SLA - P99 latency 364ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-08T23:58:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_d4df80653cad4f79",
    "content": "Query cache hit ratio: 92% over last 44 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-09T05:46:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_3f68059f92a44528",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-10T10:57:40Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_a974f92330a7472d",
    "content": "Database health check passed - response time 169ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-10T12:08:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_515c8cebba6640c0",
    "content": "Network latency optimal - 1.4ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-11T07:44:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_e63dc95350614f7b",
    "content": "Autovacuum completed on table 'sessions' - 67421 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-11T11:57:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3efd612665514ca7",
    "content": "Memory usage normal - 94.5GB of 128GB used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-12T01:27:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_943fe235e4e140c4",
    "content": "WAL archiving on schedule - 27 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-12T04:43:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_2ae9c67bb37b442a",
    "content": "WAL archiving on schedule - 20 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-12T19:28:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_871cdfc5900f433e",
    "content": "Checkpoint completed: 38990 buffers written in 1.1 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-13T04:18:45Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_48e73df5aa5743fc",
    "content": "Database health check passed - response time 49ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-13T10:13:20Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "5dea8978_sre_fab27b8c",
    "content": "Disk space alarm: 96% used, 31GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-13T14:55:00Z",
      "severity": "info",
      "incident_id": "5dea8978",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "wait_count": 42,
        "wal_size_gb": 10,
        "disk_usage_percent": 96,
        "growth_rate_gb_per_day": 12
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5dea8978_dba_0fbaa8d3",
    "content": "Emergency: 31GB free, database will halt in 5 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-13T14:58:00Z",
      "severity": "info",
      "incident_id": "5dea8978",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "free_space_gb": 31,
        "disk_usage_percent": 96
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5dea8978_data_engineer_5bb1dcf1",
    "content": "Data retention issue: 10GB logs, 96% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-13T15:05:00Z",
      "severity": "warning",
      "incident_id": "5dea8978",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 10,
        "growth_rate_gb_per_day": 12,
        "wait_count": 42,
        "days_until_full": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_82041f971c9a475a",
    "content": "CPU utilization stable at 47%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-14T03:58:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "70028d41_sre_7e9add6b",
    "content": "Linear growth: 358 idle connections, 283MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-14T22:04:00Z",
      "severity": "warning",
      "incident_id": "70028d41",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 358,
        "max_connections": 1000,
        "leak_rate_per_hour": 10.6,
        "active_connections": 538.5008728670625
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "70028d41_dba_3a60bb54",
    "content": "Connection accumulation: 10.6 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-14T22:13:00Z",
      "severity": "warning",
      "incident_id": "70028d41",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active": 538,
        "idle_connections": 358,
        "leak_rate_per_hour": 10.6,
        "memory_usage_mb": 283
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "70028d41_developer_844e89ce",
    "content": "Connection pool: 358 idle, 538 active - possible 283MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-14T22:13:00Z",
      "severity": "warning",
      "incident_id": "70028d41",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 27,
        "leak_rate_per_hour": 10.6,
        "memory_usage_mb": 283
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "55adf594_developer_f27d63a6",
    "content": "JDBC connection failed after 24794ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-15T08:49:00Z",
      "severity": "warning",
      "incident_id": "55adf594",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 90,
        "wait_ms": 24794
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "55adf594_data_engineer_6479bed0",
    "content": "DBT run failed: waited 24794ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-15T08:55:00Z",
      "severity": "critical",
      "incident_id": "55adf594",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 961.3301295594732,
        "spike_duration_min": 11.9,
        "max_connections": 1000,
        "wait_time_ms": 24794.315274188095,
        "threshold": 90
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "55adf594_dba_975d030f",
    "content": "ERROR: too many connections for role 'app_user' (current: 961, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-15T08:58:00Z",
      "severity": "critical",
      "incident_id": "55adf594",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 24794.315274188095,
        "max_connections": 1000,
        "active": 961,
        "wait_ms": 24794,
        "active_connections": 961.3301295594732
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_c6d875cc022a410a",
    "content": "Database health check passed - response time 121ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-15T20:03:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_68b12e74d8bc4147",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-16T19:28:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_652cfd34c87943b8",
    "content": "Connection recycling: 32 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-18T11:15:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_a6a417b7325f4417",
    "content": "WAL archiving on schedule - 59 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-19T04:28:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1a046bc7_developer_7216a7ac",
    "content": "Database swapping 6.4GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-19T10:37:00Z",
      "severity": "warning",
      "incident_id": "1a046bc7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 37,
        "work_mem_mb": 55,
        "oom_kills": 5,
        "swap_usage_gb": 6.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "1a046bc7_sre_2f821158",
    "content": "System swapping 6.4GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-19T10:47:00Z",
      "severity": "critical",
      "incident_id": "1a046bc7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 6.4,
        "work_mem_mb": 55,
        "oom_kills": 5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "1a046bc7_dba_813d738e",
    "content": "Memory exhaustion: 95% RAM, 6.4GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-19T10:50:00Z",
      "severity": "critical",
      "incident_id": "1a046bc7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 5,
        "wait_count": 11,
        "work_mem_mb": 55,
        "swap_usage_gb": 6.4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e6a0ad7c_sre_356c3aa4",
    "content": "Alert: Connection saturation 974/1000 for 28 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-20T02:07:00Z",
      "severity": "info",
      "incident_id": "e6a0ad7c",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 28.2,
        "active": 974,
        "max_connections": 1000,
        "threshold": 90,
        "wait_count": 14
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "e6a0ad7c_developer_9d0ec3a2",
    "content": "HikariPool-1 - Connection timeout after 20331ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-20T02:14:00Z",
      "severity": "info",
      "incident_id": "e6a0ad7c",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 20331.498605383218,
        "spike_duration_min": 28.2,
        "wait_count": 14,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_1d1ffd5b7e9745ed",
    "content": "Network latency optimal - 0.8ms average RTT",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-20T17:58:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_6382831e0ca54606",
    "content": "Backup successful: 70.2GB completed in 32 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-21T00:17:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_138ee829443b46a3",
    "content": "Database health check passed - response time 97ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-21T00:36:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_d5482dd292f14b07",
    "content": "Autovacuum completed on table 'products' - 157901 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-21T08:05:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a7d1c3bf_sre_6a7c816d",
    "content": "Replication monitoring: 255s lag on 2 replicas",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-22T23:19:00Z",
      "severity": "warning",
      "incident_id": "a7d1c3bf",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "wal_size_mb": 2967,
        "network_latency_ms": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a7d1c3bf_dba_ef14f0e3",
    "content": "Standby lag alert: 48005603 bytes behind, 2967MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-22T23:19:00Z",
      "severity": "warning",
      "incident_id": "a7d1c3bf",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 48005603,
        "wal_size_mb": 2967,
        "lag_seconds": 255
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_53f2fe664ee74038",
    "content": "SSL certificate valid for 165 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-23T17:07:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_efb1a536102e40ef",
    "content": "CPU utilization stable at 50%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-24T04:56:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "7ac12359_developer_e4ffb329",
    "content": "Application queries failing - database memory at 98%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-24T06:22:00Z",
      "severity": "info",
      "incident_id": "7ac12359",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 6,
        "swap_usage_gb": 14.1,
        "work_mem_mb": 54
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7ac12359_data_engineer_7df9ed0f",
    "content": "Data processing failed - 9 tasks killed, 14.1GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-24T06:25:00Z",
      "severity": "info",
      "incident_id": "7ac12359",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 6,
        "memory_usage_percent": 98,
        "buffer_cache_hit": 60,
        "work_mem_mb": 54
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7ac12359_sre_046b0446",
    "content": "Resource exhaustion: 9 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-24T06:36:00Z",
      "severity": "info",
      "incident_id": "7ac12359",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 98,
        "buffer_cache_hit": 60,
        "work_mem_mb": 54,
        "swap_usage_gb": 14.1,
        "oom_kills": 9
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "631bba94_sre_2ec16db4",
    "content": "Alert: Deadlock storm - 10/min affecting 70 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-24T21:45:00Z",
      "severity": "critical",
      "incident_id": "631bba94",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 12,
        "blocked_queries": 70,
        "deadlocks_per_minute": 10,
        "affected_tables": "orders"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "631bba94_data_engineer_513a3246",
    "content": "Ingestion blocked: 10/min deadlock rate on 'orders'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-24T21:47:00Z",
      "severity": "critical",
      "incident_id": "631bba94",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 6849,
        "blocked_queries": 70,
        "transaction_rollback": 49,
        "cascade_duration_min": 12
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "631bba94_dba_ad36e073",
    "content": "Deadlock storm: 70 blocked, 10/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-24T21:52:00Z",
      "severity": "critical",
      "incident_id": "631bba94",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 16,
        "cascade_duration_min": 12,
        "deadlocks_per_minute": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f983158d_dba_63e2bc27",
    "content": "Query regression: 'report_aggregate' from 177ms to 24324ms (236x slower)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-25T13:06:00Z",
      "severity": "info",
      "incident_id": "f983158d",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 177.66140897583634,
        "wait_count": 35,
        "affected_query": "report_aggregate",
        "rows_scanned": 33147481,
        "query_time_after_ms": 24324.848307393746
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f983158d_data_engineer_5193e540",
    "content": "Dashboard failing: report_aggregate query 236x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-25T13:12:00Z",
      "severity": "info",
      "incident_id": "f983158d",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 35,
        "cpu_usage": 81,
        "slowdown_factor": 236
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f983158d_developer_2217a0ee",
    "content": "User-facing slowdown: report_aggregate response time degraded 236x",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-25T13:12:00Z",
      "severity": "info",
      "incident_id": "f983158d",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 81,
        "query_time_before_ms": 177.66140897583634,
        "slowdown_factor": 236,
        "rows_scanned": 33147481,
        "affected_query": "report_aggregate"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f983158d_sre_bf6a271b",
    "content": "Performance degradation: 236x slowdown on 'report_aggregate' queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-25T13:20:00Z",
      "severity": "info",
      "incident_id": "f983158d",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 81,
        "wait_count": 35,
        "query_time_after_ms": 24324.848307393746,
        "affected_query": "report_aggregate",
        "slowdown_factor": 236
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_3c302bbedb624fc6",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-25T16:12:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_a6584227a97d42d5",
    "content": "Disk I/O normal - 8002 read IOPS, 2774 write IOPS",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-26T01:35:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_64e1df6a1d1042b8",
    "content": "Query cache hit ratio: 87% over last 19 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-27T01:12:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_8b5f223e87034b99",
    "content": "Backup successful: 92.2GB completed in 26 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-09-28T01:09:27Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_9ab725aa18e64469",
    "content": "Buffer cache warmed - 20.0GB loaded into memory",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-28T02:58:54Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_7d20ac01383f48b7",
    "content": "Connection pool stable at 206 connections (30% utilization)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-29T07:53:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_2789ef4b659b4b06",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-29T23:24:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_fe4b4dd4bafd4828",
    "content": "WAL archiving on schedule - 56 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-30T03:12:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "b7929a49_data_engineer_949fd937",
    "content": "ETL job killed: OOM at 96% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-09-30T17:42:00Z",
      "severity": "critical",
      "incident_id": "b7929a49",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 54,
        "swap_usage_gb": 9.1,
        "memory_usage_percent": 96
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b7929a49_developer_7141166c",
    "content": "Database OOM: 9 connections terminated at 96% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-09-30T17:42:00Z",
      "severity": "warning",
      "incident_id": "b7929a49",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 36,
        "wait_count": 34,
        "memory_usage_percent": 96,
        "buffer_cache_hit": 54
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b7929a49_sre_bb5a0a27",
    "content": "System swapping 9.1GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-09-30T17:43:00Z",
      "severity": "critical",
      "incident_id": "b7929a49",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 96,
        "oom_kills": 9,
        "buffer_cache_hit": 54
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_b835d616c1284459",
    "content": "Database health check passed - response time 185ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-01T21:43:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "43c77727_data_engineer_0338fa64",
    "content": "Spark executors holding 316 idle connections, 333MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-02T05:25:00Z",
      "severity": "warning",
      "incident_id": "43c77727",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 536,
        "active_connections": 536.045003480637,
        "threshold": 53,
        "idle_connections": 316,
        "leak_duration_hours": 18
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "43c77727_developer_74b5d36e",
    "content": "WARNING: 316 connections not returned to pool after 18h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-02T05:28:00Z",
      "severity": "warning",
      "incident_id": "43c77727",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 24,
        "idle_connections": 316,
        "active": 536
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "43c77727_sre_2a8c5c06",
    "content": "Anomaly: Idle connections increasing 17/hour for past 18 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-02T05:44:00Z",
      "severity": "warning",
      "incident_id": "43c77727",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 316,
        "leak_duration_hours": 18,
        "wait_count": 24
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "156971e7_sre_cc55f96f",
    "content": "Critical infrastructure alert: WAL using 61GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-03T05:14:00Z",
      "severity": "warning",
      "incident_id": "156971e7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 49,
        "days_until_full": 4,
        "disk_usage_percent": 90,
        "wait_count": 22
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "156971e7_developer_cf6bfd0c",
    "content": "Application at risk: only 49GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-03T05:14:00Z",
      "severity": "warning",
      "incident_id": "156971e7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 49,
        "free_space_gb": 49,
        "days_until_full": 4,
        "disk_usage_percent": 90,
        "wait_count": 22
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "156971e7_data_engineer_55bbe28e",
    "content": "Data pipeline may fail: only 49GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-03T05:21:00Z",
      "severity": "warning",
      "incident_id": "156971e7",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 90,
        "growth_rate_gb_per_day": 49,
        "wal_size_gb": 61,
        "wait_count": 22
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "24e5d615_dba_7b8095f4",
    "content": "pg_stat_activity shows 194 idle connections consuming 171MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-04T12:53:00Z",
      "severity": "info",
      "incident_id": "24e5d615",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 171,
        "leak_duration_hours": 23,
        "idle_connections": 194
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "24e5d615_developer_ec13f5a4",
    "content": "Application holding 194 unused connections for 23 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-04T12:58:00Z",
      "severity": "info",
      "incident_id": "24e5d615",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 15,
        "memory_usage_mb": 171,
        "leak_duration_hours": 23
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "24e5d615_sre_08ba8b5d",
    "content": "Connection leak: 10.1/hr growth rate detected over 23h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-04T13:03:00Z",
      "severity": "info",
      "incident_id": "24e5d615",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 59,
        "wait_count": 15,
        "active": 599,
        "active_connections": 599.0584646035543
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "24e5d615_data_engineer_8c37898b",
    "content": "Pipeline issue: 194 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-04T13:05:00Z",
      "severity": "info",
      "incident_id": "24e5d615",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 59,
        "memory_usage_mb": 171,
        "wait_count": 15,
        "max_connections": 1000,
        "leak_rate_per_hour": 10.1
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "23afddbb_dba_5acfa7cd",
    "content": "Replication lag: 121s behind primary, 62349222 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-04T21:29:00Z",
      "severity": "info",
      "incident_id": "23afddbb",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 3451,
        "replica_count": 2,
        "network_latency_ms": 23,
        "lag_seconds": 121,
        "lag_bytes": 62349222
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "23afddbb_data_engineer_5a5f2a9c",
    "content": "Report inconsistencies due to 121s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-04T21:37:00Z",
      "severity": "info",
      "incident_id": "23afddbb",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 16,
        "network_latency_ms": 23,
        "lag_bytes": 62349222
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_52004c07e93c4a23",
    "content": "Network latency optimal - 4.1ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-06T03:20:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_79362d0fe2b34e27",
    "content": "Query cache hit ratio: 94% over last 44 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-06T17:49:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_c74d61b6cf4e461f",
    "content": "WAL archiving on schedule - 69 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-08T15:39:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8f51d107_sre_33a58587",
    "content": "Monitoring: Connection pool 97% full, 20 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-09T03:49:00Z",
      "severity": "warning",
      "incident_id": "8f51d107",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "spike_duration_min": 12.7,
        "wait_ms": 22397,
        "error_code": "PG-53300",
        "wait_time_ms": 22397.816557479608
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8f51d107_dba_7054560e",
    "content": "ERROR: too many connections for role 'app_user' (current: 994, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-09T03:52:00Z",
      "severity": "warning",
      "incident_id": "8f51d107",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 97,
        "error_code": "PG-53300",
        "active_connections": 994.6133483302581,
        "wait_time_ms": 22397.816557479608,
        "spike_duration_min": 12.7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8f51d107_developer_0553f60e",
    "content": "JDBC connection failed after 22397ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-09T03:55:00Z",
      "severity": "warning",
      "incident_id": "8f51d107",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "max_connections": 1000,
        "wait_time_ms": 22397.816557479608,
        "spike_duration_min": 12.7,
        "wait_count": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0ca11680_sre_114a9593",
    "content": "Performance degradation: 63x slowdown on 'inventory_check' queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-09T13:03:00Z",
      "severity": "warning",
      "incident_id": "0ca11680",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "inventory_check",
        "rows_scanned": 12539387,
        "query_time_before_ms": 115.06058717320371,
        "cpu_usage": 82
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ca11680_dba_08d8812c",
    "content": "Query regression: 'inventory_check' from 115ms to 17593ms (63x slower)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-09T13:05:00Z",
      "severity": "warning",
      "incident_id": "0ca11680",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 115.06058717320371,
        "wait_count": 11,
        "slowdown_factor": 63,
        "query_time_after_ms": 17593.415024292728
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ca11680_data_engineer_899ee92b",
    "content": "Dashboard failing: inventory_check query 63x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-09T13:05:00Z",
      "severity": "warning",
      "incident_id": "0ca11680",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 82,
        "query_time_before_ms": 115.06058717320371,
        "affected_query": "inventory_check",
        "query_time_after_ms": 17593.415024292728,
        "rows_scanned": 12539387
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ca11680_developer_308540f2",
    "content": "User-facing slowdown: inventory_check response time degraded 63x",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-09T13:16:00Z",
      "severity": "warning",
      "incident_id": "0ca11680",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 11,
        "query_time_after_ms": 17593.415024292728,
        "cpu_usage": 82,
        "slowdown_factor": 63,
        "rows_scanned": 12539387
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d21d0d5b_dba_e192749b",
    "content": "CRITICAL: Disk 93% full, only 13GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-10T23:53:00Z",
      "severity": "critical",
      "incident_id": "d21d0d5b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 93,
        "days_until_full": 3,
        "free_space_gb": 13,
        "wal_size_gb": 59
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d21d0d5b_developer_7e809701",
    "content": "Application at risk: only 13GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-11T00:01:00Z",
      "severity": "warning",
      "incident_id": "d21d0d5b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 59,
        "days_until_full": 3,
        "growth_rate_gb_per_day": 27,
        "wait_count": 49,
        "free_space_gb": 13
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "d21d0d5b_sre_0a0e78dd",
    "content": "Critical infrastructure alert: WAL using 59GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-11T00:07:00Z",
      "severity": "critical",
      "incident_id": "d21d0d5b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 3,
        "growth_rate_gb_per_day": 27,
        "wait_count": 49,
        "disk_usage_percent": 93,
        "wal_size_gb": 59
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_62168f60d3484f84",
    "content": "Connection recycling: 69 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-11T02:06:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "3313dff4_developer_d9398c0a",
    "content": "User queries slow - autovacuum processing 2377737 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-13T07:44:00Z",
      "severity": "info",
      "incident_id": "3313dff4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 28,
        "tables_affected": "sessions",
        "io_usage_percent": 85,
        "dead_tuples": 2377737
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "3313dff4_sre_b02452e6",
    "content": "Maintenance window exceeded - vacuum still running after 158min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-13T07:45:00Z",
      "severity": "info",
      "incident_id": "3313dff4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 2377737,
        "vacuum_duration_min": 158,
        "table_bloat_percent": 55,
        "io_usage_percent": 85
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "3313dff4_data_engineer_9058f7a6",
    "content": "Pipeline slowdown - 85% IO used by maintenance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-13T07:46:00Z",
      "severity": "info",
      "incident_id": "3313dff4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 28,
        "tables_affected": "sessions",
        "dead_tuples": 2377737
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7597cd5be109411f",
    "content": "CPU utilization stable at 26%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-13T10:47:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "07ae54e9_developer_9a5b051e",
    "content": "Application errors: 172 transactions failed in 1min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-13T17:01:00Z",
      "severity": "info",
      "incident_id": "07ae54e9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 1,
        "lock_wait_ms": 9600,
        "affected_tables": "orders",
        "blocked_queries": 96
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "07ae54e9_sre_0da4646e",
    "content": "Critical: 23 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-13T17:12:00Z",
      "severity": "info",
      "incident_id": "07ae54e9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 1,
        "blocked_queries": 96,
        "wait_count": 25,
        "affected_tables": "orders"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "07ae54e9_data_engineer_af603134",
    "content": "Data load failing: 172 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-13T17:15:00Z",
      "severity": "warning",
      "incident_id": "07ae54e9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 96,
        "wait_count": 25,
        "cascade_duration_min": 1,
        "deadlocks_per_minute": 23
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "07ae54e9_dba_2d5c97fe",
    "content": "Deadlock cascade on 'orders' - 23/min for 1min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-13T17:20:00Z",
      "severity": "info",
      "incident_id": "07ae54e9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 96,
        "cascade_duration_min": 1,
        "affected_tables": "orders",
        "deadlocks_per_minute": 23
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_a0290806735d4a68",
    "content": "Connection pool stable at 279 connections (16% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-14T22:14:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0ba02850_sre_196bdec4",
    "content": "Linear growth: 262 idle connections, 339MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-15T07:00:00Z",
      "severity": "warning",
      "incident_id": "0ba02850",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 339,
        "leak_duration_hours": 10,
        "max_connections": 1000,
        "active_connections": 587.8248654509825,
        "idle_connections": 262
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0ba02850_data_engineer_df0b1776",
    "content": "ETL connection pool: 262 idle connections leaking at 6/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-15T07:05:00Z",
      "severity": "warning",
      "incident_id": "0ba02850",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 6.2,
        "max_connections": 1000,
        "active": 587,
        "wait_count": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_0639e3daa1024e96",
    "content": "Statistics updated for 28 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-16T08:37:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_1be5d6a374554781",
    "content": "Query performance within SLA - P99 latency 349ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-16T20:35:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "59064391_data_engineer_107aff9e",
    "content": "Pipeline slowdown - 61% IO used by maintenance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-17T09:12:00Z",
      "severity": "warning",
      "incident_id": "59064391",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 65,
        "dead_tuples": 16320153,
        "wait_count": 28,
        "tables_affected": "events"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "59064391_sre_8627624d",
    "content": "Maintenance window exceeded - vacuum still running after 224min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-17T09:14:00Z",
      "severity": "info",
      "incident_id": "59064391",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "events",
        "table_bloat_percent": 65,
        "dead_tuples": 16320153
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "59064391_developer_77d2e2ea",
    "content": "User queries slow - autovacuum processing 16320153 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-17T09:17:00Z",
      "severity": "info",
      "incident_id": "59064391",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 65,
        "dead_tuples": 16320153,
        "vacuum_duration_min": 224,
        "tables_affected": "events"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dc27e74a_dba_76d3862f",
    "content": "work_mem reduced to 49MB - 4 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-17T09:34:00Z",
      "severity": "critical",
      "incident_id": "dc27e74a",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 94,
        "oom_kills": 4,
        "swap_usage_gb": 15.7,
        "work_mem_mb": 49,
        "wait_count": 25
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dc27e74a_developer_e3c2d224",
    "content": "Out of memory errors after reaching 94% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-17T09:39:00Z",
      "severity": "critical",
      "incident_id": "dc27e74a",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 15.7,
        "memory_usage_percent": 94,
        "buffer_cache_hit": 39
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "37169bbc_sre_ad1075bd",
    "content": "Replication monitoring: 147s lag on 3 replicas",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-17T13:03:00Z",
      "severity": "warning",
      "incident_id": "37169bbc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "wal_size_mb": 2791,
        "lag_bytes": 10660273,
        "lag_seconds": 147
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37169bbc_dba_7aa4c35b",
    "content": "Standby lag alert: 10660273 bytes behind, 2791MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-17T13:04:00Z",
      "severity": "warning",
      "incident_id": "37169bbc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 10,
        "network_latency_ms": 13,
        "replica_count": 3,
        "wal_size_mb": 2791
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37169bbc_data_engineer_0c3a068c",
    "content": "Pipeline data quality affected by 147s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-17T13:07:00Z",
      "severity": "warning",
      "incident_id": "37169bbc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 13,
        "wait_count": 10,
        "lag_seconds": 147,
        "wal_size_mb": 2791
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37169bbc_developer_68964425",
    "content": "Read-after-write inconsistency - replica 147s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-17T13:16:00Z",
      "severity": "warning",
      "incident_id": "37169bbc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 13,
        "wal_size_mb": 2791,
        "wait_count": 10,
        "lag_bytes": 10660273,
        "lag_seconds": 147
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_14a8d79215374dfd",
    "content": "CPU utilization stable at 46%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-17T18:20:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_e0668aa092d24394",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-19T11:36:29Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7d74a54b060a4218",
    "content": "Statistics updated for 48 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-19T17:00:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a96ea787_sre_a7fe46a9",
    "content": "System alert: 2737ms lock wait impacting 92 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-20T12:12:00Z",
      "severity": "critical",
      "incident_id": "a96ea787",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 58,
        "deadlocks_per_minute": 46,
        "affected_tables": "orders",
        "cascade_duration_min": 9,
        "lock_wait_ms": 2737
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a96ea787_data_engineer_20633131",
    "content": "Batch process deadlocked - 9min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-20T12:17:00Z",
      "severity": "critical",
      "incident_id": "a96ea787",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "lock_wait_ms": 2737,
        "wait_count": 50,
        "blocked_queries": 58,
        "deadlocks_per_minute": 46
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a96ea787_dba_50b0a03a",
    "content": "Lock escalation: 92 transactions rolled back on 'orders'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-20T12:24:00Z",
      "severity": "critical",
      "incident_id": "a96ea787",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 50,
        "lock_wait_ms": 2737,
        "transaction_rollback": 92,
        "blocked_queries": 58
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "32addd3a_developer_26f81022",
    "content": "JDBC connection failed after 8661ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-20T13:33:00Z",
      "severity": "warning",
      "incident_id": "32addd3a",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 965,
        "wait_time_ms": 8661.351887036879,
        "max_connections": 1000,
        "wait_count": 9,
        "active_connections": 965.9432665332615
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "32addd3a_sre_ad5dd79b",
    "content": "Monitoring: Connection pool 87% full, 9 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-20T13:37:00Z",
      "severity": "warning",
      "incident_id": "32addd3a",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 36.9,
        "wait_count": 9,
        "max_connections": 1000,
        "wait_time_ms": 8661.351887036879,
        "threshold": 87
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "32addd3a_data_engineer_341c887e",
    "content": "DBT run failed: waited 8661ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-20T13:42:00Z",
      "severity": "warning",
      "incident_id": "32addd3a",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 8661.351887036879,
        "active_connections": 965.9432665332615,
        "threshold": 87,
        "active": 965,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "32addd3a_dba_c4d57965",
    "content": "ERROR: too many connections for role 'app_user' (current: 965, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-20T13:49:00Z",
      "severity": "warning",
      "incident_id": "32addd3a",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 965,
        "spike_duration_min": 36.9,
        "wait_ms": 8661,
        "wait_time_ms": 8661.351887036879
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "64f5297a_sre_6e75b5a6",
    "content": "Incident: Lock contention causing 88 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-20T14:28:00Z",
      "severity": "info",
      "incident_id": "64f5297a",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 13,
        "transaction_rollback": 61,
        "blocked_queries": 88,
        "lock_wait_ms": 1310,
        "wait_count": 33
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "64f5297a_developer_509e25ce",
    "content": "Deadlock rate 22/min causing 88 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-20T14:37:00Z",
      "severity": "info",
      "incident_id": "64f5297a",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 22,
        "affected_tables": "orders",
        "wait_count": 33,
        "blocked_queries": 88,
        "cascade_duration_min": 13
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_dc5ae6cfdc624cc2",
    "content": "Database health check passed - response time 102ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-20T15:30:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_206b06a7ecf140b9",
    "content": "Statistics updated for 39 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-21T20:33:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_df9868a3cd2a49b1",
    "content": "Connection recycling: 10 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-21T20:36:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "09f6cf00_sre_02af1cae",
    "content": "Performance degradation: 88x slowdown on 'report_aggregate' queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-22T04:27:00Z",
      "severity": "warning",
      "incident_id": "09f6cf00",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 39939622,
        "cpu_usage": 91,
        "query_time_after_ms": 50569.23941279381,
        "wait_count": 20,
        "affected_query": "report_aggregate"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "09f6cf00_data_engineer_013d6ddc",
    "content": "Dashboard failing: report_aggregate query 88x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-22T04:33:00Z",
      "severity": "warning",
      "incident_id": "09f6cf00",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "query_time_after_ms": 50569.23941279381,
        "slowdown_factor": 88,
        "cpu_usage": 91
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "09f6cf00_dba_aec28cf9",
    "content": "Query regression: 'report_aggregate' from 136ms to 50569ms (88x slower)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-22T04:43:00Z",
      "severity": "warning",
      "incident_id": "09f6cf00",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 20,
        "cpu_usage": 91,
        "query_time_before_ms": 136.13460451570202
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_34ac783a92a3420c",
    "content": "Disk I/O normal - 1425 read IOPS, 4330 write IOPS",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-22T22:46:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3673c053_dba_94700762",
    "content": "Standby lag alert: 23488703 bytes behind, 918MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-23T15:50:00Z",
      "severity": "warning",
      "incident_id": "3673c053",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 84,
        "wal_size_mb": 918,
        "wait_count": 15
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3673c053_developer_aea0780f",
    "content": "Read-after-write inconsistency - replica 84s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-23T15:51:00Z",
      "severity": "warning",
      "incident_id": "3673c053",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "network_latency_ms": 41,
        "wal_size_mb": 918
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3673c053_data_engineer_08aa0544",
    "content": "Pipeline data quality affected by 84s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-23T15:53:00Z",
      "severity": "warning",
      "incident_id": "3673c053",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_bytes": 23488703,
        "network_latency_ms": 41
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d7b50a590a2746c6",
    "content": "Query performance within SLA - P99 latency 477ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-23T19:02:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e3f16021_developer_89b4948e",
    "content": "Application queries failing - database memory at 85%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-24T16:45:00Z",
      "severity": "warning",
      "incident_id": "e3f16021",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 45,
        "oom_kills": 9,
        "wait_count": 27,
        "swap_usage_gb": 7.7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e3f16021_sre_241fdede",
    "content": "Critical: Buffer cache efficiency 60% (memory pressure)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-24T16:48:00Z",
      "severity": "warning",
      "incident_id": "e3f16021",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 27,
        "memory_usage_percent": 85,
        "work_mem_mb": 45,
        "swap_usage_gb": 7.7,
        "oom_kills": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e3f16021_dba_7f248945",
    "content": "FATAL: out of memory - 9 processes killed, 85% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-24T16:49:00Z",
      "severity": "warning",
      "incident_id": "e3f16021",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 85,
        "oom_kills": 9,
        "buffer_cache_hit": 60,
        "wait_count": 27
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e3f16021_data_engineer_1c17ea75",
    "content": "Data processing failed - 9 tasks killed, 7.7GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-24T16:59:00Z",
      "severity": "warning",
      "incident_id": "e3f16021",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 27,
        "buffer_cache_hit": 60,
        "oom_kills": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_e0bd82365d6b4383",
    "content": "Query performance within SLA - P99 latency 454ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-24T19:40:16Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "badfdbf2_sre_0d682ed4",
    "content": "Connection leak: 8.1/hr growth rate detected over 15h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-25T12:30:00Z",
      "severity": "warning",
      "incident_id": "badfdbf2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 9,
        "leak_rate_per_hour": 8.1,
        "active_connections": 666.8588011303764,
        "max_connections": 1000,
        "idle_connections": 294
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "badfdbf2_dba_b7800194",
    "content": "pg_stat_activity shows 294 idle connections consuming 351MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-25T12:33:00Z",
      "severity": "warning",
      "incident_id": "badfdbf2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 9,
        "threshold": 66,
        "active_connections": 666.8588011303764
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "badfdbf2_data_engineer_275ea5cd",
    "content": "Pipeline issue: 294 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-25T12:46:00Z",
      "severity": "warning",
      "incident_id": "badfdbf2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 666,
        "idle_connections": 294,
        "threshold": 66
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a9c2706f_data_engineer_b229579f",
    "content": "Pipeline data quality affected by 202s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-26T05:41:00Z",
      "severity": "info",
      "incident_id": "a9c2706f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 21,
        "wal_size_mb": 4852,
        "replica_count": 4
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "a9c2706f_dba_8ecd175e",
    "content": "Standby lag alert: 46550418 bytes behind, 4852MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-26T05:51:00Z",
      "severity": "info",
      "incident_id": "a9c2706f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 202,
        "network_latency_ms": 21,
        "wait_count": 14
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "a9c2706f_developer_07af0875",
    "content": "Read replica delay causing user-visible inconsistencies",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-26T05:54:00Z",
      "severity": "info",
      "incident_id": "a9c2706f",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_seconds": 202,
        "network_latency_ms": 21,
        "wal_size_mb": 4852
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_75b24a304cf04185",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-26T07:14:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a28f8674_dba_6677b32d",
    "content": "Long-running vacuum: 125min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-26T07:49:00Z",
      "severity": "warning",
      "incident_id": "a28f8674",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 58184597,
        "vacuum_duration_min": 125,
        "tables_affected": "events",
        "io_usage_percent": 57,
        "wait_count": 18
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a28f8674_developer_d0ab4228",
    "content": "Database maintenance causing 57% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-26T07:56:00Z",
      "severity": "warning",
      "incident_id": "a28f8674",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "events",
        "wait_count": 18,
        "table_bloat_percent": 42
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "8b34de2e_sre_9fadf2bb",
    "content": "Monitoring: Connection pool 93% full, 7 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-26T08:32:00Z",
      "severity": "warning",
      "incident_id": "8b34de2e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active_connections": 978.0407508497027,
        "wait_count": 7,
        "spike_duration_min": 11.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "8b34de2e_dba_d93ff625",
    "content": "ERROR: too many connections for role 'app_user' (current: 978, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-26T08:34:00Z",
      "severity": "warning",
      "incident_id": "8b34de2e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 11.4,
        "active_connections": 978.0407508497027,
        "threshold": 93,
        "error_code": "PG-53300",
        "wait_count": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "8b34de2e_data_engineer_b9a54eb6",
    "content": "DBT run failed: waited 12250ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-26T08:50:00Z",
      "severity": "warning",
      "incident_id": "8b34de2e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 11.4,
        "active": 978,
        "wait_count": 7,
        "wait_time_ms": 12250.34462135621,
        "wait_ms": 12250
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_2b9c59be7f6d4899",
    "content": "CPU utilization stable at 37%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-26T23:26:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "9553b0f6_data_engineer_91010fb3",
    "content": "Data retention issue: 42GB logs, 91% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-27T05:06:00Z",
      "severity": "warning",
      "incident_id": "9553b0f6",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 42,
        "wait_count": 32,
        "growth_rate_gb_per_day": 23
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "9553b0f6_developer_8592efef",
    "content": "Write failures imminent - 25GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-27T05:15:00Z",
      "severity": "warning",
      "incident_id": "9553b0f6",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 32,
        "days_until_full": 1,
        "growth_rate_gb_per_day": 23,
        "free_space_gb": 25,
        "disk_usage_percent": 91
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "9553b0f6_dba_ab0d838d",
    "content": "Emergency: 25GB free, database will halt in 1 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-27T05:20:00Z",
      "severity": "warning",
      "incident_id": "9553b0f6",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 32,
        "disk_usage_percent": 91,
        "days_until_full": 1,
        "free_space_gb": 25
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_660853c081b44509",
    "content": "Statistics updated for 13 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-27T10:35:26Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_e18cc7928c6341b4",
    "content": "Query performance within SLA - P99 latency 329ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-27T15:37:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e857052b_data_engineer_43ecd1ed",
    "content": "Spark job waiting 24860ms for database connection - 46 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-27T16:40:00Z",
      "severity": "critical",
      "incident_id": "e857052b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "spike_duration_min": 17.8,
        "active": 967,
        "active_connections": 967.8674009621144,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e857052b_developer_13c55942",
    "content": "Connection pool exhausted - 46 threads waiting, 967 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-27T16:48:00Z",
      "severity": "warning",
      "incident_id": "e857052b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 86,
        "active_connections": 967.8674009621144,
        "spike_duration_min": 17.8,
        "active": 967
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e857052b_sre_05a3b04f",
    "content": "AWS RDS Alert: Connection count 967, wait time 24860ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-27T16:57:00Z",
      "severity": "critical",
      "incident_id": "e857052b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 86,
        "max_connections": 1000,
        "active_connections": 967.8674009621144
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_c19c41e766d44df4",
    "content": "Connection pool stable at 218 connections (14% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-10-28T01:24:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_b6a71f45755043bd",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-28T05:40:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_017f5b94e7c54828",
    "content": "Autovacuum completed on table 'sessions' - 881802 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-10-29T11:50:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_08b71bbd55f048f9",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-29T22:25:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_33a2ac0a2600405c",
    "content": "Query cache hit ratio: 92% over last 47 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-10-30T19:22:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_4d802ac25928467e",
    "content": "Replication healthy - all 5 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-10-31T12:59:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "fe180225_data_engineer_89f2b6da",
    "content": "Data pipeline: 'order_search' degraded from 151ms to 55384ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-02T05:33:00Z",
      "severity": "warning",
      "incident_id": "fe180225",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 31646455,
        "cpu_usage": 92,
        "query_time_before_ms": 151.07940727786152
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "fe180225_dba_e7479686",
    "content": "Performance alert: 'order_search' execution time 299x slower at 92% CPU",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-02T05:38:00Z",
      "severity": "info",
      "incident_id": "fe180225",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 299,
        "affected_query": "order_search",
        "cpu_usage": 92,
        "rows_scanned": 31646455,
        "query_time_after_ms": 55384.086563130215
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_7b45d1105648432e",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-02T06:04:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7d1f52b9d04942c3",
    "content": "Connection recycling: 26 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-03T18:11:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0242595b8a4540a1",
    "content": "Autovacuum completed on table 'sessions' - 616014 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-04T23:50:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "1d73927a_dba_f0069fbc",
    "content": "Query plan changed: 'inventory_check' now takes 28204ms scanning 6623540 rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-05T03:28:00Z",
      "severity": "warning",
      "incident_id": "1d73927a",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 224,
        "rows_scanned": 6623540,
        "affected_query": "inventory_check",
        "query_time_after_ms": 28204.74170751722,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1d73927a_sre_ca8d82f1",
    "content": "CloudWatch: Query latency increased 224x for 'inventory_check'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-05T03:33:00Z",
      "severity": "warning",
      "incident_id": "1d73927a",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 224,
        "cpu_usage": 96,
        "affected_query": "inventory_check",
        "rows_scanned": 6623540,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "1d73927a_developer_f3de5c68",
    "content": "Performance regression: 'inventory_check' now 224x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-05T03:41:00Z",
      "severity": "warning",
      "incident_id": "1d73927a",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 6623540,
        "cpu_usage": 96,
        "query_time_before_ms": 159.25669741114103
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_63d4e3cc12c242b8",
    "content": "CPU utilization stable at 33%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-05T20:05:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_b87eb6efaa614835",
    "content": "Buffer cache warmed - 30.9GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-06T03:48:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "583fe2bc_developer_5d1f75fe",
    "content": "Application slowdown during 308min vacuum on 'orders'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-06T21:39:00Z",
      "severity": "warning",
      "incident_id": "583fe2bc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 45,
        "dead_tuples": 3664434,
        "table_bloat_percent": 69
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "583fe2bc_dba_75172785",
    "content": "Vacuum freeze on 'orders' - 54% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-06T21:42:00Z",
      "severity": "warning",
      "incident_id": "583fe2bc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 45,
        "dead_tuples": 3664434,
        "vacuum_duration_min": 308
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "583fe2bc_data_engineer_3799d5e1",
    "content": "Table 'orders' bloat (69%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-06T21:54:00Z",
      "severity": "warning",
      "incident_id": "583fe2bc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "orders",
        "io_usage_percent": 54,
        "wait_count": 45,
        "dead_tuples": 3664434
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e06d1d6_dba_98d1567f",
    "content": "CRITICAL: 35 queries waiting, deadlock rate: 5/min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-07T13:14:00Z",
      "severity": "critical",
      "incident_id": "6e06d1d6",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 35,
        "transaction_rollback": 130,
        "wait_count": 43,
        "deadlocks_per_minute": 5,
        "cascade_duration_min": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6e06d1d6_developer_ca056fdc",
    "content": "Deadlock rate 5/min causing 35 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-07T13:16:00Z",
      "severity": "critical",
      "incident_id": "6e06d1d6",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 8551,
        "deadlocks_per_minute": 5,
        "blocked_queries": 35
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6e06d1d6_data_engineer_fb4ae243",
    "content": "Pipeline stalled: 35 queries waiting for 8551ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-07T13:16:00Z",
      "severity": "critical",
      "incident_id": "6e06d1d6",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 43,
        "deadlocks_per_minute": 5,
        "blocked_queries": 35,
        "affected_tables": "users",
        "transaction_rollback": 130
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_e7e5204171964bee",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-08T05:03:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_6ad6ec05df9748ec",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-08T11:46:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f386fc05b4ec405b",
    "content": "Checkpoint completed: 43685 buffers written in 3.1 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-09T05:32:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_c765efe532f14f2c",
    "content": "Autovacuum completed on table 'products' - 219126 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-09T16:30:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7617506e1197402c",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-11T03:22:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_3132758f2cec473b",
    "content": "Disk I/O normal - 6485 read IOPS, 1181 write IOPS",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-11T03:42:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_101b1d18be2b4631",
    "content": "Backup successful: 1.0GB completed in 57 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-11T06:31:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3778b4be3f184b59",
    "content": "Index 'idx_products_date' rebuild completed - 88MB size reduction",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-11T18:57:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "37971e24_data_engineer_282afb02",
    "content": "Memory pressure: 89% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-12T02:05:00Z",
      "severity": "warning",
      "incident_id": "37971e24",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 2,
        "wait_count": 45,
        "work_mem_mb": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "37971e24_developer_37f624c4",
    "content": "Out of memory errors after reaching 89% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-12T02:15:00Z",
      "severity": "warning",
      "incident_id": "37971e24",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 1.6,
        "memory_usage_percent": 89,
        "buffer_cache_hit": 66,
        "wait_count": 45,
        "work_mem_mb": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0e6a2e87_data_engineer_9221f86e",
    "content": "Memory pressure: 95% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-13T02:35:00Z",
      "severity": "warning",
      "incident_id": "0e6a2e87",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 58,
        "buffer_cache_hit": 41,
        "oom_kills": 6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0e6a2e87_developer_64af7c7b",
    "content": "Out of memory errors after reaching 95% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-13T02:48:00Z",
      "severity": "info",
      "incident_id": "0e6a2e87",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 6,
        "work_mem_mb": 58,
        "wait_count": 32
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0e6a2e87_dba_7fc53152",
    "content": "work_mem reduced to 58MB - 6 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-13T02:51:00Z",
      "severity": "info",
      "incident_id": "0e6a2e87",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 3.7,
        "memory_usage_percent": 95,
        "buffer_cache_hit": 41
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_5be558c543c84674",
    "content": "Autovacuum completed on table 'sessions' - 847971 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-13T08:32:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_c623fc4f7d474585",
    "content": "Query cache hit ratio: 95% over last 30 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-14T22:25:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5902ddea_data_engineer_b9abc872",
    "content": "Data pipeline: 'report_aggregate' degraded from 100ms to 57849ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-16T00:27:00Z",
      "severity": "warning",
      "incident_id": "5902ddea",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 15445679,
        "slowdown_factor": 385,
        "query_time_after_ms": 57849.06908537499,
        "affected_query": "report_aggregate"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "5902ddea_developer_76d08fa3",
    "content": "Service degradation: 'report_aggregate' exceeding SLA by 385x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-16T00:32:00Z",
      "severity": "info",
      "incident_id": "5902ddea",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 100.3217845568026,
        "wait_count": 9,
        "slowdown_factor": 385,
        "rows_scanned": 15445679
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "5902ddea_sre_a98afec9",
    "content": "P99 spike: report_aggregate from 100ms to 57849ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-16T00:37:00Z",
      "severity": "info",
      "incident_id": "5902ddea",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 15445679,
        "query_time_before_ms": 100.3217845568026,
        "affected_query": "report_aggregate"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_e6b897d3cd314a9f",
    "content": "Database health check passed - response time 134ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-16T22:23:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_1e6798564efe43cc",
    "content": "Connection pool stable at 434 connections (34% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-17T06:42:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f16833f675834be9",
    "content": "Checkpoint completed: 7066 buffers written in 7.5 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-17T10:26:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f177e452_sre_1938b7d8",
    "content": "Critical: Buffer cache efficiency 39% (memory pressure)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-18T04:08:00Z",
      "severity": "info",
      "incident_id": "f177e452",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 17,
        "memory_usage_percent": 96,
        "swap_usage_gb": 7.3,
        "work_mem_mb": 35,
        "oom_kills": 7
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "f177e452_developer_c761e92e",
    "content": "Application queries failing - database memory at 96%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-18T04:15:00Z",
      "severity": "info",
      "incident_id": "f177e452",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 35,
        "buffer_cache_hit": 39,
        "wait_count": 17
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "f177e452_dba_717401ce",
    "content": "FATAL: out of memory - 7 processes killed, 96% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-18T04:15:00Z",
      "severity": "info",
      "incident_id": "f177e452",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 17,
        "buffer_cache_hit": 39,
        "oom_kills": 7,
        "work_mem_mb": 35
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "f177e452_data_engineer_dfb07787",
    "content": "Data processing failed - 7 tasks killed, 7.3GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-18T04:17:00Z",
      "severity": "warning",
      "incident_id": "f177e452",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 35,
        "oom_kills": 7,
        "memory_usage_percent": 96,
        "swap_usage_gb": 7.3,
        "wait_count": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_d628d210f7b24e44",
    "content": "CPU utilization stable at 47%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-18T05:12:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "cb6cbbc1_dba_94c0e216",
    "content": "Resource leak: 137 connections idle, 614 active, total memory 207MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-18T14:19:00Z",
      "severity": "critical",
      "incident_id": "cb6cbbc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 207,
        "leak_duration_hours": 12,
        "idle_connections": 137,
        "active": 614,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cb6cbbc1_developer_99e0a868",
    "content": "Connection pool growing: 614 active + 137 idle = potential leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-18T14:19:00Z",
      "severity": "warning",
      "incident_id": "cb6cbbc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 44,
        "leak_duration_hours": 12,
        "active_connections": 614.1691797578635
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cb6cbbc1_sre_bf67e138",
    "content": "Grafana: Connection leak - 10.6 connections/hour over 12h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-18T14:27:00Z",
      "severity": "critical",
      "incident_id": "cb6cbbc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 614.1691797578635,
        "leak_rate_per_hour": 10.6,
        "wait_count": 44
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_5b01fb00a3d14167",
    "content": "Buffer cache warmed - 6.4GB loaded into memory",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-18T23:44:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_156afbd575ed457a",
    "content": "Statistics updated for 41 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-19T09:35:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_b803329a31e94e0f",
    "content": "CPU utilization stable at 22%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-20T17:26:54Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_f4e001a5757c42ec",
    "content": "Disk I/O normal - 1111 read IOPS, 3534 write IOPS",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-20T20:05:28Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_db818aaadd824b05",
    "content": "Autovacuum completed on table 'orders' - 179257 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-21T02:12:21Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_82709e4d0dc54ebf",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-21T13:12:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_39e48bde69884b2b",
    "content": "Connection pool stable at 426 connections (54% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-21T16:42:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "116e4dae_sre_539fe3c3",
    "content": "Disk space alarm: 88% used, 40GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-22T06:39:00Z",
      "severity": "critical",
      "incident_id": "116e4dae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 40,
        "wal_size_gb": 62,
        "growth_rate_gb_per_day": 45
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "116e4dae_developer_2c353763",
    "content": "Write failures imminent - 40GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-22T06:44:00Z",
      "severity": "critical",
      "incident_id": "116e4dae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 88,
        "growth_rate_gb_per_day": 45,
        "wal_size_gb": 62,
        "days_until_full": 1,
        "free_space_gb": 40
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "116e4dae_data_engineer_f3e6d0b2",
    "content": "Data retention issue: 62GB logs, 88% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-22T06:54:00Z",
      "severity": "critical",
      "incident_id": "116e4dae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 1,
        "wait_count": 23,
        "wal_size_gb": 62,
        "disk_usage_percent": 88,
        "growth_rate_gb_per_day": 45
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "73ce3eee_sre_67417760",
    "content": "Connection leak: 10.9/hr growth rate detected over 14h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-22T07:11:00Z",
      "severity": "warning",
      "incident_id": "73ce3eee",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 601,
        "threshold": 60,
        "max_connections": 1000,
        "leak_rate_per_hour": 10.9,
        "idle_connections": 399
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "73ce3eee_dba_29e75e90",
    "content": "pg_stat_activity shows 399 idle connections consuming 313MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-22T07:16:00Z",
      "severity": "warning",
      "incident_id": "73ce3eee",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 60,
        "leak_duration_hours": 14,
        "active_connections": 601.8502518998176
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f0b3cd04a5684c66",
    "content": "Statistics updated for 7 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-22T07:18:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a3d6e9d8_dba_d63cdc46",
    "content": "Disk space critical: 85% used, growth rate 70GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-22T13:26:00Z",
      "severity": "warning",
      "incident_id": "a3d6e9d8",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 45,
        "disk_usage_percent": 85,
        "growth_rate_gb_per_day": 70,
        "days_until_full": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a3d6e9d8_developer_28bd019f",
    "content": "Database storage 85% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-22T13:27:00Z",
      "severity": "warning",
      "incident_id": "a3d6e9d8",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 33,
        "disk_usage_percent": 85,
        "wait_count": 46
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c7841344_data_engineer_15308c32",
    "content": "ETL reading outdated data from replica lagging 145s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-22T14:47:00Z",
      "severity": "warning",
      "incident_id": "c7841344",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 2552,
        "lag_bytes": 94150657,
        "network_latency_ms": 5,
        "replica_count": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c7841344_developer_451e91a7",
    "content": "Application seeing outdated data - 145s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-22T14:48:00Z",
      "severity": "warning",
      "incident_id": "c7841344",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "wal_size_mb": 2552,
        "network_latency_ms": 5,
        "wait_count": 39,
        "lag_seconds": 145
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "def6cc5e_data_engineer_5aa280e7",
    "content": "Data load failing: 170 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-22T19:42:00Z",
      "severity": "warning",
      "incident_id": "def6cc5e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 8,
        "wait_count": 28,
        "blocked_queries": 66,
        "lock_wait_ms": 1520,
        "deadlocks_per_minute": 41
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "def6cc5e_sre_23249267",
    "content": "Critical: 41 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-22T19:52:00Z",
      "severity": "info",
      "incident_id": "def6cc5e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 8,
        "lock_wait_ms": 1520,
        "deadlocks_per_minute": 41
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "def6cc5e_dba_650db967",
    "content": "Deadlock cascade on 'payments' - 41/min for 8min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-22T19:54:00Z",
      "severity": "info",
      "incident_id": "def6cc5e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 1520,
        "cascade_duration_min": 8,
        "deadlocks_per_minute": 41,
        "wait_count": 28,
        "affected_tables": "payments"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0791d04c_developer_40ffaab1",
    "content": "Transaction retry exhausted - 14 deadlocks/min on 'users'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-22T22:09:00Z",
      "severity": "info",
      "incident_id": "0791d04c",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "users",
        "blocked_queries": 78,
        "wait_count": 8
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0791d04c_sre_cbc76e03",
    "content": "System alert: 1093ms lock wait impacting 77 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-22T22:19:00Z",
      "severity": "info",
      "incident_id": "0791d04c",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 14,
        "transaction_rollback": 77,
        "wait_count": 8
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5702ec5_developer_f8b36bef",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 52%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-22T22:53:00Z",
      "severity": "warning",
      "incident_id": "e5702ec5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 52,
        "swap_usage_gb": 3.9,
        "memory_usage_percent": 90
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5702ec5_dba_8564632a",
    "content": "Buffer cache hit ratio dropped to 52% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-22T22:56:00Z",
      "severity": "warning",
      "incident_id": "e5702ec5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 3.9,
        "oom_kills": 6,
        "buffer_cache_hit": 52
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5702ec5_sre_f83c8b0e",
    "content": "AWS monitoring: Memory 90%, swap 3.9GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-22T22:58:00Z",
      "severity": "warning",
      "incident_id": "e5702ec5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 13,
        "work_mem_mb": 52,
        "swap_usage_gb": 3.9,
        "buffer_cache_hit": 52,
        "memory_usage_percent": 90
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_9531353e63ea4194",
    "content": "Statistics updated for 5 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-23T00:28:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "e7fe4f91_dba_d9a4ccd6",
    "content": "Connection leak: idle_in_transaction growing at 19/hour for 4h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-23T07:20:00Z",
      "severity": "info",
      "incident_id": "e7fe4f91",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 4,
        "active_connections": 610.1187421653707,
        "memory_usage_mb": 168
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e7fe4f91_developer_b21fae04",
    "content": "Memory leak in connection handling - 19 connections/hour, 168MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T07:35:00Z",
      "severity": "info",
      "incident_id": "e7fe4f91",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 61,
        "memory_usage_mb": 168,
        "wait_count": 50,
        "active": 610
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_28548abab34747d1",
    "content": "Checkpoint completed: 20654 buffers written in 8.4 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T08:28:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "1152fbed_dba_59c6f047",
    "content": "Sequential scan on 31976296 rows - index 'idx_users_email' not used, CPU at 89%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-23T10:50:00Z",
      "severity": "warning",
      "incident_id": "1152fbed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 31976296,
        "query_time_before_ms": 190.6877380521025,
        "query_time_after_ms": 50154.38684357261
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_64b71af8c34e4148",
    "content": "Backup successful: 98.5GB completed in 35 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-23T10:52:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "1152fbed_data_engineer_29499b2a",
    "content": "ETL bottleneck: 'user_dashboard' step taking 50154ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-23T10:56:00Z",
      "severity": "warning",
      "incident_id": "1152fbed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 31976296,
        "cpu_usage": 89,
        "wait_count": 29
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "1152fbed_developer_5780b3d6",
    "content": "Application query 'user_dashboard' timeout after 50154ms at 89% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T11:03:00Z",
      "severity": "warning",
      "incident_id": "1152fbed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 29,
        "query_time_after_ms": 50154.38684357261,
        "rows_scanned": 31976296
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "46d4789f_developer_6b90c00c",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 62%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T17:59:00Z",
      "severity": "critical",
      "incident_id": "46d4789f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 87,
        "wait_count": 10,
        "work_mem_mb": 29,
        "buffer_cache_hit": 62
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46d4789f_sre_2b8bb1d6",
    "content": "AWS monitoring: Memory 87%, swap 13.4GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-23T18:09:00Z",
      "severity": "critical",
      "incident_id": "46d4789f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 29,
        "swap_usage_gb": 13.4,
        "oom_kills": 9,
        "buffer_cache_hit": 62
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "46d4789f_data_engineer_5a31be36",
    "content": "Pipeline memory issues: work_mem only 29MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-23T18:10:00Z",
      "severity": "critical",
      "incident_id": "46d4789f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 62,
        "work_mem_mb": 29,
        "memory_usage_percent": 87,
        "oom_kills": 9,
        "swap_usage_gb": 13.4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_ac673390918c4a40",
    "content": "Checkpoint completed: 49177 buffers written in 3.2 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T18:51:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b3ece891_data_engineer_265d1ad4",
    "content": "Batch job delayed by autovacuum processing 82529883 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-23T22:12:00Z",
      "severity": "warning",
      "incident_id": "b3ece891",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 50,
        "dead_tuples": 82529883,
        "tables_affected": "metrics",
        "vacuum_duration_min": 202
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b3ece891_developer_7c9666f3",
    "content": "Service degradation during vacuum of 'metrics' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-23T22:22:00Z",
      "severity": "info",
      "incident_id": "b3ece891",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "metrics",
        "wait_count": 25,
        "table_bloat_percent": 48,
        "io_usage_percent": 50
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "611afd37_data_engineer_abe34e13",
    "content": "Pipeline issue: 189 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T07:15:00Z",
      "severity": "warning",
      "incident_id": "611afd37",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 63,
        "leak_duration_hours": 18,
        "active_connections": 634.9211985935949,
        "leak_rate_per_hour": 18.5,
        "wait_count": 34
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "611afd37_dba_65bd1efd",
    "content": "pg_stat_activity shows 189 idle connections consuming 450MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T07:17:00Z",
      "severity": "warning",
      "incident_id": "611afd37",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 18.5,
        "idle_connections": 189,
        "active": 634
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f0ccb962_developer_fa0dff8c",
    "content": "Database storage 94% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T07:57:00Z",
      "severity": "warning",
      "incident_id": "f0ccb962",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 40,
        "days_until_full": 3,
        "wait_count": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f0ccb962_data_engineer_ca0c22fd",
    "content": "ETL jobs at risk - disk 94% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T07:57:00Z",
      "severity": "warning",
      "incident_id": "f0ccb962",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 3,
        "disk_usage_percent": 94,
        "free_space_gb": 47,
        "growth_rate_gb_per_day": 40
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f0ccb962_dba_6085d6ec",
    "content": "Disk space critical: 94% used, growth rate 40GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T07:57:00Z",
      "severity": "warning",
      "incident_id": "f0ccb962",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 20,
        "disk_usage_percent": 94,
        "growth_rate_gb_per_day": 40,
        "free_space_gb": 47,
        "wal_size_gb": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "459130ff_dba_4f97acf0",
    "content": "Connection pool saturation: 995 active, 22 waiting, threshold 88% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T08:22:00Z",
      "severity": "warning",
      "incident_id": "459130ff",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 995,
        "error_code": "PG-53300",
        "active_connections": 995.7666660686425,
        "spike_duration_min": 47.1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "459130ff_developer_8f5a4d5d",
    "content": "Database unavailable after 25822ms wait - circuit breaker opened at 88%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T08:31:00Z",
      "severity": "warning",
      "incident_id": "459130ff",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 88,
        "max_connections": 1000,
        "active": 995
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_ed447ea8b91d4722",
    "content": "Backup successful: 67.0GB completed in 21 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T10:47:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ec133b98_dba_432c7892",
    "content": "Buffer cache hit ratio dropped to 56% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T12:32:00Z",
      "severity": "warning",
      "incident_id": "ec133b98",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 1,
        "wait_count": 34,
        "work_mem_mb": 39,
        "buffer_cache_hit": 56
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "ec133b98_data_engineer_4125de7d",
    "content": "Pipeline memory issues: work_mem only 39MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T12:34:00Z",
      "severity": "warning",
      "incident_id": "ec133b98",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 56,
        "wait_count": 34,
        "oom_kills": 1,
        "memory_usage_percent": 91
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "ec133b98_sre_113a7185",
    "content": "AWS monitoring: Memory 91%, swap 5.1GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T12:37:00Z",
      "severity": "warning",
      "incident_id": "ec133b98",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 1,
        "work_mem_mb": 39,
        "swap_usage_gb": 5.1,
        "buffer_cache_hit": 56
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4f6259e1_sre_3fb32118",
    "content": "Network issue causing 30ms latency, 196s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T14:09:00Z",
      "severity": "critical",
      "incident_id": "4f6259e1",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 1430,
        "lag_bytes": 35809381,
        "replica_count": 4,
        "lag_seconds": 196,
        "network_latency_ms": 30
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4f6259e1_data_engineer_13cd00d6",
    "content": "Report inconsistencies due to 196s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T14:14:00Z",
      "severity": "critical",
      "incident_id": "4f6259e1",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_bytes": 35809381,
        "network_latency_ms": 30,
        "wal_size_mb": 1430,
        "wait_count": 37
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4f6259e1_developer_cd97b3f5",
    "content": "Stale data on read replicas - 196s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T14:18:00Z",
      "severity": "critical",
      "incident_id": "4f6259e1",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 37,
        "replica_count": 4,
        "network_latency_ms": 30,
        "wal_size_mb": 1430,
        "lag_bytes": 35809381
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "97faa97d_data_engineer_f2254766",
    "content": "Batch process deadlocked - 13min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T14:30:00Z",
      "severity": "info",
      "incident_id": "97faa97d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "cascade_duration_min": 13,
        "deadlocks_per_minute": 31,
        "blocked_queries": 11
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "97faa97d_developer_f47e0a67",
    "content": "Transaction retry exhausted - 31 deadlocks/min on 'shipments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T14:34:00Z",
      "severity": "info",
      "incident_id": "97faa97d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 13,
        "blocked_queries": 11,
        "transaction_rollback": 197,
        "lock_wait_ms": 4268
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "97faa97d_dba_e1b70ee7",
    "content": "Lock escalation: 197 transactions rolled back on 'shipments'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T14:44:00Z",
      "severity": "info",
      "incident_id": "97faa97d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 31,
        "affected_tables": "shipments",
        "transaction_rollback": 197,
        "wait_count": 27
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b38341e_data_engineer_02eea473",
    "content": "Data retention issue: 97GB logs, 96% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T16:02:00Z",
      "severity": "critical",
      "incident_id": "9b38341e",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 6,
        "free_space_gb": 18,
        "disk_usage_percent": 96,
        "wait_count": 23
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b38341e_sre_19e373c2",
    "content": "Disk space alarm: 96% used, 18GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T16:03:00Z",
      "severity": "critical",
      "incident_id": "9b38341e",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 96,
        "free_space_gb": 18,
        "days_until_full": 6,
        "wal_size_gb": 97
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46cd1d2f_data_engineer_5519aab4",
    "content": "Data load competing with 177min vacuum operation",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T17:47:00Z",
      "severity": "warning",
      "incident_id": "46cd1d2f",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 95052501,
        "io_usage_percent": 68,
        "table_bloat_percent": 79
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46cd1d2f_sre_5ca735cc",
    "content": "Maintenance window exceeded - vacuum still running after 177min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T17:48:00Z",
      "severity": "info",
      "incident_id": "46cd1d2f",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 95052501,
        "io_usage_percent": 68,
        "wait_count": 33,
        "table_bloat_percent": 79
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "46cd1d2f_dba_7ad08fb0",
    "content": "Aggressive vacuum on 'logs' - 95052501 dead tuples, 79% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T17:51:00Z",
      "severity": "info",
      "incident_id": "46cd1d2f",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 68,
        "tables_affected": "logs",
        "wait_count": 33,
        "vacuum_duration_min": 177,
        "table_bloat_percent": 79
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_b98604a32c254368",
    "content": "SSL certificate valid for 109 more days",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T19:51:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "32a347d9_developer_82ecb1ba",
    "content": "Application holding 341 unused connections for 14 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-24T20:17:00Z",
      "severity": "warning",
      "incident_id": "32a347d9",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 19,
        "memory_usage_mb": 293,
        "threshold": 54,
        "active": 547,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "32a347d9_data_engineer_2058037f",
    "content": "Pipeline issue: 341 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-24T20:18:00Z",
      "severity": "warning",
      "incident_id": "32a347d9",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 293,
        "max_connections": 1000,
        "threshold": 54,
        "leak_duration_hours": 14,
        "active_connections": 547.234528485173
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "32a347d9_sre_11800cb4",
    "content": "Connection leak: 5.9/hr growth rate detected over 14h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-24T20:31:00Z",
      "severity": "warning",
      "incident_id": "32a347d9",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 19,
        "threshold": 54,
        "memory_usage_mb": 293
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_73c06a177d174539",
    "content": "Statistics updated for 36 tables",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-24T22:18:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2ea70021_data_engineer_684cff5b",
    "content": "Storage crisis: 3 days until pipeline failure",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T12:47:00Z",
      "severity": "warning",
      "incident_id": "2ea70021",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 91,
        "growth_rate_gb_per_day": 59,
        "free_space_gb": 23,
        "wait_count": 22,
        "days_until_full": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "2ea70021_dba_b1c90491",
    "content": "WAL accumulation: 91GB of logs, disk 98% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T12:53:00Z",
      "severity": "warning",
      "incident_id": "2ea70021",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 23,
        "growth_rate_gb_per_day": 59,
        "days_until_full": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "2ea70021_sre_acbb64d6",
    "content": "Storage capacity: 3 days until exhaustion",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T13:02:00Z",
      "severity": "warning",
      "incident_id": "2ea70021",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 22,
        "free_space_gb": 23,
        "growth_rate_gb_per_day": 59,
        "wal_size_gb": 91,
        "days_until_full": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "2ea70021_developer_2d0e9ca2",
    "content": "Critical: Database disk 98% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T13:04:00Z",
      "severity": "warning",
      "incident_id": "2ea70021",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 98,
        "free_space_gb": 23,
        "wal_size_gb": 91
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b5ffd5d_data_engineer_093a84a8",
    "content": "ETL performance impacted by vacuum on 'orders'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T14:04:00Z",
      "severity": "warning",
      "incident_id": "9b5ffd5d",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 77,
        "tables_affected": "orders",
        "dead_tuples": 55002917
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b5ffd5d_sre_a39cbd80",
    "content": "Performance impact: 220min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T14:04:00Z",
      "severity": "warning",
      "incident_id": "9b5ffd5d",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 55002917,
        "wait_count": 48,
        "vacuum_duration_min": 220
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b5ffd5d_developer_14421b1b",
    "content": "Application slowdown during 220min vacuum on 'orders'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T14:11:00Z",
      "severity": "warning",
      "incident_id": "9b5ffd5d",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 77,
        "table_bloat_percent": 33,
        "tables_affected": "orders",
        "dead_tuples": 55002917
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9b5ffd5d_dba_fcc1f11f",
    "content": "Vacuum freeze on 'orders' - 77% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T14:14:00Z",
      "severity": "warning",
      "incident_id": "9b5ffd5d",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 33,
        "tables_affected": "orders",
        "dead_tuples": 55002917,
        "vacuum_duration_min": 220
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ee15053_sre_4e19e526",
    "content": "IO spike: 78% during vacuum of 'metrics'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T15:48:00Z",
      "severity": "warning",
      "incident_id": "0ee15053",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 10,
        "dead_tuples": 9135943,
        "table_bloat_percent": 60,
        "vacuum_duration_min": 280
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ee15053_data_engineer_3e7e6045",
    "content": "Batch job delayed by autovacuum processing 9135943 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T15:52:00Z",
      "severity": "warning",
      "incident_id": "0ee15053",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 9135943,
        "table_bloat_percent": 60,
        "wait_count": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0ee15053_developer_b5665918",
    "content": "Service degradation during vacuum of 'metrics' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T16:01:00Z",
      "severity": "warning",
      "incident_id": "0ee15053",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 10,
        "dead_tuples": 9135943,
        "table_bloat_percent": 60,
        "tables_affected": "metrics"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8be2cfe8_developer_b7819ef3",
    "content": "Read-after-write inconsistency - replica 224s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T17:56:00Z",
      "severity": "info",
      "incident_id": "8be2cfe8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 21,
        "wal_size_mb": 1571,
        "replica_count": 3,
        "lag_bytes": 66895071,
        "lag_seconds": 224
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8be2cfe8_data_engineer_1cb4d030",
    "content": "Analytics queries on stale data - replica 224s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T18:06:00Z",
      "severity": "info",
      "incident_id": "8be2cfe8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_seconds": 224,
        "wait_count": 21
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2e9fa846_sre_1827083c",
    "content": "System alert: 6553ms lock wait impacting 131 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T19:20:00Z",
      "severity": "warning",
      "incident_id": "2e9fa846",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "blocked_queries": 88,
        "deadlocks_per_minute": 17,
        "transaction_rollback": 131
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2e9fa846_developer_74e0dcc4",
    "content": "Transaction retry exhausted - 17 deadlocks/min on 'payments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T19:27:00Z",
      "severity": "warning",
      "incident_id": "2e9fa846",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 17,
        "cascade_duration_min": 14,
        "lock_wait_ms": 6553,
        "transaction_rollback": 131,
        "blocked_queries": 88
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2e9fa846_dba_ae69bf4d",
    "content": "Lock escalation: 131 transactions rolled back on 'payments'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T19:30:00Z",
      "severity": "warning",
      "incident_id": "2e9fa846",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 35,
        "deadlocks_per_minute": 17,
        "affected_tables": "payments",
        "lock_wait_ms": 6553,
        "blocked_queries": 88
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2e9fa846_data_engineer_daf6c4ff",
    "content": "Batch process deadlocked - 14min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T19:38:00Z",
      "severity": "warning",
      "incident_id": "2e9fa846",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 35,
        "lock_wait_ms": 6553,
        "blocked_queries": 88
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "dbb7a6c3_sre_173ea3d7",
    "content": "CRITICAL: Database connections at 86% capacity (987 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T20:27:00Z",
      "severity": "warning",
      "incident_id": "dbb7a6c3",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 987.6003323084742,
        "wait_ms": 25886,
        "wait_count": 35,
        "spike_duration_min": 30.6,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "dbb7a6c3_dba_98f5bfa0",
    "content": "Connection pool saturation: 987 active, 35 waiting, threshold 86% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T20:47:00Z",
      "severity": "warning",
      "incident_id": "dbb7a6c3",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 987.6003323084742,
        "active": 987,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "79651b7e_data_engineer_79faf1ad",
    "content": "ETL job killed: OOM at 88% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T21:07:00Z",
      "severity": "warning",
      "incident_id": "79651b7e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 88,
        "oom_kills": 7,
        "wait_count": 37,
        "work_mem_mb": 37,
        "swap_usage_gb": 8.0
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "79651b7e_dba_8761c420",
    "content": "Critical memory pressure: cache hit 69%, swap 8.0GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T21:11:00Z",
      "severity": "info",
      "incident_id": "79651b7e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 37,
        "work_mem_mb": 37,
        "swap_usage_gb": 8.0,
        "oom_kills": 7
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "79651b7e_sre_2d11aebe",
    "content": "System swapping 8.0GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T21:23:00Z",
      "severity": "info",
      "incident_id": "79651b7e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 69,
        "oom_kills": 7,
        "work_mem_mb": 37
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b486f2c8_data_engineer_3aad599d",
    "content": "Batch job delayed: report_aggregate running 335x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-25T22:14:00Z",
      "severity": "critical",
      "incident_id": "b486f2c8",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 10228.582930924851,
        "rows_scanned": 17511812,
        "query_time_before_ms": 125.20234553315129,
        "wait_count": 28
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b486f2c8_sre_2460f69b",
    "content": "CloudWatch: Query latency increased 335x for 'report_aggregate'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-25T22:14:00Z",
      "severity": "critical",
      "incident_id": "b486f2c8",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "query_time_before_ms": 125.20234553315129,
        "slowdown_factor": 335
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b486f2c8_dba_2f7d4b5d",
    "content": "Query plan changed: 'report_aggregate' now takes 10228ms scanning 17511812 rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-25T22:18:00Z",
      "severity": "critical",
      "incident_id": "b486f2c8",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "slowdown_factor": 335,
        "rows_scanned": 17511812,
        "cpu_usage": 73,
        "query_time_after_ms": 10228.582930924851
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b486f2c8_developer_8d185a3a",
    "content": "Performance regression: 'report_aggregate' now 335x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-25T22:32:00Z",
      "severity": "warning",
      "incident_id": "b486f2c8",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 125.20234553315129,
        "wait_count": 28,
        "rows_scanned": 17511812
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "988674b0_sre_f82515dc",
    "content": "Critical: 47 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T07:22:00Z",
      "severity": "critical",
      "incident_id": "988674b0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 4,
        "affected_tables": "orders",
        "blocked_queries": 42,
        "transaction_rollback": 193
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "988674b0_data_engineer_031742e1",
    "content": "Data load failing: 193 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T07:26:00Z",
      "severity": "critical",
      "incident_id": "988674b0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 11,
        "lock_wait_ms": 2117,
        "transaction_rollback": 193
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "988674b0_dba_e0021fc8",
    "content": "Deadlock cascade on 'orders' - 47/min for 4min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T07:37:00Z",
      "severity": "critical",
      "incident_id": "988674b0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "deadlocks_per_minute": 47,
        "blocked_queries": 42,
        "lock_wait_ms": 2117,
        "transaction_rollback": 193
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "988674b0_developer_d9a95dcd",
    "content": "Application errors: 193 transactions failed in 4min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T07:41:00Z",
      "severity": "critical",
      "incident_id": "988674b0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 2117,
        "transaction_rollback": 193,
        "cascade_duration_min": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "b865072f_sre_9714ddbf",
    "content": "CRITICAL: Database connections at 94% capacity (953 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T10:11:00Z",
      "severity": "warning",
      "incident_id": "b865072f",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 24031.52455012993,
        "spike_duration_min": 22.0,
        "active": 953,
        "active_connections": 953.0638170001456,
        "wait_ms": 24031
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "b865072f_data_engineer_1b85256f",
    "content": "Airflow DAG stalled: connection pool exhausted during 22.0min window",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T10:14:00Z",
      "severity": "warning",
      "incident_id": "b865072f",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 24031.52455012993,
        "wait_count": 43,
        "spike_duration_min": 22.0
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "b865072f_dba_a1d36920",
    "content": "Connection pool saturation: 953 active, 43 waiting, threshold 94% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T10:16:00Z",
      "severity": "warning",
      "incident_id": "b865072f",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 24031,
        "spike_duration_min": 22.0,
        "max_connections": 1000,
        "wait_count": 43,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a352a7aa_dba_e24ddc54",
    "content": "CRITICAL: Disk 97% full, only 34GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T15:11:00Z",
      "severity": "warning",
      "incident_id": "a352a7aa",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 71,
        "disk_usage_percent": 97,
        "wait_count": 45,
        "free_space_gb": 34,
        "wal_size_gb": 52
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a352a7aa_developer_5b5f1257",
    "content": "Application at risk: only 34GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T15:18:00Z",
      "severity": "warning",
      "incident_id": "a352a7aa",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 34,
        "growth_rate_gb_per_day": 71,
        "days_until_full": 1,
        "disk_usage_percent": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a352a7aa_data_engineer_608aa69b",
    "content": "Data pipeline may fail: only 34GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T15:21:00Z",
      "severity": "warning",
      "incident_id": "a352a7aa",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 34,
        "growth_rate_gb_per_day": 71,
        "disk_usage_percent": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "1c67326a_dba_73f1e6ca",
    "content": "Resource leak: 262 connections idle, 635 active, total memory 172MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T17:39:00Z",
      "severity": "warning",
      "incident_id": "1c67326a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 635,
        "leak_duration_hours": 10,
        "idle_connections": 262
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "1c67326a_sre_1da7f6b5",
    "content": "Grafana: Connection leak - 18.3 connections/hour over 10h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T17:46:00Z",
      "severity": "warning",
      "incident_id": "1c67326a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "memory_usage_mb": 172,
        "active": 635
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "1c67326a_data_engineer_5bd78cb7",
    "content": "Data job resource leak: 18/hour accumulation rate",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T17:47:00Z",
      "severity": "warning",
      "incident_id": "1c67326a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 262,
        "leak_duration_hours": 10,
        "active": 635,
        "wait_count": 47
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b73cf9dc_dba_fcd0e655",
    "content": "Long-running vacuum: 95min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T18:12:00Z",
      "severity": "warning",
      "incident_id": "b73cf9dc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 67,
        "vacuum_duration_min": 95,
        "io_usage_percent": 63,
        "wait_count": 34
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b73cf9dc_developer_838bf3f2",
    "content": "Service degradation during vacuum of 'sessions' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T18:13:00Z",
      "severity": "warning",
      "incident_id": "b73cf9dc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 34,
        "io_usage_percent": 63,
        "dead_tuples": 5482315
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b73cf9dc_sre_bc0db5e1",
    "content": "IO spike: 63% during vacuum of 'sessions'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T18:25:00Z",
      "severity": "warning",
      "incident_id": "b73cf9dc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 34,
        "dead_tuples": 5482315,
        "io_usage_percent": 63
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b73cf9dc_data_engineer_d2658660",
    "content": "Batch job delayed by autovacuum processing 5482315 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T18:25:00Z",
      "severity": "warning",
      "incident_id": "b73cf9dc",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 95,
        "io_usage_percent": 63,
        "table_bloat_percent": 67
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "262bd4c9_sre_d71affe7",
    "content": "Replica health check: 93410633 bytes behind primary",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T18:49:00Z",
      "severity": "warning",
      "incident_id": "262bd4c9",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 93410633,
        "lag_seconds": 77,
        "wait_count": 25,
        "wal_size_mb": 3394,
        "replica_count": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "262bd4c9_developer_89e435b4",
    "content": "Application seeing outdated data - 77s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T18:50:00Z",
      "severity": "warning",
      "incident_id": "262bd4c9",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "lag_seconds": 77,
        "wait_count": 25
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "262bd4c9_data_engineer_71cdf448",
    "content": "ETL reading outdated data from replica lagging 77s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T18:56:00Z",
      "severity": "warning",
      "incident_id": "262bd4c9",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 3394,
        "lag_bytes": 93410633,
        "network_latency_ms": 21,
        "lag_seconds": 77,
        "replica_count": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ed8d4cf0_sre_0a45ad02",
    "content": "IO spike: 92% during vacuum of 'logs'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-26T19:06:00Z",
      "severity": "warning",
      "incident_id": "ed8d4cf0",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 19,
        "io_usage_percent": 92,
        "vacuum_duration_min": 353,
        "dead_tuples": 25032733,
        "table_bloat_percent": 68
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ed8d4cf0_data_engineer_fb54cbe1",
    "content": "Batch job delayed by autovacuum processing 25032733 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T19:06:00Z",
      "severity": "warning",
      "incident_id": "ed8d4cf0",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "table_bloat_percent": 68,
        "io_usage_percent": 92,
        "dead_tuples": 25032733
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ed8d4cf0_dba_7ee130ae",
    "content": "Long-running vacuum: 353min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-26T19:08:00Z",
      "severity": "warning",
      "incident_id": "ed8d4cf0",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 25032733,
        "wait_count": 19,
        "table_bloat_percent": 68,
        "io_usage_percent": 92
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ed8d4cf0_developer_8b76b8ff",
    "content": "Service degradation during vacuum of 'logs' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T19:09:00Z",
      "severity": "warning",
      "incident_id": "ed8d4cf0",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 92,
        "wait_count": 19,
        "vacuum_duration_min": 353,
        "tables_affected": "logs",
        "dead_tuples": 25032733
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ced0cf0d_data_engineer_5293e039",
    "content": "ETL jobs at risk - disk 97% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-26T20:39:00Z",
      "severity": "warning",
      "incident_id": "ced0cf0d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 28,
        "days_until_full": 1,
        "disk_usage_percent": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ced0cf0d_developer_0cece5f6",
    "content": "Database storage 97% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-26T20:50:00Z",
      "severity": "warning",
      "incident_id": "ced0cf0d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 97,
        "growth_rate_gb_per_day": 62,
        "free_space_gb": 24,
        "wait_count": 43
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ec4207ed_sre_14bf6f63",
    "content": "P99 spike: order_search from 95.9ms to 36451ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T07:10:00Z",
      "severity": "critical",
      "incident_id": "ec4207ed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "cpu_usage": 71,
        "query_time_after_ms": 36451.27642359225
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ec4207ed_developer_5eb0f95c",
    "content": "Service degradation: 'order_search' exceeding SLA by 259x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T07:19:00Z",
      "severity": "warning",
      "incident_id": "ec4207ed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 71,
        "query_time_before_ms": 95.94819184394996,
        "slowdown_factor": 259
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ec4207ed_data_engineer_d78e5b11",
    "content": "Data pipeline: 'order_search' degraded from 95.9ms to 36451ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T07:21:00Z",
      "severity": "critical",
      "incident_id": "ec4207ed",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 15210649,
        "query_time_before_ms": 95.94819184394996,
        "cpu_usage": 71,
        "wait_count": 50,
        "affected_query": "order_search"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "69bc9ffa_sre_0048acb2",
    "content": "Memory alert: 98% utilized, 5 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T08:32:00Z",
      "severity": "warning",
      "incident_id": "69bc9ffa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 98,
        "buffer_cache_hit": 38,
        "swap_usage_gb": 12.3,
        "oom_kills": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "69bc9ffa_data_engineer_d52285de",
    "content": "Memory pressure: 98% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T08:39:00Z",
      "severity": "warning",
      "incident_id": "69bc9ffa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 49,
        "buffer_cache_hit": 38,
        "memory_usage_percent": 98,
        "wait_count": 23
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "69bc9ffa_developer_99c3dac0",
    "content": "Out of memory errors after reaching 98% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T08:42:00Z",
      "severity": "warning",
      "incident_id": "69bc9ffa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 38,
        "swap_usage_gb": 12.3,
        "oom_kills": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9a0cf3d9_dba_84be8887",
    "content": "Buffer cache hit ratio dropped to 57% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-27T12:35:00Z",
      "severity": "critical",
      "incident_id": "9a0cf3d9",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 10,
        "oom_kills": 5,
        "work_mem_mb": 18,
        "memory_usage_percent": 91,
        "swap_usage_gb": 8.9
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9a0cf3d9_data_engineer_ec202272",
    "content": "Pipeline memory issues: work_mem only 18MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T12:41:00Z",
      "severity": "critical",
      "incident_id": "9a0cf3d9",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 18,
        "memory_usage_percent": 91,
        "swap_usage_gb": 8.9
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9a0cf3d9_developer_2a1c257b",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 57%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T12:47:00Z",
      "severity": "warning",
      "incident_id": "9a0cf3d9",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "oom_kills": 5,
        "work_mem_mb": 18,
        "wait_count": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94b6656e_developer_0ce1c61e",
    "content": "Database OOM: 2 connections terminated at 96% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T15:41:00Z",
      "severity": "warning",
      "incident_id": "94b6656e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 36,
        "work_mem_mb": 43,
        "memory_usage_percent": 96,
        "oom_kills": 2,
        "wait_count": 24
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94b6656e_sre_20659bff",
    "content": "System swapping 3.9GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T15:45:00Z",
      "severity": "critical",
      "incident_id": "94b6656e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 3.9,
        "wait_count": 24,
        "buffer_cache_hit": 36,
        "work_mem_mb": 43
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94b6656e_dba_6825ea1b",
    "content": "Critical memory pressure: cache hit 36%, swap 3.9GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-27T15:48:00Z",
      "severity": "critical",
      "incident_id": "94b6656e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 2,
        "buffer_cache_hit": 36,
        "wait_count": 24,
        "memory_usage_percent": 96
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94b6656e_data_engineer_0c093b7f",
    "content": "ETL job killed: OOM at 96% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T15:55:00Z",
      "severity": "critical",
      "incident_id": "94b6656e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 24,
        "swap_usage_gb": 3.9,
        "work_mem_mb": 43,
        "memory_usage_percent": 96
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0a0b96a7_dba_26dc7c3b",
    "content": "Circular wait on 'payments' - 7906ms average wait time",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-27T17:30:00Z",
      "severity": "critical",
      "incident_id": "0a0b96a7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 7906,
        "wait_count": 42,
        "transaction_rollback": 37,
        "deadlocks_per_minute": 18,
        "blocked_queries": 38
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0a0b96a7_developer_cd5ad830",
    "content": "TransactionRollback: deadlock after 7906ms on 'payments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T17:33:00Z",
      "severity": "warning",
      "incident_id": "0a0b96a7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 13,
        "deadlocks_per_minute": 18,
        "lock_wait_ms": 7906,
        "affected_tables": "payments",
        "blocked_queries": 38
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0a0b96a7_data_engineer_2f0d6da5",
    "content": "ETL blocked by deadlocks on 'payments' (18/min)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T17:33:00Z",
      "severity": "critical",
      "incident_id": "0a0b96a7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 38,
        "wait_count": 42,
        "lock_wait_ms": 7906
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0a0b96a7_sre_38248d17",
    "content": "Service disruption: 37 failed transactions in 13min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T17:34:00Z",
      "severity": "critical",
      "incident_id": "0a0b96a7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 7906,
        "affected_tables": "payments",
        "transaction_rollback": 37,
        "blocked_queries": 38
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c75acd0b_dba_9707ca9b",
    "content": "Connection accumulation: 8.7 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-27T17:56:00Z",
      "severity": "warning",
      "incident_id": "c75acd0b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "wait_count": 17,
        "leak_rate_per_hour": 8.7,
        "active_connections": 548.1278531485705
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c75acd0b_data_engineer_0c218f07",
    "content": "ETL connection pool: 333 idle connections leaking at 9/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T18:04:00Z",
      "severity": "warning",
      "incident_id": "c75acd0b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 548.1278531485705,
        "active": 548,
        "threshold": 54
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c75acd0b_developer_65b39920",
    "content": "Connection pool: 333 idle, 548 active - possible 223MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T18:08:00Z",
      "severity": "warning",
      "incident_id": "c75acd0b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active_connections": 548.1278531485705,
        "leak_duration_hours": 12,
        "wait_count": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c75acd0b_sre_cdcce5cf",
    "content": "Linear growth: 333 idle connections, 223MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T18:12:00Z",
      "severity": "warning",
      "incident_id": "c75acd0b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "active": 548,
        "max_connections": 1000,
        "wait_count": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "40f5112e_sre_4f70081e",
    "content": "P99 spike: user_dashboard from 116ms to 8541ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-27T19:23:00Z",
      "severity": "warning",
      "incident_id": "40f5112e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 85,
        "slowdown_factor": 107,
        "rows_scanned": 40565925
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "74de7563_data_engineer_04d0f123",
    "content": "Table 'orders' bloat (61%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T19:30:00Z",
      "severity": "warning",
      "incident_id": "74de7563",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 10,
        "io_usage_percent": 94,
        "table_bloat_percent": 61
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "40f5112e_data_engineer_ef824f3b",
    "content": "Data pipeline: 'user_dashboard' degraded from 116ms to 8541ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-27T19:33:00Z",
      "severity": "warning",
      "incident_id": "40f5112e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 40,
        "query_time_after_ms": 8541.850687044422,
        "query_time_before_ms": 116.33863653833914,
        "cpu_usage": 85,
        "rows_scanned": 40565925
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "74de7563_dba_8f478386",
    "content": "Autovacuum running 34min on 'orders', IO at 94%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-27T19:34:00Z",
      "severity": "warning",
      "incident_id": "74de7563",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 34,
        "wait_count": 10,
        "dead_tuples": 86925659,
        "table_bloat_percent": 61,
        "tables_affected": "orders"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "40f5112e_developer_ebb44afa",
    "content": "Service degradation: 'user_dashboard' exceeding SLA by 107x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T19:35:00Z",
      "severity": "warning",
      "incident_id": "40f5112e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 116.33863653833914,
        "cpu_usage": 85,
        "rows_scanned": 40565925
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "74de7563_developer_0c62d6fe",
    "content": "Database maintenance causing 94% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-27T19:36:00Z",
      "severity": "warning",
      "incident_id": "74de7563",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "orders",
        "table_bloat_percent": 61,
        "io_usage_percent": 94,
        "vacuum_duration_min": 34,
        "wait_count": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e2204547_developer_0d8574c3",
    "content": "Read-after-write inconsistency - replica 52s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T06:42:00Z",
      "severity": "warning",
      "incident_id": "e2204547",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 2651,
        "replica_count": 1,
        "network_latency_ms": 15,
        "lag_bytes": 46939925,
        "wait_count": 6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e2204547_dba_6abbc375",
    "content": "Read replica out of sync by 52 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T06:49:00Z",
      "severity": "warning",
      "incident_id": "e2204547",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 46939925,
        "wait_count": 6,
        "wal_size_mb": 2651,
        "lag_seconds": 52
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e2204547_sre_9a1f9d0e",
    "content": "Alert: WAL accumulation 2651MB, lag 52s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T06:52:00Z",
      "severity": "warning",
      "incident_id": "e2204547",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 52,
        "network_latency_ms": 15,
        "lag_bytes": 46939925
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e2204547_data_engineer_eeb21e6d",
    "content": "Analytics queries on stale data - replica 52s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T06:54:00Z",
      "severity": "warning",
      "incident_id": "e2204547",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 52,
        "wal_size_mb": 2651,
        "replica_count": 1,
        "lag_bytes": 46939925,
        "network_latency_ms": 15
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d4e3a8bb_sre_27fec1a4",
    "content": "Emergency response needed: 96% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T10:46:00Z",
      "severity": "info",
      "incident_id": "d4e3a8bb",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 10,
        "disk_usage_percent": 96,
        "wal_size_gb": 52,
        "growth_rate_gb_per_day": 85
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d4e3a8bb_data_engineer_d9a78efa",
    "content": "Storage crisis: 6 days until pipeline failure",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T10:47:00Z",
      "severity": "warning",
      "incident_id": "d4e3a8bb",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 52,
        "disk_usage_percent": 96,
        "free_space_gb": 1,
        "days_until_full": 6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d4e3a8bb_developer_a0856b92",
    "content": "Critical: Database disk 96% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T10:58:00Z",
      "severity": "info",
      "incident_id": "d4e3a8bb",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 96,
        "days_until_full": 6,
        "wait_count": 10
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d4e3a8bb_dba_5069cf3a",
    "content": "WAL accumulation: 52GB of logs, disk 96% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T10:58:00Z",
      "severity": "info",
      "incident_id": "d4e3a8bb",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 1,
        "disk_usage_percent": 96,
        "growth_rate_gb_per_day": 85,
        "wal_size_gb": 52
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e523e9cf_sre_e6be51f2",
    "content": "Disk usage alert: 'metrics' table 36% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T11:32:00Z",
      "severity": "warning",
      "incident_id": "e523e9cf",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 66,
        "dead_tuples": 58652556,
        "tables_affected": "metrics",
        "table_bloat_percent": 36,
        "wait_count": 32
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e523e9cf_dba_622445e7",
    "content": "Aggressive vacuum on 'metrics' - 58652556 dead tuples, 36% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T11:43:00Z",
      "severity": "warning",
      "incident_id": "e523e9cf",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 32,
        "io_usage_percent": 58,
        "dead_tuples": 58652556
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ceb7b0ff_developer_685cd18e",
    "content": "Service degradation: 'report_aggregate' exceeding SLA by 492x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T11:54:00Z",
      "severity": "warning",
      "incident_id": "ceb7b0ff",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 13314953,
        "query_time_before_ms": 151.24472020568135,
        "slowdown_factor": 492,
        "cpu_usage": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ceb7b0ff_data_engineer_8fcb42b3",
    "content": "Data pipeline: 'report_aggregate' degraded from 151ms to 48226ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T11:54:00Z",
      "severity": "warning",
      "incident_id": "ceb7b0ff",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 44,
        "slowdown_factor": 492,
        "query_time_before_ms": 151.24472020568135
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7cd4445c_dba_ab6bfa3c",
    "content": "Connection leak: idle_in_transaction growing at 18/hour for 13h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T11:59:00Z",
      "severity": "critical",
      "incident_id": "7cd4445c",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 623,
        "leak_duration_hours": 13,
        "leak_rate_per_hour": 17.8,
        "wait_count": 49
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7cd4445c_developer_6a667a91",
    "content": "Memory leak in connection handling - 18 connections/hour, 126MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T12:05:00Z",
      "severity": "critical",
      "incident_id": "7cd4445c",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 623,
        "leak_rate_per_hour": 17.8,
        "wait_count": 49,
        "active_connections": 623.0028406488464
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "798dcd4b_sre_c0339c79",
    "content": "Network issue causing 11ms latency, 267s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T13:14:00Z",
      "severity": "critical",
      "incident_id": "798dcd4b",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 2208,
        "wait_count": 34,
        "lag_seconds": 267
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "798dcd4b_data_engineer_2bf64c82",
    "content": "Report inconsistencies due to 267s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T13:17:00Z",
      "severity": "critical",
      "incident_id": "798dcd4b",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 267,
        "lag_bytes": 85028301,
        "replica_count": 3,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "188fd733_developer_7c2e9fc0",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 37%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T13:35:00Z",
      "severity": "warning",
      "incident_id": "188fd733",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 27,
        "swap_usage_gb": 11.6,
        "work_mem_mb": 37,
        "memory_usage_percent": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "188fd733_dba_82345e83",
    "content": "Buffer cache hit ratio dropped to 37% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T13:39:00Z",
      "severity": "warning",
      "incident_id": "188fd733",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 95,
        "buffer_cache_hit": 37,
        "work_mem_mb": 37,
        "oom_kills": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "188fd733_sre_9507768f",
    "content": "AWS monitoring: Memory 95%, swap 11.6GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T13:45:00Z",
      "severity": "warning",
      "incident_id": "188fd733",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 37,
        "swap_usage_gb": 11.6,
        "memory_usage_percent": 95,
        "wait_count": 27
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "188fd733_data_engineer_5309a73d",
    "content": "Pipeline memory issues: work_mem only 37MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T13:47:00Z",
      "severity": "warning",
      "incident_id": "188fd733",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 95,
        "oom_kills": 9,
        "wait_count": 27,
        "buffer_cache_hit": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "568e1720_dba_05e49da5",
    "content": "Index scan taking 454x longer on 'idx_orders_date' - 11601866 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T15:45:00Z",
      "severity": "warning",
      "incident_id": "568e1720",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 454,
        "wait_count": 28,
        "cpu_usage": 75
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "568e1720_data_engineer_b77c9228",
    "content": "Report timeout: order_search query exceeded 48336ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T15:46:00Z",
      "severity": "warning",
      "incident_id": "568e1720",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "query_time_before_ms": 96.51621234249589,
        "slowdown_factor": 454,
        "wait_count": 28
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "568e1720_developer_fe2c55fd",
    "content": "API timeout: 'order_search' taking 48336ms (was 96.5ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T15:57:00Z",
      "severity": "warning",
      "incident_id": "568e1720",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 11601866,
        "wait_count": 28,
        "slowdown_factor": 454,
        "query_time_before_ms": 96.51621234249589
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "568e1720_sre_496b4aae",
    "content": "Service alert: order_search queries 454x slower, CPU 75%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T15:57:00Z",
      "severity": "warning",
      "incident_id": "568e1720",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 454,
        "affected_query": "order_search",
        "rows_scanned": 11601866
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8fce32ba_sre_bb290a80",
    "content": "Critical: 10 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T16:49:00Z",
      "severity": "warning",
      "incident_id": "8fce32ba",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "wait_count": 10,
        "cascade_duration_min": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8fce32ba_developer_3575529b",
    "content": "Application errors: 49 transactions failed in 10min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T16:49:00Z",
      "severity": "warning",
      "incident_id": "8fce32ba",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 49,
        "wait_count": 10,
        "affected_tables": "payments"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8fce32ba_dba_333fdf49",
    "content": "Deadlock cascade on 'payments' - 10/min for 10min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T16:51:00Z",
      "severity": "warning",
      "incident_id": "8fce32ba",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 10,
        "affected_tables": "payments",
        "lock_wait_ms": 5329
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "05e79596_developer_e2052eaf",
    "content": "Database contention: 1234ms lock wait, 185 rollbacks",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T17:17:00Z",
      "severity": "warning",
      "incident_id": "05e79596",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 22,
        "deadlocks_per_minute": 6,
        "transaction_rollback": 185
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "05e79596_dba_9680639e",
    "content": "Deadlock storm: 26 blocked, 6/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T17:17:00Z",
      "severity": "critical",
      "incident_id": "05e79596",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 26,
        "affected_tables": "users",
        "transaction_rollback": 185,
        "lock_wait_ms": 1234,
        "wait_count": 22
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "05e79596_sre_c22e1d10",
    "content": "Alert: Deadlock storm - 6/min affecting 26 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T17:32:00Z",
      "severity": "critical",
      "incident_id": "05e79596",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 22,
        "blocked_queries": 26,
        "deadlocks_per_minute": 6,
        "cascade_duration_min": 9,
        "lock_wait_ms": 1234
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "9d21e2e0_developer_0cb236a7",
    "content": "Stale data on read replicas - 33s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T19:07:00Z",
      "severity": "info",
      "incident_id": "9d21e2e0",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 238,
        "network_latency_ms": 5,
        "wait_count": 48
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "9d21e2e0_dba_3f2d0bf4",
    "content": "Replication lag: 33s behind primary, 27941926 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T19:14:00Z",
      "severity": "info",
      "incident_id": "9d21e2e0",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 48,
        "wal_size_mb": 238,
        "network_latency_ms": 5,
        "replica_count": 2,
        "lag_bytes": 27941926
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "9d21e2e0_data_engineer_19343597",
    "content": "Report inconsistencies due to 33s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T19:19:00Z",
      "severity": "warning",
      "incident_id": "9d21e2e0",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 5,
        "replica_count": 2,
        "lag_seconds": 33,
        "lag_bytes": 27941926
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ab34a6ae_dba_b2b1ae54",
    "content": "Database refusing connections: SQLSTATE 53300 after 6415ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T19:53:00Z",
      "severity": "warning",
      "incident_id": "ab34a6ae",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 957.8149536894275,
        "error_code": "PG-53300",
        "max_connections": 1000,
        "spike_duration_min": 13.4,
        "wait_time_ms": 6415.44134897236
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ab34a6ae_data_engineer_be6207d0",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 6415ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T19:56:00Z",
      "severity": "warning",
      "incident_id": "ab34a6ae",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 957,
        "wait_time_ms": 6415.44134897236,
        "error_code": "PG-53300",
        "spike_duration_min": 13.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_d5a38a1dc3e24730",
    "content": "WAL archiving on schedule - 67 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T19:59:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8bc2f5f9_sre_b2773c9f",
    "content": "Emergency response needed: 87% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-28T22:58:00Z",
      "severity": "info",
      "incident_id": "8bc2f5f9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 33,
        "disk_usage_percent": 87,
        "days_until_full": 3
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8bc2f5f9_developer_921d59bd",
    "content": "Storage exhaustion in 3 days at current growth rate",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-28T23:05:00Z",
      "severity": "info",
      "incident_id": "8bc2f5f9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 39,
        "free_space_gb": 33,
        "disk_usage_percent": 87,
        "days_until_full": 3
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8bc2f5f9_dba_26e6b22f",
    "content": "Storage alert: Growing 87GB/day, full in 3 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-28T23:07:00Z",
      "severity": "info",
      "incident_id": "8bc2f5f9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 33,
        "disk_usage_percent": 87,
        "wal_size_gb": 39,
        "wait_count": 21,
        "growth_rate_gb_per_day": 87
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8bc2f5f9_data_engineer_8d052b8b",
    "content": "Batch processing consuming 87GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-28T23:09:00Z",
      "severity": "warning",
      "incident_id": "8bc2f5f9",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 21,
        "days_until_full": 3,
        "wal_size_gb": 39,
        "free_space_gb": 33,
        "disk_usage_percent": 87
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_5d3dc10fe73949e5",
    "content": "Connection pool stable at 189 connections (21% utilization)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T04:29:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "40fb39c3_data_engineer_7b7d5dd3",
    "content": "Spark executors holding 208 idle connections, 153MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-29T06:43:00Z",
      "severity": "critical",
      "incident_id": "40fb39c3",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 548,
        "threshold": 54,
        "idle_connections": 208,
        "leak_rate_per_hour": 5.7,
        "leak_duration_hours": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "40fb39c3_sre_87d8d5d4",
    "content": "Anomaly: Idle connections increasing 6/hour for past 10 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-29T06:44:00Z",
      "severity": "critical",
      "incident_id": "40fb39c3",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 548,
        "wait_count": 30,
        "leak_duration_hours": 10,
        "active_connections": 548.2080873588633
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "40fb39c3_dba_f44db335",
    "content": "WARNING: 208 idle connections detected (leak rate: 5.7/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-29T06:50:00Z",
      "severity": "critical",
      "incident_id": "40fb39c3",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "idle_connections": 208,
        "active_connections": 548.2080873588633,
        "leak_duration_hours": 10,
        "leak_rate_per_hour": 5.7
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee604ab0_sre_4e96f71d",
    "content": "Projection: Storage full in 1 days (68GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-29T07:59:00Z",
      "severity": "warning",
      "incident_id": "ee604ab0",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 94,
        "growth_rate_gb_per_day": 68,
        "days_until_full": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee604ab0_developer_6dac0f26",
    "content": "Storage exhaustion in 1 days at current growth rate",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T08:02:00Z",
      "severity": "warning",
      "incident_id": "ee604ab0",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 36,
        "wal_size_gb": 59,
        "disk_usage_percent": 94,
        "wait_count": 18,
        "days_until_full": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee604ab0_data_engineer_adfa5036",
    "content": "Data retention issue: 59GB logs, 94% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-29T08:06:00Z",
      "severity": "warning",
      "incident_id": "ee604ab0",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 94,
        "wal_size_gb": 59,
        "free_space_gb": 36,
        "growth_rate_gb_per_day": 68,
        "days_until_full": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ee604ab0_dba_324157e1",
    "content": "Disk space critical: 94% used, growth rate 68GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-29T08:07:00Z",
      "severity": "warning",
      "incident_id": "ee604ab0",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 18,
        "free_space_gb": 36,
        "days_until_full": 1,
        "disk_usage_percent": 94
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "196d26f1_dba_efba94ce",
    "content": "Storage alert: Growing 88GB/day, full in 1 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-29T15:01:00Z",
      "severity": "info",
      "incident_id": "196d26f1",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 15,
        "disk_usage_percent": 94,
        "wal_size_gb": 80,
        "growth_rate_gb_per_day": 88,
        "wait_count": 44
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "196d26f1_developer_d52cb355",
    "content": "Database storage 94% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T15:07:00Z",
      "severity": "info",
      "incident_id": "196d26f1",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 15,
        "wal_size_gb": 80,
        "wait_count": 44,
        "days_until_full": 1
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5f4e695e_data_engineer_9200611c",
    "content": "ETL bottleneck: 'user_dashboard' step taking 26215ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-29T15:34:00Z",
      "severity": "warning",
      "incident_id": "5f4e695e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 16,
        "rows_scanned": 43175779,
        "slowdown_factor": 345,
        "query_time_before_ms": 100.63919072831581
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5f4e695e_sre_b6e30708",
    "content": "Latency alarm: 'user_dashboard' breached 26215ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-29T15:39:00Z",
      "severity": "warning",
      "incident_id": "5f4e695e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 100.63919072831581,
        "wait_count": 16,
        "rows_scanned": 43175779
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "18ec4c8f_developer_806a4e70",
    "content": "Database swapping 2.9GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T16:06:00Z",
      "severity": "warning",
      "incident_id": "18ec4c8f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 95,
        "buffer_cache_hit": 60,
        "work_mem_mb": 39,
        "oom_kills": 8
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "18ec4c8f_dba_d537ec4f",
    "content": "Memory exhaustion: 95% RAM, 2.9GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-29T16:15:00Z",
      "severity": "warning",
      "incident_id": "18ec4c8f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 8,
        "work_mem_mb": 39,
        "memory_usage_percent": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "18ec4c8f_sre_634e1095",
    "content": "Resource exhaustion: 8 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-29T16:21:00Z",
      "severity": "warning",
      "incident_id": "18ec4c8f",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 39,
        "buffer_cache_hit": 60,
        "wait_count": 10,
        "swap_usage_gb": 2.9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "db43e2c1_data_engineer_919e06e5",
    "content": "Batch job delayed by autovacuum processing 43928062 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-29T16:57:00Z",
      "severity": "warning",
      "incident_id": "db43e2c1",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 27,
        "dead_tuples": 43928062,
        "vacuum_duration_min": 149,
        "io_usage_percent": 75
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "db43e2c1_developer_c203604d",
    "content": "Service degradation during vacuum of 'events' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T17:09:00Z",
      "severity": "warning",
      "incident_id": "db43e2c1",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 66,
        "wait_count": 27,
        "vacuum_duration_min": 149,
        "io_usage_percent": 75,
        "dead_tuples": 43928062
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "284fbf8d_data_engineer_eda5b171",
    "content": "ETL blocked by deadlocks on 'users' (26/min)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-29T22:12:00Z",
      "severity": "warning",
      "incident_id": "284fbf8d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 48,
        "cascade_duration_min": 3,
        "blocked_queries": 57,
        "lock_wait_ms": 2035,
        "transaction_rollback": 161
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "284fbf8d_sre_a3f8c61b",
    "content": "Service disruption: 161 failed transactions in 3min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-29T22:16:00Z",
      "severity": "info",
      "incident_id": "284fbf8d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 48,
        "cascade_duration_min": 3,
        "transaction_rollback": 161
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "284fbf8d_developer_e5d2e3cd",
    "content": "TransactionRollback: deadlock after 2035ms on 'users'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-29T22:19:00Z",
      "severity": "info",
      "incident_id": "284fbf8d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 26,
        "lock_wait_ms": 2035,
        "affected_tables": "users"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "284fbf8d_dba_b275afbc",
    "content": "Circular wait on 'users' - 2035ms average wait time",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-29T22:19:00Z",
      "severity": "info",
      "incident_id": "284fbf8d",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 26,
        "affected_tables": "users",
        "lock_wait_ms": 2035,
        "cascade_duration_min": 3
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_5fee7fabaee84078",
    "content": "Statistics updated for 36 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T02:26:46Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "77a67afb_data_engineer_f3892d74",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 21974ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T09:13:00Z",
      "severity": "critical",
      "incident_id": "77a67afb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 21,
        "active_connections": 981.7612960112631,
        "error_code": "PG-53300"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "77a67afb_developer_319ec4f2",
    "content": "HikariPool-1 - Connection timeout after 21974ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T09:16:00Z",
      "severity": "warning",
      "incident_id": "77a67afb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 5.2,
        "wait_time_ms": 21974.94198801864,
        "max_connections": 1000,
        "wait_ms": 21974,
        "active_connections": 981.7612960112631
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "31a4e5f7_developer_9617e9df",
    "content": "JDBC connection failed after 10989ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T10:11:00Z",
      "severity": "info",
      "incident_id": "31a4e5f7",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 960,
        "wait_time_ms": 10989.254757726492,
        "error_code": "PG-53300",
        "threshold": 86
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "31a4e5f7_sre_462aee23",
    "content": "Monitoring: Connection pool 86% full, 42 requests queued",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T10:18:00Z",
      "severity": "info",
      "incident_id": "31a4e5f7",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 960.094043814724,
        "active": 960,
        "spike_duration_min": 48.0,
        "wait_ms": 10989,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "31a4e5f7_data_engineer_4c669a1b",
    "content": "DBT run failed: waited 10989ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T10:28:00Z",
      "severity": "info",
      "incident_id": "31a4e5f7",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 42,
        "wait_ms": 10989,
        "wait_time_ms": 10989.254757726492
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "28875d3b_developer_c843f0e3",
    "content": "Application holding 164 unused connections for 22 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T10:35:00Z",
      "severity": "info",
      "incident_id": "28875d3b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 676,
        "memory_usage_mb": 162,
        "idle_connections": 164,
        "active_connections": 676.6757750731184
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "28875d3b_sre_71df4f0b",
    "content": "Connection leak: 17.4/hr growth rate detected over 22h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T10:41:00Z",
      "severity": "info",
      "incident_id": "28875d3b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 22,
        "threshold": 67,
        "wait_count": 5
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "28875d3b_data_engineer_0a929d99",
    "content": "Pipeline issue: 164 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T10:48:00Z",
      "severity": "info",
      "incident_id": "28875d3b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 676,
        "idle_connections": 164,
        "leak_duration_hours": 22,
        "threshold": 67,
        "memory_usage_mb": 162
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6ea230ff_data_engineer_1b5fc6f3",
    "content": "Pipeline memory issues: work_mem only 33MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T12:00:00Z",
      "severity": "warning",
      "incident_id": "6ea230ff",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 45,
        "swap_usage_gb": 13.3,
        "oom_kills": 6,
        "work_mem_mb": 33
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6ea230ff_sre_eed14fb5",
    "content": "AWS monitoring: Memory 91%, swap 13.3GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T12:01:00Z",
      "severity": "info",
      "incident_id": "6ea230ff",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "swap_usage_gb": 13.3,
        "work_mem_mb": 33,
        "wait_count": 37,
        "oom_kills": 6
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6ea230ff_developer_fcfc95a5",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 45%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T12:07:00Z",
      "severity": "info",
      "incident_id": "6ea230ff",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 6,
        "memory_usage_percent": 91,
        "swap_usage_gb": 13.3
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "31f008e5_developer_44363bb2",
    "content": "Database contention: 5187ms lock wait, 189 rollbacks",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T15:14:00Z",
      "severity": "warning",
      "incident_id": "31f008e5",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 38,
        "affected_tables": "payments",
        "cascade_duration_min": 6,
        "deadlocks_per_minute": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "31f008e5_dba_936698ac",
    "content": "Deadlock storm: 30 blocked, 7/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-30T15:21:00Z",
      "severity": "warning",
      "incident_id": "31f008e5",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 189,
        "deadlocks_per_minute": 7,
        "affected_tables": "payments",
        "blocked_queries": 30
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "31f008e5_sre_3af3ae9d",
    "content": "Alert: Deadlock storm - 7/min affecting 30 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T15:28:00Z",
      "severity": "warning",
      "incident_id": "31f008e5",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 30,
        "lock_wait_ms": 5187,
        "cascade_duration_min": 6,
        "deadlocks_per_minute": 7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7e919f65775545b5",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T15:31:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8acdda06_data_engineer_f485d5ba",
    "content": "Batch process deadlocked - 1min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T17:20:00Z",
      "severity": "warning",
      "incident_id": "8acdda06",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "blocked_queries": 34,
        "deadlocks_per_minute": 27,
        "wait_count": 35,
        "lock_wait_ms": 3088
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8acdda06_developer_4cc38409",
    "content": "Transaction retry exhausted - 27 deadlocks/min on 'shipments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-11-30T17:24:00Z",
      "severity": "warning",
      "incident_id": "8acdda06",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 23,
        "blocked_queries": 34,
        "lock_wait_ms": 3088
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8acdda06_sre_991c7a3b",
    "content": "System alert: 3088ms lock wait impacting 23 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T17:29:00Z",
      "severity": "warning",
      "incident_id": "8acdda06",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 34,
        "transaction_rollback": 23,
        "deadlocks_per_minute": 27,
        "affected_tables": "shipments",
        "lock_wait_ms": 3088
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3b8402f8_sre_d832c1f0",
    "content": "Replication monitoring: 105s lag on 1 replicas",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T21:28:00Z",
      "severity": "info",
      "incident_id": "3b8402f8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 4755,
        "lag_seconds": 105,
        "lag_bytes": 39721506
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3b8402f8_data_engineer_0f1cbd4d",
    "content": "Pipeline data quality affected by 105s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-11-30T21:28:00Z",
      "severity": "warning",
      "incident_id": "3b8402f8",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 105,
        "replica_count": 1,
        "wal_size_mb": 4755,
        "wait_count": 30,
        "network_latency_ms": 30
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "98ccd406_dba_c6e6ed0d",
    "content": "Critical memory pressure: cache hit 42%, swap 14.8GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-11-30T22:38:00Z",
      "severity": "critical",
      "incident_id": "98ccd406",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 49,
        "oom_kills": 9,
        "swap_usage_gb": 14.8,
        "memory_usage_percent": 94,
        "buffer_cache_hit": 42
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "98ccd406_sre_293228ae",
    "content": "System swapping 14.8GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-11-30T22:40:00Z",
      "severity": "critical",
      "incident_id": "98ccd406",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 21,
        "memory_usage_percent": 94,
        "oom_kills": 9,
        "work_mem_mb": 49
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "eabf0fe7_developer_7a7e201e",
    "content": "Deadlock rate 44/min causing 66 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-01T06:50:00Z",
      "severity": "warning",
      "incident_id": "eabf0fe7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 6916,
        "cascade_duration_min": 14,
        "blocked_queries": 66
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "eabf0fe7_sre_b6a0f3f5",
    "content": "Incident: Lock contention causing 66 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-01T06:50:00Z",
      "severity": "warning",
      "incident_id": "eabf0fe7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "deadlocks_per_minute": 44,
        "blocked_queries": 66,
        "cascade_duration_min": 14
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "eabf0fe7_data_engineer_542de30f",
    "content": "Pipeline stalled: 66 queries waiting for 6916ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-01T07:10:00Z",
      "severity": "warning",
      "incident_id": "eabf0fe7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 5,
        "lock_wait_ms": 6916,
        "blocked_queries": 66
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_452b389b66e84caa",
    "content": "CPU utilization stable at 23%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-01T09:49:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3b12063f27c44139",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-01T13:56:57Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "097a9fc4_dba_55856c2f",
    "content": "PANIC: connection limit exceeded - 968/1000 after 20.2min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-01T15:04:00Z",
      "severity": "critical",
      "incident_id": "097a9fc4",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 98,
        "wait_count": 19,
        "error_code": "PG-53300",
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "097a9fc4_sre_748a69e2",
    "content": "AWS RDS Alert: Connection count 968, wait time 29136ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-01T15:05:00Z",
      "severity": "critical",
      "incident_id": "097a9fc4",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 98,
        "wait_ms": 29136,
        "active": 968,
        "active_connections": 968.6571142922884,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "112f64e4_dba_7fe22ffe",
    "content": "Lock escalation: 110 transactions rolled back on 'orders'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-01T16:48:00Z",
      "severity": "critical",
      "incident_id": "112f64e4",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 110,
        "deadlocks_per_minute": 40,
        "wait_count": 39,
        "blocked_queries": 28,
        "lock_wait_ms": 2479
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "112f64e4_sre_50cb1fba",
    "content": "System alert: 2479ms lock wait impacting 110 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-01T16:54:00Z",
      "severity": "critical",
      "incident_id": "112f64e4",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 110,
        "affected_tables": "orders",
        "wait_count": 39,
        "blocked_queries": 28,
        "deadlocks_per_minute": 40
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "951a37b7_data_engineer_3b34f576",
    "content": "Batch job delayed by autovacuum processing 89308818 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-01T22:03:00Z",
      "severity": "info",
      "incident_id": "951a37b7",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "orders",
        "table_bloat_percent": 60,
        "vacuum_duration_min": 347,
        "io_usage_percent": 65
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "951a37b7_developer_9d86df64",
    "content": "Service degradation during vacuum of 'orders' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-01T22:07:00Z",
      "severity": "info",
      "incident_id": "951a37b7",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 347,
        "dead_tuples": 89308818,
        "table_bloat_percent": 60,
        "tables_affected": "orders",
        "io_usage_percent": 65
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "951a37b7_sre_667fc105",
    "content": "IO spike: 65% during vacuum of 'orders'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-01T22:21:00Z",
      "severity": "info",
      "incident_id": "951a37b7",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 89308818,
        "vacuum_duration_min": 347,
        "io_usage_percent": 65
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "79f22d3a_dba_66726dd4",
    "content": "pg_stat_activity shows 367 idle connections consuming 160MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T06:07:00Z",
      "severity": "warning",
      "incident_id": "79f22d3a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 367,
        "wait_count": 32,
        "memory_usage_mb": 160
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "79f22d3a_developer_39b9fc53",
    "content": "Application holding 367 unused connections for 19 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T06:17:00Z",
      "severity": "warning",
      "incident_id": "79f22d3a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "memory_usage_mb": 160,
        "leak_rate_per_hour": 16.1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "79f22d3a_sre_db7cd2bb",
    "content": "Connection leak: 16.1/hr growth rate detected over 19h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T06:17:00Z",
      "severity": "warning",
      "incident_id": "79f22d3a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 19,
        "active_connections": 622.6063209518859,
        "wait_count": 32,
        "threshold": 62
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_dcd8cb26783a4232",
    "content": "CPU utilization stable at 32%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T08:23:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7b9b0e6c_sre_91277129",
    "content": "Memory alert: 93% utilized, 9 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T08:26:00Z",
      "severity": "critical",
      "incident_id": "7b9b0e6c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 46,
        "wait_count": 17,
        "work_mem_mb": 45,
        "swap_usage_gb": 12.0,
        "memory_usage_percent": 93
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9178ead4_sre_eb5e3975",
    "content": "Storage capacity: 1 days until exhaustion",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T08:26:00Z",
      "severity": "critical",
      "incident_id": "9178ead4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 88,
        "free_space_gb": 30,
        "days_until_full": 1
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "711477a4_developer_59f06b1d",
    "content": "WARNING: 377 connections not returned to pool after 15h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T08:27:00Z",
      "severity": "critical",
      "incident_id": "711477a4",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 16.0,
        "memory_usage_mb": 403,
        "leak_duration_hours": 15
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9178ead4_data_engineer_415f93d3",
    "content": "Storage crisis: 1 days until pipeline failure",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T08:28:00Z",
      "severity": "critical",
      "incident_id": "9178ead4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 85,
        "disk_usage_percent": 88,
        "free_space_gb": 30,
        "days_until_full": 1,
        "wait_count": 47
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "711477a4_dba_7f91d811",
    "content": "WARNING: 377 idle connections detected (leak rate: 16.0/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T08:29:00Z",
      "severity": "critical",
      "incident_id": "711477a4",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 686,
        "max_connections": 1000,
        "memory_usage_mb": 403,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7b9b0e6c_data_engineer_dc7c630a",
    "content": "Memory pressure: 93% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T08:30:00Z",
      "severity": "critical",
      "incident_id": "7b9b0e6c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 46,
        "wait_count": 17,
        "oom_kills": 9
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7b9b0e6c_developer_05b17196",
    "content": "Out of memory errors after reaching 93% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T08:34:00Z",
      "severity": "warning",
      "incident_id": "7b9b0e6c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 45,
        "memory_usage_percent": 93,
        "oom_kills": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "711477a4_data_engineer_3e56c435",
    "content": "Spark executors holding 377 idle connections, 403MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T08:39:00Z",
      "severity": "critical",
      "incident_id": "711477a4",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 16.0,
        "memory_usage_mb": 403,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "9178ead4_developer_47894e0e",
    "content": "Critical: Database disk 88% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T08:41:00Z",
      "severity": "critical",
      "incident_id": "9178ead4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 54,
        "days_until_full": 1,
        "wait_count": 47
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "37e2656f_sre_362b7bbd",
    "content": "Latency alarm: 'order_search' breached 6509ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T12:47:00Z",
      "severity": "warning",
      "incident_id": "37e2656f",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 83,
        "slowdown_factor": 414,
        "query_time_before_ms": 54.94781501634405,
        "affected_query": "order_search"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37e2656f_data_engineer_53babc6b",
    "content": "ETL bottleneck: 'order_search' step taking 6509ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T12:52:00Z",
      "severity": "warning",
      "incident_id": "37e2656f",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 29,
        "query_time_before_ms": 54.94781501634405,
        "affected_query": "order_search",
        "cpu_usage": 83,
        "slowdown_factor": 414
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37e2656f_dba_ca65cd93",
    "content": "Sequential scan on 21964829 rows - index 'idx_users_email' not used, CPU at 83%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T12:52:00Z",
      "severity": "warning",
      "incident_id": "37e2656f",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 21964829,
        "query_time_before_ms": 54.94781501634405,
        "cpu_usage": 83,
        "wait_count": 29,
        "query_time_after_ms": 6509.892853681269
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "37e2656f_developer_25e2d61a",
    "content": "Application query 'order_search' timeout after 6509ms at 83% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T12:54:00Z",
      "severity": "warning",
      "incident_id": "37e2656f",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 54.94781501634405,
        "wait_count": 29,
        "affected_query": "order_search",
        "rows_scanned": 21964829,
        "cpu_usage": 83
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "df0e3727_sre_e2f4e727",
    "content": "Network issue causing 12ms latency, 77s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T13:12:00Z",
      "severity": "warning",
      "incident_id": "df0e3727",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 29,
        "replica_count": 1,
        "wal_size_mb": 2546,
        "network_latency_ms": 12
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "df0e3727_developer_055b0dab",
    "content": "Stale data on read replicas - 77s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T13:19:00Z",
      "severity": "warning",
      "incident_id": "df0e3727",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 1,
        "lag_seconds": 77,
        "network_latency_ms": 12,
        "lag_bytes": 48452623,
        "wal_size_mb": 2546
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "393b6a6b_dba_7c10dfbd",
    "content": "Resource leak: 127 connections idle, 572 active, total memory 272MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T14:33:00Z",
      "severity": "info",
      "incident_id": "393b6a6b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 15.5,
        "max_connections": 1000,
        "threshold": 57,
        "idle_connections": 127
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "393b6a6b_sre_c32cb314",
    "content": "Grafana: Connection leak - 15.5 connections/hour over 17h",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T14:35:00Z",
      "severity": "info",
      "incident_id": "393b6a6b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 57,
        "active_connections": 572.4198434529013,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "393b6a6b_data_engineer_6307e707",
    "content": "Data job resource leak: 16/hour accumulation rate",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T14:38:00Z",
      "severity": "warning",
      "incident_id": "393b6a6b",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 57,
        "idle_connections": 127,
        "wait_count": 43,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "92c729fa_dba_c4c286c3",
    "content": "Buffer cache hit ratio dropped to 48% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T17:06:00Z",
      "severity": "critical",
      "incident_id": "92c729fa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 48,
        "swap_usage_gb": 14.2,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "92c729fa_sre_50a31c78",
    "content": "AWS monitoring: Memory 98%, swap 14.2GB",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T17:21:00Z",
      "severity": "critical",
      "incident_id": "92c729fa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 2,
        "swap_usage_gb": 14.2,
        "memory_usage_percent": 98,
        "work_mem_mb": 6,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "92c729fa_developer_321a56cb",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 48%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T17:22:00Z",
      "severity": "warning",
      "incident_id": "92c729fa",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 48,
        "oom_kills": 2,
        "memory_usage_percent": 98
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "76b78666_sre_4899ba98",
    "content": "Critical: Replication lag exceeding SLA (276s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T17:23:00Z",
      "severity": "warning",
      "incident_id": "76b78666",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 13,
        "lag_bytes": 11403515,
        "replica_count": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "76b78666_developer_0ad89576",
    "content": "Data consistency issue: replicas lagging by 11403515 bytes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T17:26:00Z",
      "severity": "warning",
      "incident_id": "76b78666",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 2287,
        "replica_count": 4,
        "wait_count": 13
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "76b78666_data_engineer_fd569e52",
    "content": "Data freshness issue: 11403515 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-02T17:32:00Z",
      "severity": "warning",
      "incident_id": "76b78666",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 276,
        "wal_size_mb": 2287,
        "lag_bytes": 11403515,
        "replica_count": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5f6e7ba8_developer_08044d50",
    "content": "JDBC connection failed after 6527ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-02T18:31:00Z",
      "severity": "info",
      "incident_id": "5f6e7ba8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 15,
        "error_code": "PG-53300",
        "threshold": 87,
        "wait_ms": 6527,
        "active": 981
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5f6e7ba8_dba_f5708e26",
    "content": "ERROR: too many connections for role 'app_user' (current: 981, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T18:35:00Z",
      "severity": "info",
      "incident_id": "5f6e7ba8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 15,
        "max_connections": 1000,
        "spike_duration_min": 11.3,
        "error_code": "PG-53300",
        "active": 981
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "978d0478_dba_9ff66b89",
    "content": "Aggressive vacuum on 'logs' - 66246011 dead tuples, 79% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-02T18:52:00Z",
      "severity": "warning",
      "incident_id": "978d0478",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 79,
        "dead_tuples": 66246011,
        "io_usage_percent": 65
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "978d0478_sre_c8441eb7",
    "content": "Disk usage alert: 'logs' table 79% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-02T19:09:00Z",
      "severity": "warning",
      "incident_id": "978d0478",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 66246011,
        "wait_count": 49,
        "tables_affected": "logs",
        "table_bloat_percent": 79
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_a71bccd50d964123",
    "content": "Statistics updated for 9 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T02:41:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "6efc6f09_dba_1a85b1fb",
    "content": "Read replica out of sync by 138 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-03T06:40:00Z",
      "severity": "info",
      "incident_id": "6efc6f09",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 1,
        "wait_count": 41,
        "lag_bytes": 69814837,
        "network_latency_ms": 24,
        "wal_size_mb": 4794
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6efc6f09_data_engineer_2c0458f7",
    "content": "Analytics queries on stale data - replica 138s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T06:44:00Z",
      "severity": "info",
      "incident_id": "6efc6f09",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 41,
        "network_latency_ms": 24,
        "wal_size_mb": 4794
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6efc6f09_developer_9928edaf",
    "content": "Read-after-write inconsistency - replica 138s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-03T06:52:00Z",
      "severity": "info",
      "incident_id": "6efc6f09",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 41,
        "lag_seconds": 138,
        "wal_size_mb": 4794,
        "replica_count": 1,
        "network_latency_ms": 24
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "78a65423_developer_599b1be6",
    "content": "Memory leak in connection handling - 13 connections/hour, 131MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-03T09:41:00Z",
      "severity": "info",
      "incident_id": "78a65423",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 5,
        "max_connections": 1000,
        "wait_count": 42,
        "leak_rate_per_hour": 12.6
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "78a65423_data_engineer_ab2d04be",
    "content": "Batch job leak: 12.6 connections/hour over 5h runtime",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T09:42:00Z",
      "severity": "warning",
      "incident_id": "78a65423",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 42,
        "max_connections": 1000,
        "active_connections": 527.0719693004627,
        "leak_duration_hours": 5,
        "memory_usage_mb": 131
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "78a65423_dba_3f170ef1",
    "content": "Connection leak: idle_in_transaction growing at 13/hour for 5h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-03T09:42:00Z",
      "severity": "info",
      "incident_id": "78a65423",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 12.6,
        "wait_count": 42,
        "idle_connections": 191
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ebbb8ab5_data_engineer_39ed30d5",
    "content": "Report timeout: order_search query exceeded 54764ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T13:39:00Z",
      "severity": "warning",
      "incident_id": "ebbb8ab5",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 35051912,
        "query_time_after_ms": 54764.532458652255,
        "wait_count": 21,
        "slowdown_factor": 120,
        "cpu_usage": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "ebbb8ab5_dba_ef44fad5",
    "content": "Index scan taking 120x longer on 'idx_sessions_user' - 35051912 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-03T13:42:00Z",
      "severity": "info",
      "incident_id": "ebbb8ab5",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "query_time_before_ms": 187.0576824787028,
        "rows_scanned": 35051912,
        "slowdown_factor": 120
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6e7076e4_sre_34991600",
    "content": "Disk space alarm: 86% used, 41GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-03T20:36:00Z",
      "severity": "info",
      "incident_id": "6e7076e4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 41,
        "wait_count": 21,
        "growth_rate_gb_per_day": 40,
        "disk_usage_percent": 86,
        "wal_size_gb": 98
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7076e4_dba_5a4cfdfc",
    "content": "Emergency: 41GB free, database will halt in 6 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-03T20:40:00Z",
      "severity": "info",
      "incident_id": "6e7076e4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 6,
        "growth_rate_gb_per_day": 40,
        "free_space_gb": 41,
        "wait_count": 21
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7076e4_data_engineer_d93913dc",
    "content": "Data retention issue: 98GB logs, 86% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T20:46:00Z",
      "severity": "info",
      "incident_id": "6e7076e4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 40,
        "disk_usage_percent": 86,
        "wal_size_gb": 98,
        "free_space_gb": 41,
        "wait_count": 21
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7076e4_developer_0337d017",
    "content": "Write failures imminent - 41GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-03T20:49:00Z",
      "severity": "info",
      "incident_id": "6e7076e4",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 6,
        "wait_count": 21,
        "free_space_gb": 41,
        "disk_usage_percent": 86,
        "growth_rate_gb_per_day": 40
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5b3e5daa_data_engineer_6468df84",
    "content": "ETL bottleneck: 'report_aggregate' step taking 45572ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-03T21:25:00Z",
      "severity": "warning",
      "incident_id": "5b3e5daa",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "query_time_before_ms": 126.13839307265928,
        "cpu_usage": 97,
        "slowdown_factor": 167
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5b3e5daa_dba_cbe8f930",
    "content": "Sequential scan on 42586064 rows - index 'idx_products_sku' not used, CPU at 97%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-03T21:26:00Z",
      "severity": "warning",
      "incident_id": "5b3e5daa",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 45572.09999787804,
        "slowdown_factor": 167,
        "rows_scanned": 42586064,
        "wait_count": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5b3e5daa_developer_a2d84362",
    "content": "Application query 'report_aggregate' timeout after 45572ms at 97% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-03T21:30:00Z",
      "severity": "warning",
      "incident_id": "5b3e5daa",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "query_time_after_ms": 45572.09999787804,
        "wait_count": 20,
        "query_time_before_ms": 126.13839307265928,
        "cpu_usage": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_05db1ad1f6864a8f",
    "content": "Connection recycling: 67 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T02:30:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "5ea4f927_data_engineer_725bdf5f",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 29198ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T07:31:00Z",
      "severity": "warning",
      "incident_id": "5ea4f927",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "threshold": 85,
        "active_connections": 974.2562870304837
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "5ea4f927_developer_b10e1a05",
    "content": "HikariPool-1 - Connection timeout after 29198ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T07:41:00Z",
      "severity": "info",
      "incident_id": "5ea4f927",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_time_ms": 29198.43685210064,
        "threshold": 85,
        "wait_ms": 29198
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_54ff4759dea94a31",
    "content": "Memory usage normal - 73.5GB of 128GB used",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T09:00:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "21957d23_sre_f0503424",
    "content": "Alert: WAL accumulation 4898MB, lag 185s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T09:41:00Z",
      "severity": "warning",
      "incident_id": "21957d23",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 185,
        "replica_count": 3,
        "wal_size_mb": 4898
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "21957d23_developer_d9ee8e59",
    "content": "Read-after-write inconsistency - replica 185s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T09:58:00Z",
      "severity": "warning",
      "incident_id": "21957d23",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 20,
        "wal_size_mb": 4898,
        "wait_count": 27,
        "lag_seconds": 185
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "cbac2d8e_dba_a421a4a7",
    "content": "Critical memory pressure: cache hit 43%, swap 7.7GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T10:47:00Z",
      "severity": "warning",
      "incident_id": "cbac2d8e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "wait_count": 24,
        "buffer_cache_hit": 43
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "cbac2d8e_sre_611af739",
    "content": "System swapping 7.7GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T10:53:00Z",
      "severity": "warning",
      "incident_id": "cbac2d8e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 2,
        "memory_usage_percent": 91,
        "swap_usage_gb": 7.7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "cbac2d8e_developer_65d4376f",
    "content": "Database OOM: 2 connections terminated at 91% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T10:55:00Z",
      "severity": "warning",
      "incident_id": "cbac2d8e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 43,
        "wait_count": 24,
        "swap_usage_gb": 7.7,
        "memory_usage_percent": 91,
        "work_mem_mb": 61
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "cbac2d8e_data_engineer_d4528443",
    "content": "ETL job killed: OOM at 91% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T11:00:00Z",
      "severity": "warning",
      "incident_id": "cbac2d8e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 43,
        "swap_usage_gb": 7.7,
        "wait_count": 24,
        "memory_usage_percent": 91,
        "oom_kills": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "3bd410f3_developer_bd215b2f",
    "content": "Stale data on read replicas - 87s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T12:24:00Z",
      "severity": "warning",
      "incident_id": "3bd410f3",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 87,
        "replica_count": 4,
        "wait_count": 18
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3bd410f3_sre_4fb024f6",
    "content": "Network issue causing 31ms latency, 87s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T12:32:00Z",
      "severity": "critical",
      "incident_id": "3bd410f3",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 38093194,
        "network_latency_ms": 31,
        "lag_seconds": 87
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3bd410f3_data_engineer_947c5a32",
    "content": "Report inconsistencies due to 87s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T12:40:00Z",
      "severity": "critical",
      "incident_id": "3bd410f3",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "wal_size_mb": 886,
        "wait_count": 18,
        "lag_seconds": 87
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e94e27d8_developer_07d98ee8",
    "content": "HikariPool-1 - Connection timeout after 25664ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T14:41:00Z",
      "severity": "warning",
      "incident_id": "e94e27d8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 978,
        "error_code": "PG-53300",
        "active_connections": 978.4111261811167,
        "max_connections": 1000,
        "spike_duration_min": 45.6
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e94e27d8_dba_55aab4fb",
    "content": "Database refusing connections: SQLSTATE 53300 after 25664ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T14:47:00Z",
      "severity": "warning",
      "incident_id": "e94e27d8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 25664.48776646621,
        "max_connections": 1000,
        "threshold": 87
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e94e27d8_data_engineer_76ed0ca5",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 25664ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T14:52:00Z",
      "severity": "warning",
      "incident_id": "e94e27d8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 45.6,
        "active": 978,
        "error_code": "PG-53300",
        "wait_time_ms": 25664.48776646621
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e94e27d8_sre_7eeaa935",
    "content": "Alert: Connection saturation 978/1000 for 46 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T14:53:00Z",
      "severity": "warning",
      "incident_id": "e94e27d8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 87,
        "active_connections": 978.4111261811167,
        "wait_count": 34,
        "wait_ms": 25664
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "dae4452e_dba_560f2a1b",
    "content": "Connection pool saturation: 972 active, 45 waiting, threshold 88% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T16:55:00Z",
      "severity": "warning",
      "incident_id": "dae4452e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 972.3058453672201,
        "max_connections": 1000,
        "wait_time_ms": 7073.54593521305
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "dae4452e_developer_0f8f7b3b",
    "content": "Database unavailable after 7073ms wait - circuit breaker opened at 88%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T16:58:00Z",
      "severity": "warning",
      "incident_id": "dae4452e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 27.4,
        "active_connections": 972.3058453672201,
        "wait_count": 45
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "dae4452e_data_engineer_1cee58c5",
    "content": "Data pipeline blocked: connection pool at 88% capacity",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T17:09:00Z",
      "severity": "warning",
      "incident_id": "dae4452e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 972,
        "max_connections": 1000,
        "spike_duration_min": 27.4,
        "wait_ms": 7073,
        "active_connections": 972.3058453672201
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_038667a02efb44fb",
    "content": "CPU utilization stable at 35%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T20:47:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4f195630_sre_fc127fa6",
    "content": "Connection leak: 14.1/hr growth rate detected over 14h period",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-04T21:48:00Z",
      "severity": "info",
      "incident_id": "4f195630",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 175,
        "idle_connections": 272,
        "leak_duration_hours": 14,
        "active_connections": 688.7811351838425
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "95014458_dba_af5b2a6b",
    "content": "Replication lag: 88s behind primary, 53731288 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T21:52:00Z",
      "severity": "warning",
      "incident_id": "95014458",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 88,
        "lag_bytes": 53731288,
        "wait_count": 42
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4f195630_dba_1f533762",
    "content": "pg_stat_activity shows 272 idle connections consuming 175MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-04T21:52:00Z",
      "severity": "info",
      "incident_id": "4f195630",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 688.7811351838425,
        "max_connections": 1000,
        "idle_connections": 272,
        "active": 688,
        "wait_count": 35
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4f195630_developer_aa6c4f75",
    "content": "Application holding 272 unused connections for 14 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T21:54:00Z",
      "severity": "info",
      "incident_id": "4f195630",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 688.7811351838425,
        "memory_usage_mb": 175,
        "threshold": 68
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "95014458_developer_1675315c",
    "content": "Stale data on read replicas - 88s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-04T21:55:00Z",
      "severity": "warning",
      "incident_id": "95014458",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "network_latency_ms": 30,
        "lag_bytes": 53731288
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4f195630_data_engineer_96727182",
    "content": "Pipeline issue: 272 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-04T21:55:00Z",
      "severity": "warning",
      "incident_id": "4f195630",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 14,
        "wait_count": 35,
        "threshold": 68,
        "active": 688
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6220265b_dba_170636cb",
    "content": "PANIC: connection limit exceeded - 990/1000 after 59.4min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T06:00:00Z",
      "severity": "critical",
      "incident_id": "6220265b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 990,
        "max_connections": 1000,
        "threshold": 92,
        "wait_ms": 8424
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6220265b_sre_b1be03ee",
    "content": "AWS RDS Alert: Connection count 990, wait time 8424ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T06:03:00Z",
      "severity": "critical",
      "incident_id": "6220265b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 990,
        "active_connections": 990.2737037665873,
        "wait_ms": 8424,
        "wait_time_ms": 8424.323433921283
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6220265b_developer_87160fde",
    "content": "Connection pool exhausted - 24 threads waiting, 990 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T06:07:00Z",
      "severity": "critical",
      "incident_id": "6220265b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 8424,
        "active_connections": 990.2737037665873,
        "active": 990
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "85dd8853_dba_bd500dc9",
    "content": "Performance alert: 'user_dashboard' execution time 400x slower at 98% CPU",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T07:17:00Z",
      "severity": "warning",
      "incident_id": "85dd8853",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "user_dashboard",
        "query_time_after_ms": 7056.08380188029,
        "query_time_before_ms": 165.5106724397578,
        "cpu_usage": 98
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "85dd8853_data_engineer_6b874155",
    "content": "Data pipeline: 'user_dashboard' degraded from 165ms to 7056ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T07:30:00Z",
      "severity": "warning",
      "incident_id": "85dd8853",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 98,
        "rows_scanned": 41438709,
        "query_time_before_ms": 165.5106724397578
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a3826541_dba_79ab44ed",
    "content": "Index scan taking 68x longer on 'idx_products_sku' - 10120498 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T08:34:00Z",
      "severity": "warning",
      "incident_id": "a3826541",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 10120498,
        "wait_count": 39,
        "affected_query": "report_aggregate",
        "query_time_before_ms": 107.37598349621773,
        "cpu_usage": 82
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a3826541_sre_6c6fe5dc",
    "content": "Service alert: report_aggregate queries 68x slower, CPU 82%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T08:37:00Z",
      "severity": "warning",
      "incident_id": "a3826541",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 32334.10937165685,
        "cpu_usage": 82,
        "query_time_before_ms": 107.37598349621773,
        "affected_query": "report_aggregate",
        "slowdown_factor": 68
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a3826541_data_engineer_0e724724",
    "content": "Report timeout: report_aggregate query exceeded 32334ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T08:40:00Z",
      "severity": "warning",
      "incident_id": "a3826541",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 68,
        "rows_scanned": 10120498,
        "query_time_after_ms": 32334.10937165685
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e0d8b868_dba_9843094b",
    "content": "FATAL: remaining connection slots are reserved for non-replication superuser connections [PG-53300]",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T10:47:00Z",
      "severity": "info",
      "incident_id": "e0d8b868",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "wait_ms": 20587,
        "wait_time_ms": 20587.837334350355,
        "active": 990,
        "wait_count": 21
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e0d8b868_sre_06f9a9b7",
    "content": "CRITICAL: Database connections at 98% capacity (990 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T10:51:00Z",
      "severity": "info",
      "incident_id": "e0d8b868",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 23.1,
        "active": 990,
        "active_connections": 990.9625064071482,
        "wait_ms": 20587
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e0d8b868_developer_2d8d7df4",
    "content": "ConnectionPoolExhaustedException: All 1000 connections in use (waited 20587ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T10:54:00Z",
      "severity": "info",
      "incident_id": "e0d8b868",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 98,
        "active": 990,
        "active_connections": 990.9625064071482,
        "error_code": "PG-53300",
        "wait_time_ms": 20587.837334350355
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e0d8b868_data_engineer_77122a2d",
    "content": "Airflow DAG stalled: connection pool exhausted during 23.1min window",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T10:57:00Z",
      "severity": "info",
      "incident_id": "e0d8b868",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 990,
        "spike_duration_min": 23.1,
        "wait_ms": 20587,
        "active_connections": 990.9625064071482,
        "error_code": "PG-53300"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "913a6a09_developer_c2f0568e",
    "content": "WARNING: 391 connections not returned to pool after 11h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T10:57:00Z",
      "severity": "warning",
      "incident_id": "913a6a09",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 391,
        "memory_usage_mb": 327,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "913a6a09_dba_6b0d71f7",
    "content": "WARNING: 391 idle connections detected (leak rate: 7.5/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T11:15:00Z",
      "severity": "warning",
      "incident_id": "913a6a09",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 69,
        "active_connections": 697.0273756819893,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a4e6c4b2_data_engineer_d3499881",
    "content": "Pipeline issue: 326 connections idle after job completion",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T11:51:00Z",
      "severity": "critical",
      "incident_id": "a4e6c4b2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 10.1,
        "threshold": 60,
        "active_connections": 604.9719754799889,
        "idle_connections": 326,
        "active": 604
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a4e6c4b2_developer_65689632",
    "content": "Application holding 326 unused connections for 4 hours",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T11:52:00Z",
      "severity": "warning",
      "incident_id": "a4e6c4b2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 230,
        "active": 604,
        "leak_duration_hours": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a4e6c4b2_dba_7c7091e8",
    "content": "pg_stat_activity shows 326 idle connections consuming 230MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T11:53:00Z",
      "severity": "critical",
      "incident_id": "a4e6c4b2",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 29,
        "leak_duration_hours": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6240fab2_sre_8c7c2bd7",
    "content": "CloudWatch: DatabaseConnections crossed 961 (89% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T13:54:00Z",
      "severity": "info",
      "incident_id": "6240fab2",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 43,
        "active": 961,
        "threshold": 89,
        "max_connections": 1000,
        "error_code": "PG-53300"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6240fab2_developer_1bfc75f4",
    "content": "Database unavailable after 23956ms wait - circuit breaker opened at 89%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T13:58:00Z",
      "severity": "info",
      "incident_id": "6240fab2",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 37.2,
        "active": 961,
        "wait_ms": 23956
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6240fab2_data_engineer_1bc3b767",
    "content": "Data pipeline blocked: connection pool at 89% capacity",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T13:59:00Z",
      "severity": "warning",
      "incident_id": "6240fab2",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "max_connections": 1000,
        "active_connections": 961.039300207159,
        "threshold": 89,
        "wait_time_ms": 23956.778232746212
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "752c1f65_data_engineer_d1475993",
    "content": "Pipeline slowdown - 81% IO used by maintenance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T15:31:00Z",
      "severity": "warning",
      "incident_id": "752c1f65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "vacuum_duration_min": 102,
        "io_usage_percent": 81,
        "dead_tuples": 77725519
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "752c1f65_dba_f6e22040",
    "content": "Table bloat: 'logs' at 44% with 77725519 dead rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T15:36:00Z",
      "severity": "info",
      "incident_id": "752c1f65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 42,
        "io_usage_percent": 81,
        "dead_tuples": 77725519,
        "table_bloat_percent": 44,
        "vacuum_duration_min": 102
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "752c1f65_developer_7f1db000",
    "content": "User queries slow - autovacuum processing 77725519 dead tuples",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T15:36:00Z",
      "severity": "info",
      "incident_id": "752c1f65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "wait_count": 42,
        "vacuum_duration_min": 102,
        "dead_tuples": 77725519
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17271a6e_data_engineer_7e168c91",
    "content": "Data load failing: 57 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T17:12:00Z",
      "severity": "warning",
      "incident_id": "17271a6e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 57,
        "affected_tables": "inventory",
        "lock_wait_ms": 7585,
        "wait_count": 25,
        "blocked_queries": 33
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17271a6e_dba_e4d3d236",
    "content": "Deadlock cascade on 'inventory' - 40/min for 10min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T17:16:00Z",
      "severity": "info",
      "incident_id": "17271a6e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 25,
        "deadlocks_per_minute": 40,
        "cascade_duration_min": 10,
        "affected_tables": "inventory"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17271a6e_sre_25625db9",
    "content": "Critical: 40 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T17:29:00Z",
      "severity": "info",
      "incident_id": "17271a6e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 7585,
        "cascade_duration_min": 10,
        "wait_count": 25,
        "affected_tables": "inventory"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17271a6e_developer_452474c5",
    "content": "Application errors: 57 transactions failed in 10min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T17:29:00Z",
      "severity": "info",
      "incident_id": "17271a6e",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 7585,
        "deadlocks_per_minute": 40,
        "blocked_queries": 33,
        "affected_tables": "inventory"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5508648d_dba_c9c9f4ff",
    "content": "WAL accumulation: 50GB of logs, disk 92% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-05T17:38:00Z",
      "severity": "critical",
      "incident_id": "5508648d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 50,
        "growth_rate_gb_per_day": 32,
        "disk_usage_percent": 92,
        "days_until_full": 6
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5508648d_developer_d88f41d3",
    "content": "Critical: Database disk 92% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T17:38:00Z",
      "severity": "critical",
      "incident_id": "5508648d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 11,
        "days_until_full": 6,
        "growth_rate_gb_per_day": 32,
        "wal_size_gb": 50,
        "disk_usage_percent": 92
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5508648d_sre_067a74fc",
    "content": "Storage capacity: 6 days until exhaustion",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T17:52:00Z",
      "severity": "critical",
      "incident_id": "5508648d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 50,
        "growth_rate_gb_per_day": 32,
        "free_space_gb": 3,
        "wait_count": 11
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e211e619_developer_35a78e5f",
    "content": "Database unavailable after 23065ms wait - circuit breaker opened at 94%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T19:59:00Z",
      "severity": "info",
      "incident_id": "e211e619",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 23065.840572340974,
        "active": 973,
        "active_connections": 973.1158445850213,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e211e619_data_engineer_aad79089",
    "content": "Data pipeline blocked: connection pool at 94% capacity",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-05T20:00:00Z",
      "severity": "info",
      "incident_id": "e211e619",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 46,
        "spike_duration_min": 59.1,
        "threshold": 94,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4756b840_developer_54d3ca23",
    "content": "Query performance degraded - table 'orders' 48% bloated",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-05T20:55:00Z",
      "severity": "warning",
      "incident_id": "4756b840",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 52,
        "table_bloat_percent": 48,
        "dead_tuples": 30638753
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "4756b840_sre_fbca8b1b",
    "content": "Disk usage alert: 'orders' table 48% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-05T21:03:00Z",
      "severity": "critical",
      "incident_id": "4756b840",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 48,
        "wait_count": 17,
        "vacuum_duration_min": 279,
        "tables_affected": "orders"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c14996a7_sre_8cb631c0",
    "content": "Latency alarm: 'inventory_check' breached 40009ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T03:10:00Z",
      "severity": "warning",
      "incident_id": "c14996a7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 40009.80381025769,
        "query_time_before_ms": 114.41784108935046,
        "slowdown_factor": 342
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "c14996a7_dba_1ce4430c",
    "content": "Sequential scan on 31504962 rows - index 'idx_sessions_user' not used, CPU at 98%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-06T03:10:00Z",
      "severity": "warning",
      "incident_id": "c14996a7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 45,
        "slowdown_factor": 342,
        "cpu_usage": 98,
        "query_time_before_ms": 114.41784108935046
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "c14996a7_data_engineer_210a33e3",
    "content": "ETL bottleneck: 'inventory_check' step taking 40009ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-06T03:13:00Z",
      "severity": "warning",
      "incident_id": "c14996a7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 31504962,
        "cpu_usage": 98,
        "slowdown_factor": 342,
        "query_time_after_ms": 40009.80381025769
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "c14996a7_developer_76b47d7b",
    "content": "Application query 'inventory_check' timeout after 40009ms at 98% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T03:26:00Z",
      "severity": "warning",
      "incident_id": "c14996a7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 114.41784108935046,
        "affected_query": "inventory_check",
        "cpu_usage": 98,
        "query_time_after_ms": 40009.80381025769,
        "wait_count": 45
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "363e2754_dba_aa454972",
    "content": "CRITICAL: Disk 91% full, only 48GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-06T07:49:00Z",
      "severity": "critical",
      "incident_id": "363e2754",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 32,
        "wal_size_gb": 50,
        "disk_usage_percent": 91,
        "growth_rate_gb_per_day": 99
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "363e2754_sre_896a7aa5",
    "content": "Critical infrastructure alert: WAL using 50GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T07:55:00Z",
      "severity": "critical",
      "incident_id": "363e2754",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 91,
        "days_until_full": 3,
        "wal_size_gb": 50,
        "wait_count": 32,
        "free_space_gb": 48
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "363e2754_data_engineer_da6852d3",
    "content": "Data pipeline may fail: only 48GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-06T07:57:00Z",
      "severity": "critical",
      "incident_id": "363e2754",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 99,
        "days_until_full": 3,
        "free_space_gb": 48,
        "wal_size_gb": 50,
        "wait_count": 32
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "fa0b1bbd_developer_c81b2779",
    "content": "Database maintenance causing 78% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T10:53:00Z",
      "severity": "info",
      "incident_id": "fa0b1bbd",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 21,
        "table_bloat_percent": 69,
        "vacuum_duration_min": 273
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "fa0b1bbd_sre_abf65b92",
    "content": "Monitoring: Autovacuum causing 78% IO saturation",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T10:57:00Z",
      "severity": "info",
      "incident_id": "fa0b1bbd",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 97992544,
        "tables_affected": "orders",
        "vacuum_duration_min": 273,
        "io_usage_percent": 78,
        "table_bloat_percent": 69
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "71301dbf_dba_c7aeecf8",
    "content": "Index scan taking 436x longer on 'idx_users_email' - 32816185 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-06T11:11:00Z",
      "severity": "warning",
      "incident_id": "71301dbf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 436,
        "query_time_after_ms": 48015.38316779914,
        "wait_count": 19,
        "rows_scanned": 32816185,
        "affected_query": "report_aggregate"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "71301dbf_sre_4d94d882",
    "content": "Service alert: report_aggregate queries 436x slower, CPU 84%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T11:11:00Z",
      "severity": "warning",
      "incident_id": "71301dbf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 84,
        "wait_count": 19,
        "query_time_after_ms": 48015.38316779914
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "71301dbf_developer_1c79307a",
    "content": "API timeout: 'report_aggregate' taking 48015ms (was 133ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T11:15:00Z",
      "severity": "warning",
      "incident_id": "71301dbf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 133.81863281664639,
        "cpu_usage": 84,
        "query_time_after_ms": 48015.38316779914
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "36942319_data_engineer_58cb8fdb",
    "content": "Batch job delayed: user_dashboard running 275x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-06T11:45:00Z",
      "severity": "warning",
      "incident_id": "36942319",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 72,
        "affected_query": "user_dashboard",
        "rows_scanned": 15619363
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "36942319_developer_d0d7ef25",
    "content": "Performance regression: 'user_dashboard' now 275x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T11:47:00Z",
      "severity": "info",
      "incident_id": "36942319",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 72,
        "slowdown_factor": 275,
        "query_time_after_ms": 42415.82769238718
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "36942319_sre_d250e995",
    "content": "CloudWatch: Query latency increased 275x for 'user_dashboard'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T11:54:00Z",
      "severity": "info",
      "incident_id": "36942319",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 275,
        "query_time_before_ms": 176.11462430654458,
        "wait_count": 25
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "c78dbc3d_developer_552b4a1e",
    "content": "Storage exhaustion in 5 days at current growth rate",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T13:56:00Z",
      "severity": "warning",
      "incident_id": "c78dbc3d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 46,
        "growth_rate_gb_per_day": 20,
        "wal_size_gb": 40,
        "free_space_gb": 29,
        "days_until_full": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c78dbc3d_data_engineer_7485ec7b",
    "content": "Batch processing consuming 20GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-06T13:56:00Z",
      "severity": "warning",
      "incident_id": "c78dbc3d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 46,
        "days_until_full": 5,
        "wal_size_gb": 40,
        "free_space_gb": 29,
        "disk_usage_percent": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c78dbc3d_dba_27c755b1",
    "content": "Storage alert: Growing 20GB/day, full in 5 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-06T13:57:00Z",
      "severity": "warning",
      "incident_id": "c78dbc3d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 46,
        "disk_usage_percent": 97,
        "days_until_full": 5,
        "growth_rate_gb_per_day": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c78dbc3d_sre_ca3f145b",
    "content": "Emergency response needed: 97% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T14:07:00Z",
      "severity": "warning",
      "incident_id": "c78dbc3d",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 29,
        "disk_usage_percent": 97,
        "wait_count": 46,
        "growth_rate_gb_per_day": 20
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_a92dc4a1b04f460a",
    "content": "Query cache hit ratio: 92% over last 13 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T15:37:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "029b98ed_developer_ee29ac00",
    "content": "Deadlock rate 46/min causing 74 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-06T17:16:00Z",
      "severity": "info",
      "incident_id": "029b98ed",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 74,
        "affected_tables": "inventory",
        "wait_count": 41,
        "cascade_duration_min": 3,
        "transaction_rollback": 46
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "029b98ed_data_engineer_667d7b33",
    "content": "Pipeline stalled: 74 queries waiting for 2197ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-06T17:18:00Z",
      "severity": "warning",
      "incident_id": "029b98ed",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 46,
        "transaction_rollback": 46,
        "cascade_duration_min": 3,
        "blocked_queries": 74,
        "wait_count": 41
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "029b98ed_sre_a99e10d0",
    "content": "Incident: Lock contention causing 74 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T17:27:00Z",
      "severity": "info",
      "incident_id": "029b98ed",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 3,
        "wait_count": 41,
        "blocked_queries": 74,
        "transaction_rollback": 46,
        "lock_wait_ms": 2197
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "029b98ed_dba_39dce3a7",
    "content": "CRITICAL: 74 queries waiting, deadlock rate: 46/min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-06T17:27:00Z",
      "severity": "info",
      "incident_id": "029b98ed",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 46,
        "wait_count": 41,
        "cascade_duration_min": 3,
        "lock_wait_ms": 2197,
        "blocked_queries": 74
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_190467e8d9334662",
    "content": "Replication healthy - all 5 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-06T20:45:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "56ec23ac_data_engineer_70049d58",
    "content": "Batch processing consuming 28GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-07T16:51:00Z",
      "severity": "critical",
      "incident_id": "56ec23ac",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 94,
        "growth_rate_gb_per_day": 28,
        "free_space_gb": 23
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_4eabcdd03f774e56",
    "content": "Connection pool stable at 479 connections (30% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-07T16:59:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "56ec23ac_developer_f09f6e47",
    "content": "Storage exhaustion in 1 days at current growth rate",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-07T17:01:00Z",
      "severity": "warning",
      "incident_id": "56ec23ac",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 1,
        "disk_usage_percent": 94,
        "growth_rate_gb_per_day": 28
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7d021663b72f4b5e",
    "content": "Network latency optimal - 1.9ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-08T17:12:10Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0fa09dad_dba_79726e8f",
    "content": "work_mem reduced to 17MB - 1 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-09T00:49:00Z",
      "severity": "warning",
      "incident_id": "0fa09dad",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 38,
        "swap_usage_gb": 7.9,
        "oom_kills": 1,
        "work_mem_mb": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0fa09dad_sre_aeadf54e",
    "content": "Memory alert: 92% utilized, 1 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-09T00:54:00Z",
      "severity": "warning",
      "incident_id": "0fa09dad",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 1,
        "wait_count": 38,
        "work_mem_mb": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_ddcfce6973784bf5",
    "content": "Backup successful: 23.3GB completed in 56 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-10T03:08:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_efe0e2692c7d47d3",
    "content": "Network latency optimal - 4.5ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-10T07:13:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_90b0e17db48f4612",
    "content": "CPU utilization stable at 58%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-10T19:30:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3fbca320_data_engineer_fcd6debf",
    "content": "ETL performance impacted by vacuum on 'events'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-12T11:14:00Z",
      "severity": "warning",
      "incident_id": "3fbca320",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "events",
        "wait_count": 18,
        "io_usage_percent": 53,
        "dead_tuples": 34970357
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "3fbca320_sre_76eba85c",
    "content": "Performance impact: 101min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-12T11:22:00Z",
      "severity": "info",
      "incident_id": "3fbca320",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 18,
        "vacuum_duration_min": 101,
        "io_usage_percent": 53,
        "table_bloat_percent": 39
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dd4388ee_developer_05c93992",
    "content": "User-facing slowdown: inventory_check response time degraded 79x",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-13T08:27:00Z",
      "severity": "warning",
      "incident_id": "dd4388ee",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 20778566,
        "wait_count": 5,
        "affected_query": "inventory_check"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dd4388ee_data_engineer_6d16ff1e",
    "content": "Dashboard failing: inventory_check query 79x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-13T08:31:00Z",
      "severity": "warning",
      "incident_id": "dd4388ee",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 26502.812295653563,
        "rows_scanned": 20778566,
        "affected_query": "inventory_check",
        "cpu_usage": 76,
        "wait_count": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dd4388ee_sre_b755d513",
    "content": "Performance degradation: 79x slowdown on 'inventory_check' queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-13T08:33:00Z",
      "severity": "warning",
      "incident_id": "dd4388ee",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 5,
        "query_time_before_ms": 98.8402013033983,
        "affected_query": "inventory_check",
        "query_time_after_ms": 26502.812295653563,
        "slowdown_factor": 79
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "dd4388ee_dba_b032d678",
    "content": "Query regression: 'inventory_check' from 98.8ms to 26502ms (79x slower)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-13T08:39:00Z",
      "severity": "warning",
      "incident_id": "dd4388ee",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 98.8402013033983,
        "rows_scanned": 20778566,
        "wait_count": 5,
        "cpu_usage": 76,
        "slowdown_factor": 79
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_b04316e60b874908",
    "content": "Buffer cache warmed - 3.3GB loaded into memory",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-14T07:22:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ebab5dd3_sre_5b63bc85",
    "content": "Service disruption: 62 failed transactions in 12min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-14T16:09:00Z",
      "severity": "critical",
      "incident_id": "ebab5dd3",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 12,
        "affected_tables": "users",
        "blocked_queries": 99,
        "lock_wait_ms": 9713,
        "wait_count": 18
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "ebab5dd3_data_engineer_2818447d",
    "content": "ETL blocked by deadlocks on 'users' (48/min)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-14T16:27:00Z",
      "severity": "critical",
      "incident_id": "ebab5dd3",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "users",
        "cascade_duration_min": 12,
        "lock_wait_ms": 9713,
        "deadlocks_per_minute": 48
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1c9df2dc16e94614",
    "content": "Network latency optimal - 4.5ms average RTT",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-14T19:12:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5fbd34a_data_engineer_e2eaf7aa",
    "content": "Report inconsistencies due to 235s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-14T20:27:00Z",
      "severity": "critical",
      "incident_id": "e5fbd34a",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_bytes": 68734134,
        "lag_seconds": 235,
        "wal_size_mb": 4212
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5fbd34a_dba_136d5972",
    "content": "Replication lag: 235s behind primary, 68734134 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-14T20:35:00Z",
      "severity": "critical",
      "incident_id": "e5fbd34a",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 4212,
        "network_latency_ms": 48,
        "lag_seconds": 235,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e5fbd34a_developer_fc2a6a88",
    "content": "Stale data on read replicas - 235s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-14T20:39:00Z",
      "severity": "critical",
      "incident_id": "e5fbd34a",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 4212,
        "lag_seconds": 235,
        "replica_count": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "2c49f62b_data_engineer_aaaf2e37",
    "content": "Batch processing consuming 67GB/day",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-15T11:46:00Z",
      "severity": "critical",
      "incident_id": "2c49f62b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 99,
        "disk_usage_percent": 96,
        "days_until_full": 3
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "2c49f62b_sre_fd6bb5ec",
    "content": "Projection: Storage full in 3 days (67GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-15T11:47:00Z",
      "severity": "critical",
      "incident_id": "2c49f62b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 67,
        "days_until_full": 3,
        "free_space_gb": 17,
        "wal_size_gb": 99
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "2c49f62b_dba_7246c186",
    "content": "Storage alert: Growing 67GB/day, full in 3 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-15T11:50:00Z",
      "severity": "critical",
      "incident_id": "2c49f62b",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 3,
        "wait_count": 20,
        "free_space_gb": 17
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_686fbbd3b1f8440c",
    "content": "SSL certificate valid for 336 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-15T13:35:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_832330dcfcfc46a8",
    "content": "Query performance within SLA - P99 latency 401ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-15T22:27:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_6c6f657a70fe4628",
    "content": "Disk I/O normal - 4942 read IOPS, 4680 write IOPS",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-16T06:41:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "22e97234_developer_6b849e5d",
    "content": "Database unavailable after 26255ms wait - circuit breaker opened at 95%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-16T11:08:00Z",
      "severity": "warning",
      "incident_id": "22e97234",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 20,
        "spike_duration_min": 30.9,
        "wait_ms": 26255
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "22e97234_data_engineer_ba8830d4",
    "content": "Data pipeline blocked: connection pool at 95% capacity",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-16T11:16:00Z",
      "severity": "critical",
      "incident_id": "22e97234",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 20,
        "active": 982
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "22e97234_dba_f2d61f41",
    "content": "Connection pool saturation: 982 active, 20 waiting, threshold 95% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-16T11:16:00Z",
      "severity": "critical",
      "incident_id": "22e97234",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active": 982,
        "wait_time_ms": 26255.663180870928,
        "active_connections": 982.6494761683487,
        "spike_duration_min": 30.9
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_13dba9879d01455b",
    "content": "WAL archiving on schedule - 47 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-16T17:07:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1f99a8956ad04999",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-16T20:46:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_1b7e124e32c44c8b",
    "content": "Query cache hit ratio: 93% over last 30 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-18T06:08:05Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3268cd86b27a4d52",
    "content": "Checkpoint completed: 48842 buffers written in 8.4 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-19T04:14:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_78bfed725e1b44de",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-19T12:47:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_3ec589091efa4081",
    "content": "Autovacuum completed on table 'products' - 548033 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-19T23:16:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3ced786b_data_engineer_85341b2d",
    "content": "ETL bottleneck: 'user_dashboard' step taking 35300ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-20T13:19:00Z",
      "severity": "warning",
      "incident_id": "3ced786b",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "user_dashboard",
        "query_time_after_ms": 35300.37048525792,
        "rows_scanned": 44689702
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3ced786b_developer_b6963822",
    "content": "Application query 'user_dashboard' timeout after 35300ms at 79% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-20T13:35:00Z",
      "severity": "warning",
      "incident_id": "3ced786b",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 35300.37048525792,
        "slowdown_factor": 482,
        "wait_count": 24,
        "rows_scanned": 44689702,
        "cpu_usage": 79
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_f052d171121b40d4",
    "content": "Connection pool stable at 433 connections (56% utilization)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-21T07:09:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "4fa56c1e_developer_d8e90c7f",
    "content": "Application slowdown during 342min vacuum on 'metrics'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-21T12:07:00Z",
      "severity": "info",
      "incident_id": "4fa56c1e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 61,
        "dead_tuples": 27499475,
        "tables_affected": "metrics",
        "vacuum_duration_min": 342,
        "table_bloat_percent": 54
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4fa56c1e_dba_68af2ea2",
    "content": "Long-running vacuum: 342min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-21T12:11:00Z",
      "severity": "info",
      "incident_id": "4fa56c1e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 27499475,
        "vacuum_duration_min": 342,
        "wait_count": 20
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1eb431e9de914a04",
    "content": "Network latency optimal - 1.5ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-22T05:56:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_41a5117c0d3f494b",
    "content": "Autovacuum completed on table 'events' - 607013 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-22T15:51:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_2464f619530b4812",
    "content": "Connection recycling: 13 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-22T18:18:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6310d094_dba_6e13830e",
    "content": "Deadlock storm: 92 blocked, 16/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-23T22:07:00Z",
      "severity": "critical",
      "incident_id": "6310d094",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 141,
        "cascade_duration_min": 3,
        "lock_wait_ms": 5251
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6310d094_developer_79ad372d",
    "content": "Database contention: 5251ms lock wait, 141 rollbacks",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-23T22:07:00Z",
      "severity": "warning",
      "incident_id": "6310d094",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 39,
        "transaction_rollback": 141,
        "lock_wait_ms": 5251,
        "blocked_queries": 92,
        "affected_tables": "shipments"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6310d094_sre_951cb2e7",
    "content": "Alert: Deadlock storm - 16/min affecting 92 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-23T22:14:00Z",
      "severity": "critical",
      "incident_id": "6310d094",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 39,
        "transaction_rollback": 141,
        "cascade_duration_min": 3,
        "deadlocks_per_minute": 16,
        "lock_wait_ms": 5251
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_e55221c7d8994d49",
    "content": "Autovacuum completed on table 'orders' - 670727 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-24T09:16:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_6405385b5f3b4613",
    "content": "Database health check passed - response time 180ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-24T10:21:40Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "53eb69f2_data_engineer_7e3c9c7e",
    "content": "Dashboard failing: user_dashboard query 264x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-24T10:48:00Z",
      "severity": "warning",
      "incident_id": "53eb69f2",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 27,
        "rows_scanned": 40561971,
        "cpu_usage": 86
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "53eb69f2_sre_7d5a242b",
    "content": "Performance degradation: 264x slowdown on 'user_dashboard' queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-24T10:57:00Z",
      "severity": "warning",
      "incident_id": "53eb69f2",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 27,
        "cpu_usage": 86,
        "affected_query": "user_dashboard",
        "query_time_after_ms": 21728.799146451405
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_10809ccaba764180",
    "content": "Query cache hit ratio: 99% over last 14 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-25T10:41:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_be4d762e20914bed",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-26T10:18:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_5a1d02f6156c4fed",
    "content": "Network latency optimal - 2.6ms average RTT",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-26T13:27:27Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cf7ee2b9_sre_e73c424c",
    "content": "CRITICAL: Database connections at 89% capacity (964 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-26T16:17:00Z",
      "severity": "warning",
      "incident_id": "cf7ee2b9",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 964,
        "wait_ms": 14359,
        "threshold": 89,
        "error_code": "PG-53300",
        "spike_duration_min": 45.5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cf7ee2b9_data_engineer_8639bd6e",
    "content": "Airflow DAG stalled: connection pool exhausted during 45.5min window",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-26T16:21:00Z",
      "severity": "warning",
      "incident_id": "cf7ee2b9",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_ms": 14359,
        "threshold": 89,
        "active": 964
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cf7ee2b9_dba_586610cb",
    "content": "ERROR: too many connections for role 'app_user' (current: 964, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-26T16:26:00Z",
      "severity": "warning",
      "incident_id": "cf7ee2b9",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 964.6245170249442,
        "threshold": 89,
        "max_connections": 1000,
        "error_code": "PG-53300",
        "wait_time_ms": 14359.391713260115
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b1ff9b39_developer_40da3e50",
    "content": "Memory leak in connection handling - 16 connections/hour, 365MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-26T22:20:00Z",
      "severity": "warning",
      "incident_id": "b1ff9b39",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 7,
        "wait_count": 41,
        "active_connections": 617.6960233043548,
        "idle_connections": 278
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b1ff9b39_data_engineer_c85c1621",
    "content": "Batch job leak: 15.9 connections/hour over 7h runtime",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-26T22:27:00Z",
      "severity": "warning",
      "incident_id": "b1ff9b39",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 617,
        "wait_count": 41,
        "leak_rate_per_hour": 15.9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b1ff9b39_sre_3ff362c9",
    "content": "Resource alert: 278 idle connections using 365MB RAM",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-26T22:35:00Z",
      "severity": "warning",
      "incident_id": "b1ff9b39",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 7,
        "memory_usage_mb": 365,
        "idle_connections": 278
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b1ff9b39_dba_225f796a",
    "content": "Connection leak: idle_in_transaction growing at 16/hour for 7h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2024-12-26T22:36:00Z",
      "severity": "warning",
      "incident_id": "b1ff9b39",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "idle_connections": 278,
        "active_connections": 617.6960233043548,
        "leak_rate_per_hour": 15.9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c0aa2a2ac61f4505",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-27T08:17:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_b6f65355478746b1",
    "content": "Connection recycling: 47 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-30T04:02:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ca2cd837_sre_77912027",
    "content": "System alert: 5981ms lock wait impacting 115 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-30T14:57:00Z",
      "severity": "info",
      "incident_id": "ca2cd837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 22,
        "cascade_duration_min": 10,
        "affected_tables": "orders",
        "deadlocks_per_minute": 15,
        "transaction_rollback": 115
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "ca2cd837_developer_de4cecbd",
    "content": "Transaction retry exhausted - 15 deadlocks/min on 'orders'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-30T15:08:00Z",
      "severity": "info",
      "incident_id": "ca2cd837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 5981,
        "wait_count": 19,
        "blocked_queries": 22
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8d35028c_data_engineer_eae43bee",
    "content": "ETL job killed: OOM at 95% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2024-12-30T15:38:00Z",
      "severity": "critical",
      "incident_id": "8d35028c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 4.9,
        "memory_usage_percent": 95,
        "wait_count": 16,
        "oom_kills": 3,
        "buffer_cache_hit": 57
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8d35028c_developer_a4979096",
    "content": "Database OOM: 3 connections terminated at 95% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-30T15:50:00Z",
      "severity": "warning",
      "incident_id": "8d35028c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 95,
        "swap_usage_gb": 4.9,
        "work_mem_mb": 63
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8d35028c_sre_5f50040f",
    "content": "System swapping 4.9GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-30T15:53:00Z",
      "severity": "critical",
      "incident_id": "8d35028c",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 95,
        "oom_kills": 3,
        "wait_count": 16
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d24c1c0fac8b4e67",
    "content": "CPU utilization stable at 59%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-31T01:44:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_6feb6fc6a3e64cec",
    "content": "SSL certificate valid for 307 more days",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-31T08:40:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_684e73d2bbdf4f61",
    "content": "Buffer cache warmed - 17.4GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2024-12-31T13:31:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_14e8b0777e254ad5",
    "content": "Query performance within SLA - P99 latency 193ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2024-12-31T14:07:17Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ac6472916d49441d",
    "content": "Query cache hit ratio: 96% over last 14 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-01T07:30:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "5fa7f4d3_developer_ec59e88f",
    "content": "Database storage 92% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-01T13:23:00Z",
      "severity": "warning",
      "incident_id": "5fa7f4d3",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 73,
        "days_until_full": 6,
        "disk_usage_percent": 92,
        "wait_count": 38,
        "free_space_gb": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5fa7f4d3_sre_7667544d",
    "content": "Projection: Storage full in 6 days (97GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-01T13:37:00Z",
      "severity": "warning",
      "incident_id": "5fa7f4d3",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 73,
        "growth_rate_gb_per_day": 97,
        "wait_count": 38,
        "free_space_gb": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "83b762a4_data_engineer_67befd69",
    "content": "Ingestion blocked: 17/min deadlock rate on 'inventory'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-02T18:26:00Z",
      "severity": "warning",
      "incident_id": "83b762a4",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 23,
        "deadlocks_per_minute": 17,
        "affected_tables": "inventory"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "83b762a4_sre_0d2ec9cb",
    "content": "Alert: Deadlock storm - 17/min affecting 23 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-02T18:30:00Z",
      "severity": "warning",
      "incident_id": "83b762a4",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 23,
        "deadlocks_per_minute": 17,
        "lock_wait_ms": 3321,
        "wait_count": 33
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "83b762a4_dba_1b1ef7e0",
    "content": "Deadlock storm: 23 blocked, 17/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-02T18:35:00Z",
      "severity": "warning",
      "incident_id": "83b762a4",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 23,
        "cascade_duration_min": 8,
        "wait_count": 33,
        "lock_wait_ms": 3321,
        "deadlocks_per_minute": 17
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "870927bc_data_engineer_9d54c2f8",
    "content": "Pipeline data quality affected by 140s replica delay",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-03T19:45:00Z",
      "severity": "warning",
      "incident_id": "870927bc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 140,
        "network_latency_ms": 14,
        "replica_count": 4,
        "wait_count": 24
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "870927bc_dba_f6f91319",
    "content": "Standby lag alert: 82044450 bytes behind, 3713MB WAL accumulated",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-03T19:46:00Z",
      "severity": "info",
      "incident_id": "870927bc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_bytes": 82044450,
        "lag_seconds": 140
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_08098338dc5a4ae4",
    "content": "WAL archiving on schedule - 25 segments archived",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-04T14:08:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "85a6b42b_sre_b510ba32",
    "content": "Alert: Connection saturation 980/1000 for 56 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-05T12:40:00Z",
      "severity": "info",
      "incident_id": "85a6b42b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 26,
        "max_connections": 1000,
        "threshold": 89
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "85a6b42b_dba_c8fd5063",
    "content": "Database refusing connections: SQLSTATE 53300 after 26809ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-05T12:40:00Z",
      "severity": "info",
      "incident_id": "85a6b42b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "threshold": 89,
        "active": 980,
        "wait_ms": 26809,
        "wait_time_ms": 26809.789461934353
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "85a6b42b_data_engineer_543dfedc",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 26809ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-05T12:45:00Z",
      "severity": "warning",
      "incident_id": "85a6b42b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 26809.789461934353,
        "wait_ms": 26809,
        "threshold": 89,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_3da15f0b26ec4c1c",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-05T19:37:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "43ab86e3_data_engineer_ec713446",
    "content": "Data load failing: 166 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-06T01:38:00Z",
      "severity": "warning",
      "incident_id": "43ab86e3",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 1918,
        "affected_tables": "orders",
        "blocked_queries": 72
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "43ab86e3_sre_ebb56356",
    "content": "Critical: 17 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-06T01:39:00Z",
      "severity": "warning",
      "incident_id": "43ab86e3",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "orders",
        "deadlocks_per_minute": 17,
        "blocked_queries": 72
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0396b07a_developer_ed625bdd",
    "content": "Connection pool: 287 idle, 637 active - possible 211MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-07T06:42:00Z",
      "severity": "warning",
      "incident_id": "0396b07a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 211,
        "idle_connections": 287,
        "leak_rate_per_hour": 5.7,
        "threshold": 63
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0396b07a_sre_ce1cd62a",
    "content": "Linear growth: 287 idle connections, 211MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-07T06:54:00Z",
      "severity": "warning",
      "incident_id": "0396b07a",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 287,
        "wait_count": 32,
        "threshold": 63
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3e4f6d1219404b66",
    "content": "Autovacuum completed on table 'sessions' - 426866 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-07T15:12:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e33cb31e_data_engineer_29238ac0",
    "content": "Data load competing with 168min vacuum operation",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-07T20:30:00Z",
      "severity": "warning",
      "incident_id": "e33cb31e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 56,
        "table_bloat_percent": 49,
        "vacuum_duration_min": 168
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e33cb31e_developer_8f8147f1",
    "content": "Query performance degraded - table 'orders' 49% bloated",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-07T20:40:00Z",
      "severity": "warning",
      "incident_id": "e33cb31e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 33,
        "tables_affected": "orders",
        "io_usage_percent": 56,
        "dead_tuples": 34001467
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e33cb31e_dba_2c015e2d",
    "content": "Aggressive vacuum on 'orders' - 34001467 dead tuples, 49% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-07T20:48:00Z",
      "severity": "warning",
      "incident_id": "e33cb31e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "orders",
        "table_bloat_percent": 49,
        "wait_count": 33
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_8a8e90e35a9a4fe5",
    "content": "Buffer cache warmed - 9.5GB loaded into memory",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-07T21:34:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "cf8cbcc1_dba_47003cac",
    "content": "Connection accumulation: 10.2 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-08T02:36:00Z",
      "severity": "critical",
      "incident_id": "cf8cbcc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "leak_rate_per_hour": 10.2,
        "memory_usage_mb": 395
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "cf8cbcc1_sre_ce91f7fc",
    "content": "Linear growth: 128 idle connections, 395MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-08T02:37:00Z",
      "severity": "critical",
      "incident_id": "cf8cbcc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 61,
        "active": 610,
        "active_connections": 610.4546032385921
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "cf8cbcc1_developer_caf0dabd",
    "content": "Connection pool: 128 idle, 610 active - possible 395MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-08T02:50:00Z",
      "severity": "critical",
      "incident_id": "cf8cbcc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 610.4546032385921,
        "leak_rate_per_hour": 10.2,
        "wait_count": 13
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "cf8cbcc1_data_engineer_447b22d7",
    "content": "ETL connection pool: 128 idle connections leaking at 10/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-08T02:56:00Z",
      "severity": "critical",
      "incident_id": "cf8cbcc1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 128,
        "wait_count": 13,
        "threshold": 61,
        "leak_rate_per_hour": 10.2,
        "active_connections": 610.4546032385921
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_b20faaf142884e77",
    "content": "Network latency optimal - 2.0ms average RTT",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-08T03:04:17Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_a12d5b968fce4fa3",
    "content": "Network latency optimal - 3.3ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-08T11:59:47Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_2c93c36d0f2c42aa",
    "content": "Network latency optimal - 2.5ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-10T08:43:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a8168dcd_sre_a2274a48",
    "content": "Anomaly: Idle connections increasing 17/hour for past 22 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-10T21:09:00Z",
      "severity": "critical",
      "incident_id": "a8168dcd",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 138,
        "max_connections": 1000,
        "active_connections": 541.3190339776152,
        "active": 541,
        "leak_rate_per_hour": 17.1
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a8168dcd_data_engineer_92593c68",
    "content": "Spark executors holding 127 idle connections, 138MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-10T21:12:00Z",
      "severity": "critical",
      "incident_id": "a8168dcd",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "memory_usage_mb": 138,
        "leak_duration_hours": 22
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a8168dcd_dba_91a83804",
    "content": "WARNING: 127 idle connections detected (leak rate: 17.1/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-10T21:13:00Z",
      "severity": "critical",
      "incident_id": "a8168dcd",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "active": 541,
        "idle_connections": 127
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_b08259a6a51d4ce4",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-11T06:32:40Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "c1501831_sre_9123f4d1",
    "content": "Disk space alarm: 97% used, 32GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-11T13:31:00Z",
      "severity": "warning",
      "incident_id": "c1501831",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 1,
        "disk_usage_percent": 97,
        "free_space_gb": 32,
        "wait_count": 45,
        "growth_rate_gb_per_day": 70
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c1501831_developer_dfd90c73",
    "content": "Write failures imminent - 32GB free space left",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-11T13:31:00Z",
      "severity": "warning",
      "incident_id": "c1501831",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 97,
        "free_space_gb": 32,
        "wait_count": 45
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c1501831_data_engineer_309c4f60",
    "content": "Data retention issue: 50GB logs, 97% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-11T13:31:00Z",
      "severity": "warning",
      "incident_id": "c1501831",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 32,
        "wait_count": 45,
        "wal_size_gb": 50,
        "growth_rate_gb_per_day": 70,
        "days_until_full": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ea529e974a28494e",
    "content": "Index 'idx_users_date' rebuild completed - 337MB size reduction",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-11T14:56:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d0ea6056375b407e",
    "content": "Statistics updated for 45 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-12T15:12:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_55e264692f9f4c9c",
    "content": "Database health check passed - response time 32ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-12T21:06:47Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_29e1e0462cba42eb",
    "content": "Query cache hit ratio: 92% over last 54 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-12T23:06:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_e6c2deec2b21428c",
    "content": "Statistics updated for 40 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-13T14:43:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_8fcfbe85915b46ae",
    "content": "Connection pool stable at 50 connections (17% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-13T22:51:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_6d08da84073a4e3f",
    "content": "Autovacuum completed on table 'products' - 787055 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-14T10:25:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "be916bd4_data_engineer_9b6a31d5",
    "content": "Analytics queries on stale data - replica 101s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-14T17:05:00Z",
      "severity": "warning",
      "incident_id": "be916bd4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 101,
        "lag_bytes": 70840671,
        "replica_count": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "be916bd4_developer_a307612b",
    "content": "Read-after-write inconsistency - replica 101s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-14T17:15:00Z",
      "severity": "info",
      "incident_id": "be916bd4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "lag_seconds": 101,
        "wait_count": 49
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "be916bd4_dba_1e09af48",
    "content": "Read replica out of sync by 101 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-14T17:17:00Z",
      "severity": "info",
      "incident_id": "be916bd4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 70840671,
        "replica_count": 4,
        "wal_size_mb": 538,
        "lag_seconds": 101,
        "wait_count": 49
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "be916bd4_sre_18165751",
    "content": "Alert: WAL accumulation 538MB, lag 101s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-14T17:21:00Z",
      "severity": "info",
      "incident_id": "be916bd4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 49,
        "network_latency_ms": 33,
        "wal_size_mb": 538
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8384110d_sre_bb7b4bf0",
    "content": "Critical: Buffer cache efficiency 31% (memory pressure)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-14T23:45:00Z",
      "severity": "warning",
      "incident_id": "8384110d",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 50,
        "buffer_cache_hit": 31,
        "oom_kills": 3,
        "memory_usage_percent": 90
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8384110d_dba_0842ed36",
    "content": "FATAL: out of memory - 3 processes killed, 90% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-14T23:54:00Z",
      "severity": "warning",
      "incident_id": "8384110d",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 3,
        "memory_usage_percent": 90,
        "buffer_cache_hit": 31,
        "swap_usage_gb": 7.0,
        "work_mem_mb": 58
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_4c5aa9ca9cba45a6",
    "content": "Query cache hit ratio: 91% over last 15 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-15T10:06:29Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_21374a17648746ca",
    "content": "Buffer cache warmed - 28.7GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-15T15:53:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_911dad36a5a2427b",
    "content": "Autovacuum completed on table 'products' - 974556 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-16T15:05:26Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_e722a7af36964b6d",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-17T05:17:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_01e7414a9a7e4ef7",
    "content": "Backup successful: 47.9GB completed in 7 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-17T22:32:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0b4fb881b7be480a",
    "content": "Query cache hit ratio: 99% over last 19 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-18T13:27:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_286b09d32ca744bc",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-18T17:17:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_f531977cef604244",
    "content": "Autovacuum completed on table 'events' - 523702 rows processed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-19T12:45:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "01a0c8f2_developer_c7dbd379",
    "content": "Critical: Database disk 86% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-20T08:54:00Z",
      "severity": "critical",
      "incident_id": "01a0c8f2",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 78,
        "days_until_full": 1,
        "free_space_gb": 25
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "01a0c8f2_dba_57fe8361",
    "content": "WAL accumulation: 78GB of logs, disk 86% utilized",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-20T09:11:00Z",
      "severity": "critical",
      "incident_id": "01a0c8f2",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 25,
        "disk_usage_percent": 86,
        "growth_rate_gb_per_day": 78
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_53e2b8d904a2412e",
    "content": "Buffer cache warmed - 22.8GB loaded into memory",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-20T23:08:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "76dbf663_sre_9ecbd1e5",
    "content": "Resource alert: 302 idle connections using 337MB RAM",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-23T15:46:00Z",
      "severity": "warning",
      "incident_id": "76dbf663",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 302,
        "max_connections": 1000,
        "active_connections": 664.5870702016346,
        "threshold": 66,
        "leak_duration_hours": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "76dbf663_developer_64f35dd6",
    "content": "Memory leak in connection handling - 10 connections/hour, 337MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-23T15:51:00Z",
      "severity": "warning",
      "incident_id": "76dbf663",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 10,
        "memory_usage_mb": 337,
        "threshold": 66,
        "max_connections": 1000,
        "active": 664
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "76dbf663_data_engineer_5513579e",
    "content": "Batch job leak: 10.4 connections/hour over 10h runtime",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-23T15:57:00Z",
      "severity": "warning",
      "incident_id": "76dbf663",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 6,
        "memory_usage_mb": 337,
        "active": 664
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ee7ee51c2c314837",
    "content": "Index 'idx_users_id' rebuild completed - 249MB size reduction",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-23T22:44:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_05e5262a36d44a90",
    "content": "Query cache hit ratio: 87% over last 50 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-24T15:29:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c50f63fb_data_engineer_1514b929",
    "content": "ETL connection pool: 205 idle connections leaking at 19/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-24T20:45:00Z",
      "severity": "warning",
      "incident_id": "c50f63fb",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 19.0,
        "wait_count": 41,
        "active": 579,
        "threshold": 57
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c50f63fb_developer_1f90ee00",
    "content": "Connection pool: 205 idle, 579 active - possible 262MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-24T20:52:00Z",
      "severity": "warning",
      "incident_id": "c50f63fb",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 41,
        "active": 579,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c50f63fb_dba_d24c7519",
    "content": "Connection accumulation: 19.0 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-24T20:55:00Z",
      "severity": "warning",
      "incident_id": "c50f63fb",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 57,
        "active_connections": 579.9773058103581,
        "max_connections": 1000,
        "active": 579,
        "leak_duration_hours": 2
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_81bc5cd168d84af4",
    "content": "WAL archiving on schedule - 38 segments archived",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-25T02:23:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_d19f2b1962224ca5",
    "content": "Statistics updated for 47 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-25T15:58:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_aa7ac4b486674a48",
    "content": "Query performance within SLA - P99 latency 206ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-25T17:59:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_0def46d25b884776",
    "content": "WAL archiving on schedule - 66 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-26T03:31:22Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_9ada1678ae054785",
    "content": "Autovacuum completed on table 'sessions' - 633906 rows processed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-26T05:31:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4a7105ef_dba_ff191b3e",
    "content": "CRITICAL: Disk 88% full, only 28GB remaining",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-26T14:42:00Z",
      "severity": "critical",
      "incident_id": "4a7105ef",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "free_space_gb": 28,
        "disk_usage_percent": 88
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4a7105ef_developer_b9fc57aa",
    "content": "Application at risk: only 28GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-26T14:49:00Z",
      "severity": "warning",
      "incident_id": "4a7105ef",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wal_size_gb": 11,
        "growth_rate_gb_per_day": 39,
        "free_space_gb": 28,
        "days_until_full": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4a7105ef_sre_17cb7a4f",
    "content": "Critical infrastructure alert: WAL using 11GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-26T14:59:00Z",
      "severity": "critical",
      "incident_id": "4a7105ef",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 36,
        "wal_size_gb": 11,
        "disk_usage_percent": 88
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_1ca06eb1b2b24bdf",
    "content": "Query cache hit ratio: 85% over last 38 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-26T18:34:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_a435672786c74d6e",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-27T08:14:05Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "57ee7722_sre_c0be4abf",
    "content": "Linear growth: 119 idle connections, 185MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-27T18:36:00Z",
      "severity": "info",
      "incident_id": "57ee7722",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 119,
        "active": 524,
        "leak_duration_hours": 5,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "57ee7722_dba_06ec11c3",
    "content": "Connection accumulation: 6.9 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-27T18:37:00Z",
      "severity": "info",
      "incident_id": "57ee7722",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 52,
        "idle_connections": 119,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "57ee7722_data_engineer_fbfa86bf",
    "content": "ETL connection pool: 119 idle connections leaking at 7/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-27T18:38:00Z",
      "severity": "warning",
      "incident_id": "57ee7722",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 6.9,
        "max_connections": 1000,
        "leak_duration_hours": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c52a85ff_sre_b8e302c9",
    "content": "IO spike: 76% during vacuum of 'orders'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-28T02:27:00Z",
      "severity": "info",
      "incident_id": "c52a85ff",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 69014197,
        "tables_affected": "orders",
        "io_usage_percent": 76,
        "vacuum_duration_min": 210,
        "table_bloat_percent": 65
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "c52a85ff_developer_3f209128",
    "content": "Service degradation during vacuum of 'orders' table",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-28T02:40:00Z",
      "severity": "info",
      "incident_id": "c52a85ff",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "orders",
        "wait_count": 15,
        "table_bloat_percent": 65
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "c52a85ff_data_engineer_7897f91a",
    "content": "Batch job delayed by autovacuum processing 69014197 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-28T02:40:00Z",
      "severity": "warning",
      "incident_id": "c52a85ff",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 76,
        "tables_affected": "orders",
        "wait_count": 15,
        "dead_tuples": 69014197
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_de375f96e89547d4",
    "content": "Statistics updated for 32 tables",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-28T03:21:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_b22d7adbf71d4d03",
    "content": "Statistics updated for 44 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-29T05:11:36Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_5d59c3aa14694919",
    "content": "Index 'idx_users_date' rebuild completed - 429MB size reduction",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-01-29T13:40:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d8b2a7321aba4218",
    "content": "Autovacuum completed on table 'orders' - 419975 rows processed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-30T00:52:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_c2be2e51c97247da",
    "content": "Index 'idx_products_status' rebuild completed - 128MB size reduction",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-30T02:26:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "efba037d_sre_163c3017",
    "content": "AWS RDS Alert: Connection count 958, wait time 22246ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-01-30T20:49:00Z",
      "severity": "info",
      "incident_id": "efba037d",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 22246,
        "spike_duration_min": 41.1,
        "active": 958,
        "wait_time_ms": 22246.226412508113
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "efba037d_developer_593f77d2",
    "content": "Connection pool exhausted - 18 threads waiting, 958 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-01-30T21:07:00Z",
      "severity": "info",
      "incident_id": "efba037d",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 18,
        "wait_ms": 22246,
        "active": 958
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "efba037d_data_engineer_6122c70c",
    "content": "Spark job waiting 22246ms for database connection - 18 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-30T21:08:00Z",
      "severity": "warning",
      "incident_id": "efba037d",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 94,
        "max_connections": 1000,
        "wait_ms": 22246
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_f7be110265aa43ba",
    "content": "Checkpoint completed: 49251 buffers written in 3.0 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-01-31T08:35:33Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_cc8aac3047e44114",
    "content": "Buffer cache warmed - 7.6GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-01T06:05:36Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_338c43413e654ad5",
    "content": "Connection pool stable at 50 connections (41% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-01T11:39:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_37b539f4995a4c1a",
    "content": "Connection recycling: 41 connections refreshed",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-01T20:28:20Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fd0f8528_sre_4c6b5b20",
    "content": "Network issue causing 35ms latency, 106s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-01T20:35:00Z",
      "severity": "info",
      "incident_id": "fd0f8528",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 81157818,
        "lag_seconds": 106,
        "wait_count": 45,
        "wal_size_mb": 120
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fd0f8528_developer_6646aa91",
    "content": "Stale data on read replicas - 106s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-01T20:38:00Z",
      "severity": "info",
      "incident_id": "fd0f8528",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 4,
        "wait_count": 45,
        "lag_bytes": 81157818
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fd0f8528_dba_c72661ee",
    "content": "Replication lag: 106s behind primary, 81157818 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-01T20:53:00Z",
      "severity": "info",
      "incident_id": "fd0f8528",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 35,
        "replica_count": 4,
        "lag_bytes": 81157818
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "66ddbb30_sre_d5c52e9a",
    "content": "Disk space alarm: 94% used, 18GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-02T00:11:00Z",
      "severity": "critical",
      "incident_id": "66ddbb30",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 18,
        "growth_rate_gb_per_day": 71,
        "days_until_full": 1,
        "wal_size_gb": 75
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "66ddbb30_developer_09c6d7a9",
    "content": "Critical: Database disk 94% full, operations impacted",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-02T00:19:00Z",
      "severity": "warning",
      "incident_id": "66ddbb30",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 1,
        "wait_count": 44,
        "growth_rate_gb_per_day": 71,
        "free_space_gb": 18,
        "wal_size_gb": 75
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "66ddbb30_data_engineer_e055fe56",
    "content": "ETL jobs at risk - disk 94% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-02T00:20:00Z",
      "severity": "critical",
      "incident_id": "66ddbb30",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 71,
        "disk_usage_percent": 94,
        "free_space_gb": 18,
        "wait_count": 44
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "a41a2e14_dba_9a0d865a",
    "content": "Critical memory pressure: cache hit 69%, swap 10.9GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-02T07:13:00Z",
      "severity": "critical",
      "incident_id": "a41a2e14",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 69,
        "memory_usage_percent": 93,
        "oom_kills": 5,
        "wait_count": 45,
        "work_mem_mb": 48
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a41a2e14_data_engineer_16427243",
    "content": "ETL job killed: OOM at 93% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-02T07:24:00Z",
      "severity": "critical",
      "incident_id": "a41a2e14",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 69,
        "oom_kills": 5,
        "work_mem_mb": 48
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a41a2e14_developer_d8e393d7",
    "content": "Database OOM: 5 connections terminated at 93% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-02T07:25:00Z",
      "severity": "warning",
      "incident_id": "a41a2e14",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 69,
        "wait_count": 45,
        "memory_usage_percent": 93
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a1b9d1c5_developer_b39a35ab",
    "content": "Read-after-write inconsistency - replica 135s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-03T06:32:00Z",
      "severity": "warning",
      "incident_id": "a1b9d1c5",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 15,
        "wait_count": 49,
        "wal_size_mb": 3235,
        "lag_bytes": 4319608
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a1b9d1c5_dba_0d6715d6",
    "content": "Read replica out of sync by 135 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-03T06:33:00Z",
      "severity": "warning",
      "incident_id": "a1b9d1c5",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 135,
        "network_latency_ms": 15,
        "wait_count": 49
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a1b9d1c5_data_engineer_677d660b",
    "content": "Analytics queries on stale data - replica 135s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-03T06:46:00Z",
      "severity": "warning",
      "incident_id": "a1b9d1c5",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "network_latency_ms": 15,
        "wait_count": 49,
        "lag_seconds": 135,
        "wal_size_mb": 3235
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_48fecc71809341a1",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-03T18:08:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0814ee8096804a10",
    "content": "Buffer cache warmed - 20.3GB loaded into memory",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-03T20:45:21Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6656ab60_sre_6701e8ec",
    "content": "CloudWatch: Query latency increased 166x for 'inventory_check'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-05T10:30:00Z",
      "severity": "critical",
      "incident_id": "6656ab60",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 166,
        "wait_count": 39,
        "cpu_usage": 83,
        "query_time_after_ms": 28399.545951064396,
        "query_time_before_ms": 184.21620779364238
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6656ab60_developer_0acdb9a1",
    "content": "Performance regression: 'inventory_check' now 166x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-05T10:30:00Z",
      "severity": "warning",
      "incident_id": "6656ab60",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "inventory_check",
        "slowdown_factor": 166,
        "query_time_before_ms": 184.21620779364238,
        "rows_scanned": 16312382,
        "query_time_after_ms": 28399.545951064396
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6656ab60_data_engineer_e34c8124",
    "content": "Batch job delayed: inventory_check running 166x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-05T10:40:00Z",
      "severity": "critical",
      "incident_id": "6656ab60",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 184.21620779364238,
        "rows_scanned": 16312382,
        "affected_query": "inventory_check",
        "slowdown_factor": 166
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6656ab60_dba_4b82ce52",
    "content": "Query plan changed: 'inventory_check' now takes 28399ms scanning 16312382 rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-05T10:43:00Z",
      "severity": "critical",
      "incident_id": "6656ab60",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 166,
        "wait_count": 39,
        "query_time_before_ms": 184.21620779364238,
        "affected_query": "inventory_check",
        "cpu_usage": 83
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "19d8222b_dba_bce365bf",
    "content": "Long-running vacuum: 215min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-05T21:04:00Z",
      "severity": "info",
      "incident_id": "19d8222b",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "sessions",
        "vacuum_duration_min": 215,
        "dead_tuples": 55365824,
        "io_usage_percent": 52,
        "wait_count": 35
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "19d8222b_data_engineer_0c37d222",
    "content": "Batch job delayed by autovacuum processing 55365824 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-05T21:04:00Z",
      "severity": "warning",
      "incident_id": "19d8222b",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 215,
        "tables_affected": "sessions",
        "table_bloat_percent": 79,
        "wait_count": 35,
        "io_usage_percent": 52
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_22bd852b39f94301",
    "content": "SSL certificate valid for 228 more days",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-06T06:33:57Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "eb5601e7_developer_b54baac8",
    "content": "Out of memory errors after reaching 91% usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-06T08:06:00Z",
      "severity": "critical",
      "incident_id": "eb5601e7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "buffer_cache_hit": 65,
        "work_mem_mb": 8,
        "wait_count": 34,
        "swap_usage_gb": 9.1
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "eb5601e7_dba_cc91116b",
    "content": "work_mem reduced to 8MB - 8 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-06T08:06:00Z",
      "severity": "critical",
      "incident_id": "eb5601e7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 8,
        "buffer_cache_hit": 65,
        "memory_usage_percent": 91,
        "swap_usage_gb": 9.1
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "eb5601e7_sre_48d84f20",
    "content": "Memory alert: 91% utilized, 8 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-06T08:09:00Z",
      "severity": "critical",
      "incident_id": "eb5601e7",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 8,
        "buffer_cache_hit": 65,
        "wait_count": 34
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f3f5362553b04aa4",
    "content": "CPU utilization stable at 30%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-06T21:18:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_56735506738f4699",
    "content": "SSL certificate valid for 315 more days",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-07T19:16:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_20f415435fc34f8c",
    "content": "SSL certificate valid for 266 more days",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-08T10:24:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_eefffdaf615e473f",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-09T04:39:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "78b8f7ad_dba_936c8c67",
    "content": "Deadlock storm: 53 blocked, 5/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-09T18:57:00Z",
      "severity": "critical",
      "incident_id": "78b8f7ad",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "users",
        "wait_count": 39,
        "deadlocks_per_minute": 5,
        "lock_wait_ms": 9854
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "78b8f7ad_data_engineer_3dc48af3",
    "content": "Ingestion blocked: 5/min deadlock rate on 'users'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-09T19:03:00Z",
      "severity": "critical",
      "incident_id": "78b8f7ad",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 9854,
        "cascade_duration_min": 8,
        "transaction_rollback": 69
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "78b8f7ad_sre_76f60742",
    "content": "Alert: Deadlock storm - 5/min affecting 53 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-09T19:08:00Z",
      "severity": "critical",
      "incident_id": "78b8f7ad",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 69,
        "deadlocks_per_minute": 5,
        "blocked_queries": 53
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "17598b13_dba_d97f65df",
    "content": "Index scan taking 58x longer on 'idx_products_sku' - 33718938 rows examined",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-10T17:47:00Z",
      "severity": "warning",
      "incident_id": "17598b13",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 41,
        "affected_query": "user_dashboard",
        "rows_scanned": 33718938,
        "query_time_after_ms": 59642.26754006477
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17598b13_data_engineer_58c065be",
    "content": "Report timeout: user_dashboard query exceeded 59642ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-10T17:48:00Z",
      "severity": "warning",
      "incident_id": "17598b13",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 59642.26754006477,
        "rows_scanned": 33718938,
        "slowdown_factor": 58
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "17598b13_sre_2129bb0b",
    "content": "Service alert: user_dashboard queries 58x slower, CPU 90%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-10T18:00:00Z",
      "severity": "warning",
      "incident_id": "17598b13",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "user_dashboard",
        "query_time_after_ms": 59642.26754006477,
        "query_time_before_ms": 130.74624319103367
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_be62dc69197b4d64",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-10T19:03:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_ad43cd93ccd0476b",
    "content": "Statistics updated for 33 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-11T06:47:03Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_c45bed9da52743ee",
    "content": "Query performance within SLA - P99 latency 75ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-13T05:41:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_e457c6e9b1204e49",
    "content": "Query performance within SLA - P99 latency 63ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-13T09:11:26Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_e6ee0faf36ab413a",
    "content": "Statistics updated for 8 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-13T14:31:12Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8f6cde49_developer_cd1ed435",
    "content": "ConnectionPoolExhaustedException: All 1000 connections in use (waited 13825ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-14T00:05:00Z",
      "severity": "info",
      "incident_id": "8f6cde49",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 13825.455308512048,
        "threshold": 88,
        "max_connections": 1000
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8f6cde49_sre_81aa5730",
    "content": "CRITICAL: Database connections at 88% capacity (981 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-14T00:05:00Z",
      "severity": "info",
      "incident_id": "8f6cde49",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 981.8175765469715,
        "error_code": "PG-53300",
        "wait_ms": 13825,
        "wait_count": 47
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_a24918011c1d447b",
    "content": "Connection pool stable at 365 connections (16% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-14T06:44:47Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_fb29f4ce19b24ab7",
    "content": "Checkpoint completed: 38599 buffers written in 6.3 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-14T13:06:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e7a6770e_sre_23a7eaa9",
    "content": "Anomaly: Idle connections increasing 15/hour for past 11 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-15T09:05:00Z",
      "severity": "warning",
      "incident_id": "e7a6770e",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 11,
        "active_connections": 580.235977702888,
        "max_connections": 1000,
        "leak_rate_per_hour": 14.8
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e7a6770e_data_engineer_c615f5d1",
    "content": "Spark executors holding 260 idle connections, 461MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-15T09:07:00Z",
      "severity": "warning",
      "incident_id": "e7a6770e",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 58,
        "leak_rate_per_hour": 14.8,
        "active_connections": 580.235977702888
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_f276e79e52ce4731",
    "content": "Query performance within SLA - P99 latency 165ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-16T05:41:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_12f79ebc27404e7d",
    "content": "Backup successful: 95.5GB completed in 31 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-16T20:55:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7f4564_dba_ad17e505",
    "content": "Long-running vacuum: 62min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-16T21:44:00Z",
      "severity": "info",
      "incident_id": "6e7f4564",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 69,
        "wait_count": 8,
        "tables_affected": "metrics"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7f4564_sre_384435cb",
    "content": "IO spike: 69% during vacuum of 'metrics'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-16T21:51:00Z",
      "severity": "info",
      "incident_id": "6e7f4564",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 62,
        "dead_tuples": 98998269,
        "table_bloat_percent": 75
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6e7f4564_data_engineer_028d5a38",
    "content": "Batch job delayed by autovacuum processing 98998269 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-16T21:59:00Z",
      "severity": "info",
      "incident_id": "6e7f4564",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "metrics",
        "io_usage_percent": 69,
        "table_bloat_percent": 75,
        "vacuum_duration_min": 62
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_3aee14ca7fd5440a",
    "content": "Statistics updated for 12 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-17T08:40:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e9e3c2b4_sre_668ac912",
    "content": "P99 spike: user_dashboard from 73.5ms to 42507ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-17T09:24:00Z",
      "severity": "warning",
      "incident_id": "e9e3c2b4",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 42507.74031213343,
        "rows_scanned": 38433149,
        "slowdown_factor": 371
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e9e3c2b4_developer_3aa1b4a1",
    "content": "Service degradation: 'user_dashboard' exceeding SLA by 371x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-17T09:32:00Z",
      "severity": "warning",
      "incident_id": "e9e3c2b4",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 73.50885644726463,
        "slowdown_factor": 371,
        "cpu_usage": 96,
        "query_time_after_ms": 42507.74031213343,
        "affected_query": "user_dashboard"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "baea60f6_developer_f581fc22",
    "content": "Read-after-write inconsistency - replica 280s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-17T22:08:00Z",
      "severity": "warning",
      "incident_id": "baea60f6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_seconds": 280,
        "wal_size_mb": 2397,
        "network_latency_ms": 33,
        "wait_count": 9
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "baea60f6_dba_4b50d28b",
    "content": "Read replica out of sync by 280 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-17T22:21:00Z",
      "severity": "warning",
      "incident_id": "baea60f6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 33,
        "wal_size_mb": 2397,
        "lag_seconds": 280,
        "wait_count": 9,
        "lag_bytes": 62559874
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "baea60f6_data_engineer_6ec2f9aa",
    "content": "Analytics queries on stale data - replica 280s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-17T22:24:00Z",
      "severity": "warning",
      "incident_id": "baea60f6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_seconds": 280,
        "wait_count": 9,
        "lag_bytes": 62559874
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_8e9ef3fb8f494b71",
    "content": "Buffer cache warmed - 10.8GB loaded into memory",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-17T22:40:27Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "1bf17a9e_developer_15346f14",
    "content": "Database swapping 12.6GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-19T15:25:00Z",
      "severity": "warning",
      "incident_id": "1bf17a9e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 55,
        "swap_usage_gb": 12.6,
        "memory_usage_percent": 96,
        "oom_kills": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "1bf17a9e_data_engineer_42e9e0f4",
    "content": "Batch job performance degraded - cache hit 55%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-19T15:26:00Z",
      "severity": "critical",
      "incident_id": "1bf17a9e",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 5,
        "oom_kills": 4,
        "wait_count": 30,
        "buffer_cache_hit": 55,
        "swap_usage_gb": 12.6
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_dd8c1dd35cb341d3",
    "content": "Statistics updated for 26 tables",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-20T11:26:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "776d7a81_sre_27597f18",
    "content": "Network issue causing 31ms latency, 247s replica lag",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-21T14:54:00Z",
      "severity": "info",
      "incident_id": "776d7a81",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 3782,
        "lag_bytes": 17276905,
        "network_latency_ms": 31,
        "wait_count": 20
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "776d7a81_developer_1bbe5a38",
    "content": "Stale data on read replicas - 247s replication delay",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-21T14:58:00Z",
      "severity": "info",
      "incident_id": "776d7a81",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 31,
        "lag_bytes": 17276905,
        "replica_count": 4,
        "wait_count": 20,
        "lag_seconds": 247
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d560fca6_data_engineer_1079fa99",
    "content": "ETL reading outdated data from replica lagging 13s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-21T19:40:00Z",
      "severity": "warning",
      "incident_id": "d560fca6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 33,
        "lag_bytes": 48069414,
        "network_latency_ms": 36
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d560fca6_developer_ba77ccf3",
    "content": "Application seeing outdated data - 13s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-21T19:41:00Z",
      "severity": "info",
      "incident_id": "d560fca6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 33,
        "replica_count": 3,
        "wal_size_mb": 1163,
        "network_latency_ms": 36
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d560fca6_sre_bcfac636",
    "content": "Replica health check: 48069414 bytes behind primary",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-21T19:44:00Z",
      "severity": "info",
      "incident_id": "d560fca6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 36,
        "lag_seconds": 13,
        "lag_bytes": 48069414
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "66c7a0e1_data_engineer_fb2195d8",
    "content": "Report inconsistencies due to 65s replication lag",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-21T22:16:00Z",
      "severity": "warning",
      "incident_id": "66c7a0e1",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 65,
        "replica_count": 3,
        "lag_bytes": 26527184
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "66c7a0e1_dba_6ffbf4f9",
    "content": "Replication lag: 65s behind primary, 26527184 bytes pending",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-21T22:23:00Z",
      "severity": "warning",
      "incident_id": "66c7a0e1",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 26527184,
        "wal_size_mb": 2440,
        "wait_count": 43,
        "replica_count": 3,
        "network_latency_ms": 44
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "28fd0592_sre_2d534080",
    "content": "Alert: WAL accumulation 3881MB, lag 150s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-23T02:34:00Z",
      "severity": "warning",
      "incident_id": "28fd0592",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 40362871,
        "lag_seconds": 150,
        "wait_count": 46,
        "replica_count": 1,
        "network_latency_ms": 42
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "28fd0592_data_engineer_8be3b0a5",
    "content": "Analytics queries on stale data - replica 150s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-23T02:42:00Z",
      "severity": "warning",
      "incident_id": "28fd0592",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 46,
        "lag_bytes": 40362871,
        "replica_count": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "28fd0592_developer_b49c7744",
    "content": "Read-after-write inconsistency - replica 150s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-23T02:44:00Z",
      "severity": "warning",
      "incident_id": "28fd0592",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 3881,
        "lag_seconds": 150,
        "replica_count": 1,
        "lag_bytes": 40362871
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_3cc74ff1fab0456f",
    "content": "Database health check passed - response time 167ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-23T15:41:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_9793c619005b4690",
    "content": "Memory usage normal - 49.3GB of 128GB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-24T10:50:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "66a1f514_sre_30c64769",
    "content": "P99 spike: user_dashboard from 189ms to 6301ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-25T03:16:00Z",
      "severity": "warning",
      "incident_id": "66a1f514",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 40975058,
        "affected_query": "user_dashboard",
        "query_time_before_ms": 189.00234463321902,
        "wait_count": 45,
        "slowdown_factor": 214
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "66a1f514_dba_b9faed50",
    "content": "Performance alert: 'user_dashboard' execution time 214x slower at 79% CPU",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-25T03:24:00Z",
      "severity": "warning",
      "incident_id": "66a1f514",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 40975058,
        "wait_count": 45,
        "cpu_usage": 79,
        "query_time_before_ms": 189.00234463321902
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "66a1f514_developer_fa844f19",
    "content": "Service degradation: 'user_dashboard' exceeding SLA by 214x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-25T03:25:00Z",
      "severity": "warning",
      "incident_id": "66a1f514",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 40975058,
        "cpu_usage": 79,
        "query_time_after_ms": 6301.019844151826,
        "query_time_before_ms": 189.00234463321902,
        "slowdown_factor": 214
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_645f514b6dcd4cc3",
    "content": "Buffer cache warmed - 10.2GB loaded into memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-25T13:54:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_be0d1bee08e24bad",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-25T18:16:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "00b0ab71_developer_f62199b6",
    "content": "Performance regression: 'inventory_check' now 463x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-27T09:37:00Z",
      "severity": "warning",
      "incident_id": "00b0ab71",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "inventory_check",
        "cpu_usage": 75,
        "wait_count": 42
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "00b0ab71_data_engineer_a090588d",
    "content": "Batch job delayed: inventory_check running 463x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-27T09:41:00Z",
      "severity": "critical",
      "incident_id": "00b0ab71",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 75,
        "wait_count": 42,
        "query_time_after_ms": 47721.03608438943
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "00b0ab71_dba_3216bb34",
    "content": "Query plan changed: 'inventory_check' now takes 47721ms scanning 24086412 rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-27T09:46:00Z",
      "severity": "critical",
      "incident_id": "00b0ab71",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 47721.03608438943,
        "cpu_usage": 75,
        "wait_count": 42,
        "affected_query": "inventory_check"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "00b0ab71_sre_9fbde147",
    "content": "CloudWatch: Query latency increased 463x for 'inventory_check'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-02-27T09:48:00Z",
      "severity": "critical",
      "incident_id": "00b0ab71",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 47721.03608438943,
        "cpu_usage": 75,
        "wait_count": 42
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_23d30d6072de486c",
    "content": "Database health check passed - response time 83ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-28T02:20:38Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_10ab284b85cb4293",
    "content": "Autovacuum completed on table 'products' - 772483 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-02-28T03:11:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_93453257b644414d",
    "content": "Buffer cache warmed - 28.1GB loaded into memory",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-02-28T05:23:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_f9fe26dc0aa940de",
    "content": "Query cache hit ratio: 94% over last 27 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-02-28T07:41:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_e3d2fed137134348",
    "content": "Database health check passed - response time 39ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-01T10:52:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_3f14ec9360eb434a",
    "content": "Connection pool stable at 446 connections (75% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-02T01:42:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "3d90f1b6_sre_61e59d22",
    "content": "P99 spike: inventory_check from 110ms to 58294ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-02T15:02:00Z",
      "severity": "warning",
      "incident_id": "3d90f1b6",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 110.06513696849123,
        "slowdown_factor": 283,
        "cpu_usage": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "3d90f1b6_dba_4aea02b5",
    "content": "Performance alert: 'inventory_check' execution time 283x slower at 97% CPU",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-02T15:15:00Z",
      "severity": "warning",
      "incident_id": "3d90f1b6",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 110.06513696849123,
        "rows_scanned": 45845366,
        "wait_count": 27,
        "cpu_usage": 97
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_fe09d7f73c7c4eb5",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-03T11:10:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7d58a2c682b046df",
    "content": "CPU utilization stable at 23%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-03T11:45:54Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_7edb61ea493942c3",
    "content": "Query cache hit ratio: 89% over last 28 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-04T04:59:10Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0b87e729_dba_9620e02e",
    "content": "PANIC: connection limit exceeded - 970/1000 after 8.4min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-04T18:10:00Z",
      "severity": "critical",
      "incident_id": "0b87e729",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 970,
        "threshold": 95,
        "wait_time_ms": 9214.388018666516,
        "active_connections": 970.4031731491751,
        "spike_duration_min": 8.4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0b87e729_sre_37f02faf",
    "content": "AWS RDS Alert: Connection count 970, wait time 9214ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-04T18:10:00Z",
      "severity": "critical",
      "incident_id": "0b87e729",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active_connections": 970.4031731491751,
        "spike_duration_min": 8.4,
        "wait_time_ms": 9214.388018666516,
        "active": 970
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0b87e729_data_engineer_f012113e",
    "content": "Spark job waiting 9214ms for database connection - 30 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-04T18:24:00Z",
      "severity": "critical",
      "incident_id": "0b87e729",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active": 970,
        "wait_count": 30,
        "active_connections": 970.4031731491751,
        "spike_duration_min": 8.4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0afdfc23414b4c3e",
    "content": "Statistics updated for 24 tables",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-05T12:26:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "94ec57fd_dba_e014ef42",
    "content": "work_mem reduced to 53MB - 6 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-05T21:47:00Z",
      "severity": "warning",
      "incident_id": "94ec57fd",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 47,
        "oom_kills": 6,
        "work_mem_mb": 53
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "94ec57fd_sre_2b306a07",
    "content": "Memory alert: 93% utilized, 6 OOM kills",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-05T21:54:00Z",
      "severity": "warning",
      "incident_id": "94ec57fd",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 93,
        "wait_count": 20,
        "oom_kills": 6,
        "work_mem_mb": 53,
        "swap_usage_gb": 4.3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "94ec57fd_developer_aa36bbf1",
    "content": "Application queries failing - database memory at 93%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-05T21:57:00Z",
      "severity": "warning",
      "incident_id": "94ec57fd",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 4.3,
        "work_mem_mb": 53,
        "memory_usage_percent": 93,
        "wait_count": 20,
        "buffer_cache_hit": 47
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_bbb8ae65108546a0",
    "content": "Query performance within SLA - P99 latency 131ms",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-06T12:38:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_76598fe7d77e42ef",
    "content": "CPU utilization stable at 46%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-07T04:58:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "09a77d3b_developer_670790da",
    "content": "ConnectionPoolExhaustedException: All 1000 connections in use (waited 15616ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-07T09:43:00Z",
      "severity": "warning",
      "incident_id": "09a77d3b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 15616,
        "spike_duration_min": 19.0,
        "active_connections": 996.3755617374268,
        "error_code": "PG-53300",
        "active": 996
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "09a77d3b_sre_e54c212c",
    "content": "CRITICAL: Database connections at 90% capacity (996 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-07T09:53:00Z",
      "severity": "warning",
      "incident_id": "09a77d3b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_time_ms": 15616.750696565028,
        "active_connections": 996.3755617374268,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "09a77d3b_dba_f6444db5",
    "content": "PANIC: connection limit exceeded - 996/1000 after 19.0min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-07T10:02:00Z",
      "severity": "warning",
      "incident_id": "09a77d3b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 21,
        "wait_ms": 15616,
        "active": 996,
        "error_code": "PG-53300",
        "wait_time_ms": 15616.750696565028
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_9678b1aa64a14a5f",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-07T23:23:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_e28dd2fb205740de",
    "content": "Query performance within SLA - P99 latency 427ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-08T05:22:40Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "fff8a044_data_engineer_072b5afe",
    "content": "Pipeline memory issues: work_mem only 50MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-08T20:32:00Z",
      "severity": "critical",
      "incident_id": "fff8a044",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 69,
        "work_mem_mb": 50,
        "wait_count": 50
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fff8a044_dba_48a95b03",
    "content": "Buffer cache hit ratio dropped to 69% due to memory pressure",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-08T20:42:00Z",
      "severity": "critical",
      "incident_id": "fff8a044",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 91,
        "oom_kills": 8,
        "swap_usage_gb": 12.1,
        "wait_count": 50
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fff8a044_developer_e3a46efc",
    "content": "Memory exhaustion causing slowdown - cache hit ratio 69%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-08T20:48:00Z",
      "severity": "warning",
      "incident_id": "fff8a044",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 12.1,
        "work_mem_mb": 50,
        "memory_usage_percent": 91
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ad2998f7_developer_d0153de6",
    "content": "Performance regression: 'order_search' now 420x slower than baseline",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-09T04:41:00Z",
      "severity": "warning",
      "incident_id": "ad2998f7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "cpu_usage": 82,
        "wait_count": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ad2998f7_data_engineer_fa328a77",
    "content": "Batch job delayed: order_search running 420x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-09T04:43:00Z",
      "severity": "warning",
      "incident_id": "ad2998f7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 37,
        "slowdown_factor": 420,
        "query_time_after_ms": 57140.63990144686
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ad2998f7_sre_953ab3e2",
    "content": "CloudWatch: Query latency increased 420x for 'order_search'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-09T04:43:00Z",
      "severity": "warning",
      "incident_id": "ad2998f7",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "rows_scanned": 16712769,
        "cpu_usage": 82,
        "slowdown_factor": 420,
        "query_time_after_ms": 57140.63990144686
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "481357fb_data_engineer_49f48eb6",
    "content": "Data freshness issue: 82409142 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-10T03:26:00Z",
      "severity": "info",
      "incident_id": "481357fb",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 48,
        "wal_size_mb": 1391,
        "lag_bytes": 82409142
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "481357fb_sre_b779198d",
    "content": "Critical: Replication lag exceeding SLA (254s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-10T03:32:00Z",
      "severity": "info",
      "incident_id": "481357fb",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 48,
        "lag_bytes": 82409142,
        "replica_count": 3,
        "lag_seconds": 254,
        "wal_size_mb": 1391
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "481357fb_dba_4be0ed1d",
    "content": "WARNING: Replica 3 lagging 254s, WAL 1391MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-10T03:35:00Z",
      "severity": "info",
      "incident_id": "481357fb",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 3,
        "lag_seconds": 254,
        "lag_bytes": 82409142,
        "wal_size_mb": 1391
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "cccf4e65_data_engineer_0929dc5d",
    "content": "Table 'sessions' bloat (67%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-11T12:34:00Z",
      "severity": "warning",
      "incident_id": "cccf4e65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 235,
        "table_bloat_percent": 67,
        "tables_affected": "sessions",
        "io_usage_percent": 54,
        "dead_tuples": 19130786
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cccf4e65_dba_5c9f92e7",
    "content": "Autovacuum running 235min on 'sessions', IO at 54%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-11T12:38:00Z",
      "severity": "info",
      "incident_id": "cccf4e65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "wait_count": 36,
        "table_bloat_percent": 67,
        "dead_tuples": 19130786,
        "vacuum_duration_min": 235,
        "io_usage_percent": 54
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "cccf4e65_developer_afa642db",
    "content": "Database maintenance causing 54% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-11T12:44:00Z",
      "severity": "info",
      "incident_id": "cccf4e65",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 19130786,
        "io_usage_percent": 54,
        "tables_affected": "sessions"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_91669bd9195f4097",
    "content": "Replication healthy - all 5 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-12T08:35:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_2383be29a0a345f3",
    "content": "WAL archiving on schedule - 57 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-12T16:55:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b5369bd2_dba_601b38cd",
    "content": "Aggressive vacuum on 'metrics' - 88124592 dead tuples, 71% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-12T20:11:00Z",
      "severity": "info",
      "incident_id": "b5369bd2",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 58,
        "dead_tuples": 88124592,
        "wait_count": 16,
        "table_bloat_percent": 71
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b5369bd2_sre_78bf7074",
    "content": "IO spike: 58% during vacuum of 'metrics'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-12T20:14:00Z",
      "severity": "info",
      "incident_id": "b5369bd2",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 71,
        "wait_count": 16,
        "vacuum_duration_min": 198
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "b5369bd2_developer_f995913f",
    "content": "Query performance degraded - table 'metrics' 71% bloated",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-12T20:18:00Z",
      "severity": "info",
      "incident_id": "b5369bd2",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "metrics",
        "wait_count": 16,
        "io_usage_percent": 58,
        "vacuum_duration_min": 198
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "01501adb_sre_41744c3f",
    "content": "CRITICAL: Database connections at 85% capacity (952 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-12T21:58:00Z",
      "severity": "warning",
      "incident_id": "01501adb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "spike_duration_min": 26.9,
        "active_connections": 952.6630686276326,
        "wait_ms": 23317,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "01501adb_dba_809e8148",
    "content": "Database refusing connections: SQLSTATE 53300 after 23317ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-12T22:05:00Z",
      "severity": "warning",
      "incident_id": "01501adb",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 23317,
        "error_code": "PG-53300",
        "max_connections": 1000,
        "wait_time_ms": 23317.42031751477,
        "active_connections": 952.6630686276326
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_653e1f77dbe94743",
    "content": "Connection pool stable at 230 connections (60% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-13T12:15:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ac9640ca73d1491a",
    "content": "Query performance within SLA - P99 latency 162ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-14T03:20:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "49cadad5_data_engineer_e19c8159",
    "content": "Batch job leak: 5.7 connections/hour over 2h runtime",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-14T04:13:00Z",
      "severity": "warning",
      "incident_id": "49cadad5",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 66,
        "idle_connections": 205,
        "wait_count": 8,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "49cadad5_sre_b3d5c8b5",
    "content": "Resource alert: 205 idle connections using 434MB RAM",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-14T04:17:00Z",
      "severity": "warning",
      "incident_id": "49cadad5",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 8,
        "leak_rate_per_hour": 5.7,
        "active": 666
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "49cadad5_developer_2f0f7313",
    "content": "Memory leak in connection handling - 6 connections/hour, 434MB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-14T04:29:00Z",
      "severity": "warning",
      "incident_id": "49cadad5",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 205,
        "memory_usage_mb": 434,
        "wait_count": 8,
        "leak_rate_per_hour": 5.7
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "49cadad5_dba_e28b2d66",
    "content": "Connection leak: idle_in_transaction growing at 6/hour for 2h",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-14T04:30:00Z",
      "severity": "warning",
      "incident_id": "49cadad5",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 434,
        "max_connections": 1000,
        "wait_count": 8,
        "threshold": 66,
        "idle_connections": 205
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_7e61d0f0c3434ce7",
    "content": "Connection recycling: 48 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-15T11:14:14Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_963e117c07b447b4",
    "content": "Buffer cache warmed - 23.0GB loaded into memory",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-15T23:32:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "ae83d118_sre_6e0316f0",
    "content": "System alert: 1784ms lock wait impacting 159 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-16T06:15:00Z",
      "severity": "warning",
      "incident_id": "ae83d118",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 35,
        "transaction_rollback": 159,
        "cascade_duration_min": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ae83d118_dba_8cf5089c",
    "content": "Lock escalation: 159 transactions rolled back on 'users'",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-16T06:17:00Z",
      "severity": "warning",
      "incident_id": "ae83d118",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 159,
        "deadlocks_per_minute": 30,
        "affected_tables": "users",
        "blocked_queries": 35,
        "cascade_duration_min": 4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ae83d118_developer_4856ee4a",
    "content": "Transaction retry exhausted - 30 deadlocks/min on 'users'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-16T06:28:00Z",
      "severity": "warning",
      "incident_id": "ae83d118",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 30,
        "cascade_duration_min": 4,
        "wait_count": 36,
        "lock_wait_ms": 1784
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "276c3140_developer_5494774d",
    "content": "HikariPool-1 - Connection timeout after 15913ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-16T09:39:00Z",
      "severity": "critical",
      "incident_id": "276c3140",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 30,
        "wait_time_ms": 15913.07287860114,
        "threshold": 91
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "276c3140_sre_96bac64d",
    "content": "Alert: Connection saturation 997/1000 for 58 minutes",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-16T09:41:00Z",
      "severity": "critical",
      "incident_id": "276c3140",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 15913,
        "active_connections": 997.8377103978873,
        "error_code": "PG-53300",
        "threshold": 91
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "276c3140_dba_e4b0b7b0",
    "content": "Database refusing connections: SQLSTATE 53300 after 15913ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-16T09:42:00Z",
      "severity": "critical",
      "incident_id": "276c3140",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 997.8377103978873,
        "error_code": "PG-53300",
        "wait_ms": 15913
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "ec9630dc_dba_dcfe22aa",
    "content": "Streaming replication delayed 290s due to 5ms network latency",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-17T02:56:00Z",
      "severity": "warning",
      "incident_id": "ec9630dc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 35,
        "lag_seconds": 290,
        "wal_size_mb": 917,
        "lag_bytes": 88872762
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ec9630dc_data_engineer_622b8985",
    "content": "ETL reading outdated data from replica lagging 290s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-17T03:02:00Z",
      "severity": "warning",
      "incident_id": "ec9630dc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 290,
        "wait_count": 35,
        "wal_size_mb": 917,
        "replica_count": 4,
        "network_latency_ms": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "ec9630dc_developer_c7be1383",
    "content": "Application seeing outdated data - 290s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-17T03:03:00Z",
      "severity": "warning",
      "incident_id": "ec9630dc",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 35,
        "lag_seconds": 290,
        "replica_count": 4,
        "lag_bytes": 88872762,
        "network_latency_ms": 5
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "e9f37116_dba_6ce7c2d9",
    "content": "Connection accumulation: 14.2 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-17T10:51:00Z",
      "severity": "info",
      "incident_id": "e9f37116",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 14.2,
        "threshold": 63,
        "max_connections": 1000,
        "active": 634
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e9f37116_sre_dc617c9c",
    "content": "Linear growth: 304 idle connections, 336MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-17T11:01:00Z",
      "severity": "info",
      "incident_id": "e9f37116",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "memory_usage_mb": 336,
        "active": 634,
        "active_connections": 634.8029131806314,
        "max_connections": 1000,
        "idle_connections": 304
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e9f37116_developer_e347c7d9",
    "content": "Connection pool: 304 idle, 634 active - possible 336MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-17T11:04:00Z",
      "severity": "info",
      "incident_id": "e9f37116",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 634.8029131806314,
        "max_connections": 1000,
        "memory_usage_mb": 336,
        "threshold": 63,
        "idle_connections": 304
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "e9f37116_data_engineer_9f52a4e0",
    "content": "ETL connection pool: 304 idle connections leaking at 14/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-17T11:11:00Z",
      "severity": "info",
      "incident_id": "e9f37116",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 304,
        "wait_count": 34,
        "leak_duration_hours": 17,
        "memory_usage_mb": 336
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "410d73f7_developer_7c7c0a8e",
    "content": "Transaction retry exhausted - 38 deadlocks/min on 'orders'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-17T12:53:00Z",
      "severity": "warning",
      "incident_id": "410d73f7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 12,
        "lock_wait_ms": 2569,
        "wait_count": 27
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "410d73f7_sre_4e215e71",
    "content": "System alert: 2569ms lock wait impacting 169 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-17T13:07:00Z",
      "severity": "critical",
      "incident_id": "410d73f7",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 38,
        "cascade_duration_min": 12,
        "affected_tables": "orders",
        "transaction_rollback": 169
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "8ecd9e65_dba_0030e0a9",
    "content": "work_mem reduced to 56MB - 5 OOM events occurred",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-18T04:16:00Z",
      "severity": "critical",
      "incident_id": "8ecd9e65",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 41,
        "buffer_cache_hit": 31,
        "oom_kills": 5,
        "swap_usage_gb": 8.6
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "8ecd9e65_data_engineer_140de116",
    "content": "Memory pressure: 92% used, jobs failing",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-18T04:33:00Z",
      "severity": "critical",
      "incident_id": "8ecd9e65",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 92,
        "swap_usage_gb": 8.6,
        "buffer_cache_hit": 31
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "11eb88b4_data_engineer_c3fbbc75",
    "content": "Data freshness issue: 95870423 bytes replication backlog",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-19T01:08:00Z",
      "severity": "warning",
      "incident_id": "11eb88b4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 16,
        "wal_size_mb": 2978,
        "lag_seconds": 200,
        "lag_bytes": 95870423
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "11eb88b4_sre_73fca062",
    "content": "Critical: Replication lag exceeding SLA (200s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-19T01:16:00Z",
      "severity": "warning",
      "incident_id": "11eb88b4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 27,
        "lag_bytes": 95870423,
        "wal_size_mb": 2978,
        "wait_count": 16,
        "replica_count": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "11eb88b4_dba_ac37bd35",
    "content": "WARNING: Replica 3 lagging 200s, WAL 2978MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-19T01:16:00Z",
      "severity": "warning",
      "incident_id": "11eb88b4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 27,
        "replica_count": 3,
        "lag_bytes": 95870423,
        "lag_seconds": 200
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "11eb88b4_developer_14b609e5",
    "content": "Data consistency issue: replicas lagging by 95870423 bytes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-19T01:24:00Z",
      "severity": "warning",
      "incident_id": "11eb88b4",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 27,
        "lag_seconds": 200,
        "wait_count": 16,
        "lag_bytes": 95870423,
        "replica_count": 3
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_227a52af477349a2",
    "content": "Statistics updated for 28 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-19T23:20:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a6eff10b_developer_967842ac",
    "content": "HikariPool-1 - Connection timeout after 28670ms during traffic surge",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-21T12:53:00Z",
      "severity": "warning",
      "incident_id": "a6eff10b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "spike_duration_min": 50.5,
        "active_connections": 954.3007367443755,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a6eff10b_dba_7d16852f",
    "content": "Database refusing connections: SQLSTATE 53300 after 28670ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-21T13:03:00Z",
      "severity": "critical",
      "incident_id": "a6eff10b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 8,
        "active_connections": 954.3007367443755,
        "wait_time_ms": 28670.434027915137
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a6eff10b_data_engineer_514438d3",
    "content": "ETL job 'daily_aggregation' failed - connection timeout after 28670ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-21T13:09:00Z",
      "severity": "critical",
      "incident_id": "a6eff10b",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 50.5,
        "active": 954,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "806830cf_dba_9c53b7fa",
    "content": "Performance alert: 'user_dashboard' execution time 174x slower at 93% CPU",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-22T06:22:00Z",
      "severity": "critical",
      "incident_id": "806830cf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 93,
        "affected_query": "user_dashboard",
        "rows_scanned": 24576847,
        "query_time_before_ms": 115.92164016773518,
        "slowdown_factor": 174
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "806830cf_data_engineer_3a0f20d5",
    "content": "Data pipeline: 'user_dashboard' degraded from 115ms to 54018ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-22T06:26:00Z",
      "severity": "critical",
      "incident_id": "806830cf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "user_dashboard",
        "wait_count": 19,
        "rows_scanned": 24576847,
        "query_time_after_ms": 54018.44554576524
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "806830cf_sre_19636fd9",
    "content": "P99 spike: user_dashboard from 115ms to 54018ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-22T06:30:00Z",
      "severity": "critical",
      "incident_id": "806830cf",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 19,
        "cpu_usage": 93,
        "query_time_after_ms": 54018.44554576524,
        "slowdown_factor": 174,
        "query_time_before_ms": 115.92164016773518
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "bfffbdc2_sre_7e9f36e4",
    "content": "System alert: 7301ms lock wait impacting 194 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-22T12:39:00Z",
      "severity": "warning",
      "incident_id": "bfffbdc2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "deadlocks_per_minute": 41,
        "blocked_queries": 41,
        "lock_wait_ms": 7301,
        "transaction_rollback": 194
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "bfffbdc2_developer_b5cfb52d",
    "content": "Transaction retry exhausted - 41 deadlocks/min on 'shipments'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-22T12:41:00Z",
      "severity": "warning",
      "incident_id": "bfffbdc2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 38,
        "deadlocks_per_minute": 41,
        "transaction_rollback": 194
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "dd099fd3_data_engineer_77b78fa3",
    "content": "ETL connection pool: 288 idle connections leaking at 9/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-23T00:55:00Z",
      "severity": "warning",
      "incident_id": "dd099fd3",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 636.12872862583,
        "max_connections": 1000,
        "idle_connections": 288,
        "memory_usage_mb": 102
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "dd099fd3_sre_0815f92a",
    "content": "Linear growth: 288 idle connections, 102MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-23T01:04:00Z",
      "severity": "warning",
      "incident_id": "dd099fd3",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "wait_count": 47,
        "memory_usage_mb": 102,
        "leak_duration_hours": 2,
        "idle_connections": 288
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_9c72eb1c7a014ba2",
    "content": "Backup successful: 5.6GB completed in 58 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-23T10:52:30Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "7fb0b851_developer_7dcbd35d",
    "content": "JDBC connection failed after 18303ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-23T13:43:00Z",
      "severity": "warning",
      "incident_id": "7fb0b851",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "threshold": 97,
        "spike_duration_min": 28.0,
        "wait_count": 18,
        "active": 977
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "7fb0b851_dba_9e6a5edf",
    "content": "ERROR: too many connections for role 'app_user' (current: 977, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-23T13:44:00Z",
      "severity": "warning",
      "incident_id": "7fb0b851",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "error_code": "PG-53300",
        "active": 977,
        "active_connections": 977.81096609083
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_a279994d2abb4e8d",
    "content": "Connection pool stable at 341 connections (11% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-23T14:27:19Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "f8517868_data_engineer_6b42cb2e",
    "content": "Report timeout: report_aggregate query exceeded 18732ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-23T22:45:00Z",
      "severity": "info",
      "incident_id": "f8517868",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 270,
        "cpu_usage": 85,
        "rows_scanned": 8123500,
        "wait_count": 10,
        "query_time_after_ms": 18732.50657615853
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f8517868_sre_fbbf73c6",
    "content": "Service alert: report_aggregate queries 270x slower, CPU 85%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-23T22:48:00Z",
      "severity": "info",
      "incident_id": "f8517868",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "wait_count": 10,
        "rows_scanned": 8123500,
        "query_time_after_ms": 18732.50657615853,
        "slowdown_factor": 270
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f8517868_developer_684622fe",
    "content": "API timeout: 'report_aggregate' taking 18732ms (was 182ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-23T23:01:00Z",
      "severity": "info",
      "incident_id": "f8517868",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "report_aggregate",
        "wait_count": 10,
        "cpu_usage": 85,
        "query_time_after_ms": 18732.50657615853,
        "rows_scanned": 8123500
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_f9a6d22ddce043ff",
    "content": "Connection pool stable at 108 connections (45% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-24T00:23:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_f31b9ddc72314ab9",
    "content": "Database health check passed - response time 36ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-24T05:49:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "000735a1_data_engineer_d31df5fe",
    "content": "Spark job waiting 11598ms for database connection - 46 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-26T08:38:00Z",
      "severity": "warning",
      "incident_id": "000735a1",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 8.2,
        "active_connections": 954.8154454932213,
        "wait_count": 46,
        "wait_ms": 11598
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "000735a1_sre_e93a00be",
    "content": "AWS RDS Alert: Connection count 954, wait time 11598ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-26T08:41:00Z",
      "severity": "warning",
      "incident_id": "000735a1",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "max_connections": 1000,
        "active_connections": 954.8154454932213,
        "wait_ms": 11598
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "000735a1_dba_b76c3069",
    "content": "PANIC: connection limit exceeded - 954/1000 after 8.2min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-26T08:57:00Z",
      "severity": "warning",
      "incident_id": "000735a1",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 954,
        "wait_count": 46,
        "wait_ms": 11598,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_054f4a39827e42f0",
    "content": "Statistics updated for 25 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-26T21:14:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6ad7565e_sre_4a84464d",
    "content": "AWS RDS Alert: Connection count 997, wait time 13040ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-27T02:44:00Z",
      "severity": "info",
      "incident_id": "6ad7565e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 5.3,
        "max_connections": 1000,
        "wait_ms": 13040
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "6ad7565e_developer_c36f680e",
    "content": "Connection pool exhausted - 14 threads waiting, 997 active connections",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-27T02:53:00Z",
      "severity": "info",
      "incident_id": "6ad7565e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_ms": 13040,
        "max_connections": 1000,
        "wait_time_ms": 13040.89323037124,
        "active": 997
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "6ad7565e_data_engineer_9f6f59d3",
    "content": "Spark job waiting 13040ms for database connection - 14 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-27T02:53:00Z",
      "severity": "warning",
      "incident_id": "6ad7565e",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 997.8290922939261,
        "spike_duration_min": 5.3,
        "max_connections": 1000,
        "wait_time_ms": 13040.89323037124,
        "error_code": "PG-53300"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "42df31d5_developer_b6cff134",
    "content": "Application queries failing - database memory at 86%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-27T10:32:00Z",
      "severity": "warning",
      "incident_id": "42df31d5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 8,
        "swap_usage_gb": 11.2,
        "memory_usage_percent": 86
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "42df31d5_data_engineer_ba99f06c",
    "content": "Data processing failed - 8 tasks killed, 11.2GB swap",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-27T10:38:00Z",
      "severity": "critical",
      "incident_id": "42df31d5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 40,
        "wait_count": 8,
        "work_mem_mb": 42,
        "swap_usage_gb": 11.2,
        "memory_usage_percent": 86
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "42df31d5_dba_95a1eb1b",
    "content": "FATAL: out of memory - 8 processes killed, 86% memory used",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-27T10:41:00Z",
      "severity": "critical",
      "incident_id": "42df31d5",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 40,
        "wait_count": 8,
        "swap_usage_gb": 11.2,
        "oom_kills": 8
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_df9eebaf29764fe4",
    "content": "CPU utilization stable at 26%",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-27T12:25:20Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "49579d18_dba_692f4382",
    "content": "Aggressive vacuum on 'metrics' - 86212469 dead tuples, 71% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-28T08:30:00Z",
      "severity": "info",
      "incident_id": "49579d18",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 86212469,
        "tables_affected": "metrics",
        "wait_count": 22,
        "table_bloat_percent": 71
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "49579d18_data_engineer_8408bbfc",
    "content": "Data load competing with 268min vacuum operation",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-28T08:36:00Z",
      "severity": "warning",
      "incident_id": "49579d18",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 71,
        "wait_count": 22,
        "dead_tuples": 86212469,
        "io_usage_percent": 84,
        "tables_affected": "metrics"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "49579d18_sre_784211ac",
    "content": "Disk usage alert: 'metrics' table 71% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-28T08:45:00Z",
      "severity": "info",
      "incident_id": "49579d18",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 268,
        "wait_count": 22,
        "table_bloat_percent": 71,
        "dead_tuples": 86212469,
        "tables_affected": "metrics"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d14923ae_dba_97b93119",
    "content": "Emergency: 35GB free, database will halt in 4 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-28T14:26:00Z",
      "severity": "info",
      "incident_id": "d14923ae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 36,
        "days_until_full": 4,
        "disk_usage_percent": 87,
        "free_space_gb": 35
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d14923ae_data_engineer_07de0565",
    "content": "Data retention issue: 59GB logs, 87% disk used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-28T14:26:00Z",
      "severity": "info",
      "incident_id": "d14923ae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 87,
        "wal_size_gb": 59,
        "wait_count": 36,
        "days_until_full": 4,
        "free_space_gb": 35
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "d14923ae_sre_b2ed9396",
    "content": "Disk space alarm: 87% used, 35GB free",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-03-28T14:40:00Z",
      "severity": "info",
      "incident_id": "d14923ae",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 35,
        "growth_rate_gb_per_day": 74,
        "disk_usage_percent": 87
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7219ec7e24b24a79",
    "content": "Network latency optimal - 1.1ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-29T01:05:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_f96ff596585247e5",
    "content": "Autovacuum completed on table 'orders' - 15926 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-29T16:12:05Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "0d62ec6e_dba_378042ca",
    "content": "Vacuum freeze on 'sessions' - 88% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-29T18:46:00Z",
      "severity": "info",
      "incident_id": "0d62ec6e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 138,
        "wait_count": 22,
        "dead_tuples": 30183752,
        "tables_affected": "sessions",
        "io_usage_percent": 88
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "0d62ec6e_data_engineer_8f85f9af",
    "content": "ETL performance impacted by vacuum on 'sessions'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-29T18:56:00Z",
      "severity": "warning",
      "incident_id": "0d62ec6e",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 22,
        "io_usage_percent": 88,
        "dead_tuples": 30183752,
        "wait_count": 22,
        "vacuum_duration_min": 138
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_249741553eb14b1f",
    "content": "Disk I/O normal - 1930 read IOPS, 2657 write IOPS",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-29T21:50:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_384af16775994655",
    "content": "Backup successful: 46.3GB completed in 6 minutes",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-03-30T03:06:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_20c6e699db2e4860",
    "content": "Connection pool stable at 475 connections (24% utilization)",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-03-31T02:41:09Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_7a3a2ccea28a4d2a",
    "content": "Database health check passed - response time 89ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-03-31T15:22:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_232f47e243af4ea1",
    "content": "Memory usage normal - 48.9GB of 128GB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-01T15:11:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "42a70077_dba_ccd026ba",
    "content": "Connection pool saturation: 971 active, 26 waiting, threshold 87% exceeded",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-02T13:42:00Z",
      "severity": "critical",
      "incident_id": "42a70077",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "spike_duration_min": 34.5,
        "wait_count": 26,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "42a70077_sre_d426b974",
    "content": "CRITICAL: Database connections at 87% capacity (971 connections)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-02T13:53:00Z",
      "severity": "critical",
      "incident_id": "42a70077",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 26,
        "active": 971,
        "threshold": 87,
        "wait_ms": 10936,
        "wait_time_ms": 10936.92716435495
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "42a70077_data_engineer_aa5ee4af",
    "content": "Airflow DAG stalled: connection pool exhausted during 34.5min window",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-02T14:01:00Z",
      "severity": "critical",
      "incident_id": "42a70077",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 26,
        "wait_ms": 10936,
        "active": 971,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "42a70077_developer_918cca50",
    "content": "ConnectionPoolExhaustedException: All 1000 connections in use (waited 10936ms)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-02T14:01:00Z",
      "severity": "warning",
      "incident_id": "42a70077",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 971,
        "wait_count": 26,
        "wait_time_ms": 10936.92716435495
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "67120e31_sre_57ed4945",
    "content": "Latency alarm: 'report_aggregate' breached 30359ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-02T21:24:00Z",
      "severity": "info",
      "incident_id": "67120e31",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 6,
        "slowdown_factor": 66,
        "query_time_before_ms": 63.572117937298614,
        "query_time_after_ms": 30359.524090717518
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "67120e31_developer_5442d2e5",
    "content": "Application query 'report_aggregate' timeout after 30359ms at 89% CPU",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-02T21:29:00Z",
      "severity": "info",
      "incident_id": "67120e31",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 41471282,
        "wait_count": 6,
        "query_time_before_ms": 63.572117937298614,
        "slowdown_factor": 66
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c604b27a309d4c0a",
    "content": "Checkpoint completed: 36350 buffers written in 3.6 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-03T03:39:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_00dc53a6138e43bc",
    "content": "Database health check passed - response time 159ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-03T15:39:54Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_7148a4841f664eef",
    "content": "SSL certificate valid for 255 more days",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-03T16:58:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b4bc9f16_sre_66934d58",
    "content": "Emergency response needed: 85% disk utilization",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-04T12:15:00Z",
      "severity": "warning",
      "incident_id": "b4bc9f16",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "growth_rate_gb_per_day": 98,
        "wal_size_gb": 41,
        "free_space_gb": 29,
        "disk_usage_percent": 85,
        "wait_count": 27
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b4bc9f16_dba_62b2a419",
    "content": "Storage alert: Growing 98GB/day, full in 5 days",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-04T12:22:00Z",
      "severity": "warning",
      "incident_id": "b4bc9f16",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 85,
        "wal_size_gb": 41,
        "free_space_gb": 29
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "b4bc9f16_developer_1240126c",
    "content": "Application at risk: only 29GB disk space remaining",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-04T12:27:00Z",
      "severity": "warning",
      "incident_id": "b4bc9f16",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 5,
        "wait_count": 27,
        "wal_size_gb": 41,
        "disk_usage_percent": 85,
        "free_space_gb": 29
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_55d07bd64be64398",
    "content": "Database health check passed - response time 71ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-04T12:30:59Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_aabc679cb8cf4987",
    "content": "SSL certificate valid for 84 more days",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-05T00:31:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4ca55373_sre_a8e071d0",
    "content": "P99 spike: inventory_check from 78.7ms to 51183ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-05T00:51:00Z",
      "severity": "info",
      "incident_id": "4ca55373",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "inventory_check",
        "wait_count": 41,
        "rows_scanned": 2704560,
        "cpu_usage": 97
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4ca55373_data_engineer_a232087f",
    "content": "Data pipeline: 'inventory_check' degraded from 78.7ms to 51183ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-05T00:55:00Z",
      "severity": "info",
      "incident_id": "4ca55373",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 41,
        "cpu_usage": 97,
        "query_time_after_ms": 51183.30029275601,
        "rows_scanned": 2704560,
        "slowdown_factor": 277
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4ca55373_developer_663b84f4",
    "content": "Service degradation: 'inventory_check' exceeding SLA by 277x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-05T01:02:00Z",
      "severity": "info",
      "incident_id": "4ca55373",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 97,
        "query_time_before_ms": 78.6655484678322,
        "query_time_after_ms": 51183.30029275601
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "6261daa0_sre_2e3f8d7b",
    "content": "Incident: Lock contention causing 96 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-05T19:31:00Z",
      "severity": "warning",
      "incident_id": "6261daa0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 24,
        "cascade_duration_min": 12,
        "transaction_rollback": 198
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6261daa0_dba_aee9e19a",
    "content": "CRITICAL: 96 queries waiting, deadlock rate: 38/min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-05T19:39:00Z",
      "severity": "warning",
      "incident_id": "6261daa0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 198,
        "lock_wait_ms": 4651,
        "deadlocks_per_minute": 38
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6261daa0_data_engineer_b849634d",
    "content": "Pipeline stalled: 96 queries waiting for 4651ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-05T19:41:00Z",
      "severity": "warning",
      "incident_id": "6261daa0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 24,
        "affected_tables": "payments",
        "blocked_queries": 96,
        "lock_wait_ms": 4651
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "6261daa0_developer_1e529511",
    "content": "Deadlock rate 38/min causing 96 request failures",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-05T19:48:00Z",
      "severity": "warning",
      "incident_id": "6261daa0",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 24,
        "transaction_rollback": 198,
        "affected_tables": "payments",
        "blocked_queries": 96,
        "lock_wait_ms": 4651
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c676f607e85049e5",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-06T04:38:25Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_a6baec876d7a4392",
    "content": "CPU utilization stable at 26%",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-06T21:25:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_87c9fbe941d14518",
    "content": "Disk I/O normal - 2095 read IOPS, 2752 write IOPS",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-07T03:12:23Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_18231e5b42f746dd",
    "content": "Backup successful: 58.7GB completed in 46 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-07T04:23:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "df0de6ed_sre_133a0ea4",
    "content": "Replica health check: 78408801 bytes behind primary",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-07T07:22:00Z",
      "severity": "warning",
      "incident_id": "df0de6ed",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 230,
        "wait_count": 7,
        "lag_bytes": 78408801
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "df0de6ed_developer_1f47f821",
    "content": "Application seeing outdated data - 230s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-07T07:24:00Z",
      "severity": "warning",
      "incident_id": "df0de6ed",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 7,
        "replica_count": 1,
        "lag_seconds": 230,
        "lag_bytes": 78408801,
        "wal_size_mb": 3897
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "df0de6ed_dba_b9378e88",
    "content": "Streaming replication delayed 230s due to 3ms network latency",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-07T07:26:00Z",
      "severity": "warning",
      "incident_id": "df0de6ed",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 78408801,
        "network_latency_ms": 3,
        "wal_size_mb": 3897,
        "lag_seconds": 230,
        "replica_count": 1
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "68801bcb_dba_81d87023",
    "content": "Critical memory pressure: cache hit 52%, swap 13.4GB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-08T19:06:00Z",
      "severity": "warning",
      "incident_id": "68801bcb",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 6,
        "wait_count": 40,
        "swap_usage_gb": 13.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "68801bcb_developer_4dbc6e59",
    "content": "Database OOM: 6 connections terminated at 97% memory",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-08T19:08:00Z",
      "severity": "warning",
      "incident_id": "68801bcb",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 20,
        "memory_usage_percent": 97,
        "swap_usage_gb": 13.4,
        "wait_count": 40
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "68801bcb_data_engineer_adfe02ec",
    "content": "ETL job killed: OOM at 97% memory usage",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-08T19:14:00Z",
      "severity": "warning",
      "incident_id": "68801bcb",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 6,
        "memory_usage_percent": 97,
        "wait_count": 40
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "68801bcb_sre_c585f26a",
    "content": "System swapping 13.4GB - performance degradation detected",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-08T19:18:00Z",
      "severity": "warning",
      "incident_id": "68801bcb",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 40,
        "work_mem_mb": 20,
        "swap_usage_gb": 13.4
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_7c3c84df46a04377",
    "content": "Connection recycling: 53 connections refreshed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-09T13:08:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_e91d7d7f87f04edd",
    "content": "WAL archiving on schedule - 22 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-09T23:23:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_7f5d84834d344533",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-10T06:13:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_12740013223e4a7c",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-10T08:51:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_25583f16fff54449",
    "content": "Connection recycling: 86 connections refreshed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-10T15:48:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d6f34c7ed9924668",
    "content": "Query cache hit ratio: 88% over last 22 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-10T18:57:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_dc950037d9d84ac1",
    "content": "WAL archiving on schedule - 17 segments archived",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-11T00:15:34Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_646ed1f967fb4f98",
    "content": "Replication healthy - all 3 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-11T04:55:28Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "05530ba6_sre_d3975636",
    "content": "Alert: WAL accumulation 4186MB, lag 60s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-11T09:56:00Z",
      "severity": "warning",
      "incident_id": "05530ba6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 4186,
        "replica_count": 4,
        "wait_count": 25
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "05530ba6_developer_92aa3b48",
    "content": "Read-after-write inconsistency - replica 60s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-11T09:58:00Z",
      "severity": "warning",
      "incident_id": "05530ba6",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 78187941,
        "replica_count": 4,
        "lag_seconds": 60
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "fc0293ce_dba_1e7f5a33",
    "content": "Aggressive vacuum on 'logs' - 73340153 dead tuples, 76% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-11T18:58:00Z",
      "severity": "warning",
      "incident_id": "fc0293ce",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "vacuum_duration_min": 301,
        "table_bloat_percent": 76,
        "wait_count": 10,
        "dead_tuples": 73340153
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "fc0293ce_developer_e27be27f",
    "content": "Query performance degraded - table 'logs' 76% bloated",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-11T19:02:00Z",
      "severity": "warning",
      "incident_id": "fc0293ce",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 73340153,
        "table_bloat_percent": 76,
        "vacuum_duration_min": 301,
        "wait_count": 10,
        "io_usage_percent": 84
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "85115b07_developer_9fc2b5eb",
    "content": "Connection pool: 167 idle, 504 active - possible 338MB leak",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-12T09:08:00Z",
      "severity": "warning",
      "incident_id": "85115b07",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 50,
        "idle_connections": 167,
        "memory_usage_mb": 338,
        "leak_rate_per_hour": 16.8
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "85115b07_dba_44717b54",
    "content": "Connection accumulation: 16.8 connections/hour not released properly",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-12T09:18:00Z",
      "severity": "warning",
      "incident_id": "85115b07",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 20,
        "max_connections": 1000,
        "active": 504,
        "idle_connections": 167
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "85115b07_sre_559b3a60",
    "content": "Linear growth: 167 idle connections, 338MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-12T09:26:00Z",
      "severity": "warning",
      "incident_id": "85115b07",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 504.22466588666134,
        "active": 504,
        "memory_usage_mb": 338,
        "wait_count": 18,
        "leak_rate_per_hour": 16.8
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_0f77659d172b448a",
    "content": "Checkpoint completed: 49542 buffers written in 7.8 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-13T08:06:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_6b52efe7b7da4495",
    "content": "Memory usage normal - 11.6GB of 128GB used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-13T20:43:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8ffa8138_dba_aa07fe0c",
    "content": "Streaming replication delayed 239s due to 41ms network latency",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-14T23:07:00Z",
      "severity": "critical",
      "incident_id": "8ffa8138",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 1,
        "wait_count": 22,
        "lag_seconds": 239
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8ffa8138_sre_9a775c59",
    "content": "Replica health check: 63214455 bytes behind primary",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-14T23:08:00Z",
      "severity": "critical",
      "incident_id": "8ffa8138",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 1,
        "lag_seconds": 239,
        "network_latency_ms": 41,
        "wal_size_mb": 324
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8ffa8138_data_engineer_b2c3d778",
    "content": "ETL reading outdated data from replica lagging 239s",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-14T23:19:00Z",
      "severity": "critical",
      "incident_id": "8ffa8138",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 22,
        "lag_seconds": 239,
        "lag_bytes": 63214455,
        "wal_size_mb": 324
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "8ffa8138_developer_0aa9b7fc",
    "content": "Application seeing outdated data - 239s replication lag",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-14T23:21:00Z",
      "severity": "warning",
      "incident_id": "8ffa8138",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 41,
        "replica_count": 1,
        "lag_seconds": 239,
        "wal_size_mb": 324,
        "wait_count": 22
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0abeab9590884c9a",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-15T02:12:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_e6917ee9f7d84a26",
    "content": "Query performance within SLA - P99 latency 96ms",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-15T12:49:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_5a37cccb8fb1463e",
    "content": "Network latency optimal - 4.5ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-16T22:22:43Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "da137837_developer_f7cee05c",
    "content": "Database contention: 4922ms lock wait, 143 rollbacks",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-17T01:20:00Z",
      "severity": "warning",
      "incident_id": "da137837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 5,
        "wait_count": 46,
        "lock_wait_ms": 4922,
        "blocked_queries": 78
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "da137837_sre_67a4de3c",
    "content": "Alert: Deadlock storm - 30/min affecting 78 queries",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-17T01:28:00Z",
      "severity": "warning",
      "incident_id": "da137837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 78,
        "cascade_duration_min": 5,
        "affected_tables": "payments",
        "deadlocks_per_minute": 30,
        "lock_wait_ms": 4922
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "da137837_data_engineer_e36eec45",
    "content": "Ingestion blocked: 30/min deadlock rate on 'payments'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-17T01:34:00Z",
      "severity": "warning",
      "incident_id": "da137837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 30,
        "cascade_duration_min": 5,
        "wait_count": 46,
        "lock_wait_ms": 4922,
        "affected_tables": "payments"
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "da137837_dba_a62bd557",
    "content": "Deadlock storm: 78 blocked, 30/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-17T01:35:00Z",
      "severity": "warning",
      "incident_id": "da137837",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "payments",
        "transaction_rollback": 143,
        "cascade_duration_min": 5,
        "deadlocks_per_minute": 30
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_eee33a32ce914d08",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-17T10:35:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_42b09f3aa8c74fc4",
    "content": "Checkpoint completed: 9003 buffers written in 8.7 seconds",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-17T11:05:15Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_25d94480359247e8",
    "content": "Memory usage normal - 77.0GB of 128GB used",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-18T08:31:11Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "24febbd2_dba_1c824936",
    "content": "Deadlock cascade on 'shipments' - 41/min for 14min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-19T13:48:00Z",
      "severity": "warning",
      "incident_id": "24febbd2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "deadlocks_per_minute": 41,
        "cascade_duration_min": 14,
        "transaction_rollback": 95
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "24febbd2_sre_4b366bac",
    "content": "System alert: 6147ms lock wait impacting 95 transactions",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-19T14:02:00Z",
      "severity": "warning",
      "incident_id": "24febbd2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "wait_count": 35,
        "transaction_rollback": 95,
        "blocked_queries": 47,
        "lock_wait_ms": 6147
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "395a1857_dba_5739208a",
    "content": "Vacuum freeze on 'logs' - 75% IO utilization",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-19T22:21:00Z",
      "severity": "info",
      "incident_id": "395a1857",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 246,
        "dead_tuples": 44393429,
        "wait_count": 46
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "395a1857_sre_f28d087d",
    "content": "Performance impact: 246min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-19T22:24:00Z",
      "severity": "info",
      "incident_id": "395a1857",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "logs",
        "wait_count": 46,
        "io_usage_percent": 75
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_39d3cf8628164bb2",
    "content": "WAL archiving on schedule - 71 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-20T14:44:50Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e8b7b443_developer_61cccc99",
    "content": "Service degradation: 'report_aggregate' exceeding SLA by 447x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-20T22:04:00Z",
      "severity": "warning",
      "incident_id": "e8b7b443",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 16,
        "affected_query": "report_aggregate",
        "query_time_after_ms": 17675.27247015239,
        "rows_scanned": 22512097
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e8b7b443_data_engineer_0f131ca3",
    "content": "Data pipeline: 'report_aggregate' degraded from 105ms to 17675ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-20T22:05:00Z",
      "severity": "critical",
      "incident_id": "e8b7b443",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 22512097,
        "query_time_before_ms": 105.02001518621435,
        "slowdown_factor": 447
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "e8b7b443_sre_85d7ec94",
    "content": "P99 spike: report_aggregate from 105ms to 17675ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-20T22:06:00Z",
      "severity": "critical",
      "incident_id": "e8b7b443",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 22512097,
        "cpu_usage": 95,
        "slowdown_factor": 447,
        "query_time_before_ms": 105.02001518621435
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "be217278_dba_4df18c60",
    "content": "Sequential scan on 40965851 rows - index 'idx_products_sku' not used, CPU at 80%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-21T08:50:00Z",
      "severity": "critical",
      "incident_id": "be217278",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 71.55652948675304,
        "query_time_after_ms": 27111.215676910833,
        "rows_scanned": 40965851,
        "affected_query": "report_aggregate",
        "wait_count": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "be217278_sre_0ae3ed55",
    "content": "Latency alarm: 'report_aggregate' breached 27111ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-21T08:58:00Z",
      "severity": "critical",
      "incident_id": "be217278",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 27111.215676910833,
        "query_time_before_ms": 71.55652948675304,
        "wait_count": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "0d8ab999_sre_bd24893b",
    "content": "Projection: Storage full in 4 days (47GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-22T05:11:00Z",
      "severity": "info",
      "incident_id": "0d8ab999",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 39,
        "growth_rate_gb_per_day": 47,
        "disk_usage_percent": 93,
        "days_until_full": 4,
        "wal_size_gb": 45
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0d8ab999_dba_1524527e",
    "content": "Disk space critical: 93% used, growth rate 47GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-22T05:12:00Z",
      "severity": "info",
      "incident_id": "0d8ab999",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 4,
        "wal_size_gb": 45,
        "growth_rate_gb_per_day": 47,
        "wait_count": 39,
        "disk_usage_percent": 93
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "0d8ab999_developer_fca1e7cf",
    "content": "Database storage 93% full - writes may fail soon",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-22T05:15:00Z",
      "severity": "info",
      "incident_id": "0d8ab999",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "days_until_full": 4,
        "free_space_gb": 8,
        "disk_usage_percent": 93
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_dc242e33acaf4c55",
    "content": "Buffer cache warmed - 9.2GB loaded into memory",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-22T14:52:55Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "5f34c819_sre_c67796d3",
    "content": "Critical: Replication lag exceeding SLA (225s)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-22T23:18:00Z",
      "severity": "info",
      "incident_id": "5f34c819",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wal_size_mb": 1801,
        "replica_count": 3,
        "lag_bytes": 29294109,
        "network_latency_ms": 20
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5f34c819_developer_383e6990",
    "content": "Data consistency issue: replicas lagging by 29294109 bytes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-22T23:27:00Z",
      "severity": "info",
      "incident_id": "5f34c819",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "wait_count": 8,
        "wal_size_mb": 1801,
        "lag_seconds": 225
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "5f34c819_dba_e651bea8",
    "content": "WARNING: Replica 3 lagging 225s, WAL 1801MB",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-22T23:30:00Z",
      "severity": "info",
      "incident_id": "5f34c819",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 225,
        "wait_count": 8,
        "wal_size_mb": 1801,
        "lag_bytes": 29294109,
        "network_latency_ms": 20
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_95ed57b05e1043ce",
    "content": "Disk I/O normal - 5255 read IOPS, 3033 write IOPS",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-23T01:30:06Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_a5afa035f3644824",
    "content": "CPU utilization stable at 43%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-23T10:05:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "f68ef138_dba_e3145643",
    "content": "Long-running vacuum: 148min, blocking DDL operations",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-23T21:19:00Z",
      "severity": "info",
      "incident_id": "f68ef138",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "io_usage_percent": 82,
        "wait_count": 36,
        "dead_tuples": 74920795,
        "table_bloat_percent": 50
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f68ef138_data_engineer_d77b2cde",
    "content": "Batch job delayed by autovacuum processing 74920795 rows",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-23T21:28:00Z",
      "severity": "info",
      "incident_id": "f68ef138",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 74920795,
        "vacuum_duration_min": 148,
        "table_bloat_percent": 50,
        "tables_affected": "sessions"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "f68ef138_sre_6ca2c488",
    "content": "IO spike: 82% during vacuum of 'sessions'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-23T21:32:00Z",
      "severity": "info",
      "incident_id": "f68ef138",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "vacuum_duration_min": 148,
        "table_bloat_percent": 50,
        "dead_tuples": 74920795
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "02e38149_data_engineer_750c4c7e",
    "content": "Airflow DAG stalled: connection pool exhausted during 60.0min window",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-24T02:50:00Z",
      "severity": "info",
      "incident_id": "02e38149",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 91,
        "active_connections": 985.5529332317594,
        "max_connections": 1000,
        "wait_count": 34
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "02e38149_dba_ec7fffb6",
    "content": "Database refusing connections: SQLSTATE 53300 after 29809ms wait",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-24T03:08:00Z",
      "severity": "info",
      "incident_id": "02e38149",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "threshold": 91,
        "active": 985,
        "wait_ms": 29809
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_01c1511d0015472e",
    "content": "Autovacuum completed on table 'users' - 594094 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-24T13:55:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c80fde71_sre_d609868e",
    "content": "Monitoring: Autovacuum causing 89% IO saturation",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-24T17:57:00Z",
      "severity": "critical",
      "incident_id": "c80fde71",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "metrics",
        "table_bloat_percent": 39,
        "io_usage_percent": 89
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c80fde71_dba_2c015c5b",
    "content": "Autovacuum running 304min on 'metrics', IO at 89%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-24T17:58:00Z",
      "severity": "critical",
      "incident_id": "c80fde71",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 39,
        "tables_affected": "metrics",
        "vacuum_duration_min": 304,
        "io_usage_percent": 89
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "c80fde71_data_engineer_5d2c5892",
    "content": "Table 'metrics' bloat (39%) affecting query performance",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-24T18:02:00Z",
      "severity": "critical",
      "incident_id": "c80fde71",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 39,
        "tables_affected": "metrics",
        "io_usage_percent": 89
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "c80fde71_developer_9f8a9a99",
    "content": "Database maintenance causing 89% IO usage",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-24T18:09:00Z",
      "severity": "warning",
      "incident_id": "c80fde71",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 27519276,
        "vacuum_duration_min": 304,
        "tables_affected": "metrics",
        "table_bloat_percent": 39
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_6352ae236f7443b9",
    "content": "Disk I/O normal - 7136 read IOPS, 4753 write IOPS",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-25T10:31:21Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_584a43b3405941f2",
    "content": "WAL archiving on schedule - 99 segments archived",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-25T19:41:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_2724204981dc41c1",
    "content": "Connection pool stable at 52 connections (15% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-25T22:16:58Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_f481604b554a4043",
    "content": "Checkpoint completed: 16251 buffers written in 8.9 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-26T14:43:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "69090243_sre_08257f5a",
    "content": "Anomaly: Idle connections increasing 7/hour for past 10 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-26T23:23:00Z",
      "severity": "critical",
      "incident_id": "69090243",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 540,
        "active_connections": 540.1094384149476,
        "wait_count": 47,
        "threshold": 54,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "69090243_dba_aac1d3f3",
    "content": "WARNING: 256 idle connections detected (leak rate: 7.3/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-26T23:28:00Z",
      "severity": "critical",
      "incident_id": "69090243",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 540.1094384149476,
        "wait_count": 47,
        "leak_duration_hours": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "69090243_data_engineer_25c62be2",
    "content": "Spark executors holding 256 idle connections, 452MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-26T23:34:00Z",
      "severity": "critical",
      "incident_id": "69090243",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "threshold": 54,
        "max_connections": 1000,
        "idle_connections": 256,
        "leak_rate_per_hour": 7.3,
        "leak_duration_hours": 10
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "69090243_developer_45450d4e",
    "content": "WARNING: 256 connections not returned to pool after 10h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-26T23:37:00Z",
      "severity": "critical",
      "incident_id": "69090243",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "idle_connections": 256,
        "memory_usage_mb": 452,
        "leak_rate_per_hour": 7.3,
        "wait_count": 47
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_6e7446fdbacb47b5",
    "content": "Checkpoint completed: 39412 buffers written in 7.1 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-27T10:49:05Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_46f008d645f04421",
    "content": "Statistics updated for 10 tables",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-28T05:57:31Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "88051926_dba_e24f9d4f",
    "content": "Deadlock storm: 29 blocked, 16/min rate",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-30T00:34:00Z",
      "severity": "critical",
      "incident_id": "88051926",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 9523,
        "cascade_duration_min": 6,
        "deadlocks_per_minute": 16
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "88051926_developer_7e5f8faa",
    "content": "Database contention: 9523ms lock wait, 47 rollbacks",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-04-30T00:34:00Z",
      "severity": "critical",
      "incident_id": "88051926",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 16,
        "transaction_rollback": 47,
        "cascade_duration_min": 6,
        "lock_wait_ms": 9523,
        "affected_tables": "payments"
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "a4621b33_sre_600c2b12",
    "content": "Latency alarm: 'order_search' breached 5507ms threshold",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-04-30T15:55:00Z",
      "severity": "warning",
      "incident_id": "a4621b33",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 108.58392224052051,
        "wait_count": 38,
        "slowdown_factor": 316
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a4621b33_data_engineer_d6e4bbf0",
    "content": "ETL bottleneck: 'order_search' step taking 5507ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-04-30T15:56:00Z",
      "severity": "warning",
      "incident_id": "a4621b33",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "affected_query": "order_search",
        "wait_count": 38,
        "slowdown_factor": 316,
        "rows_scanned": 7031751,
        "query_time_before_ms": 108.58392224052051
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a4621b33_dba_39677059",
    "content": "Sequential scan on 7031751 rows - index 'idx_sessions_user' not used, CPU at 89%",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-04-30T16:09:00Z",
      "severity": "warning",
      "incident_id": "a4621b33",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 38,
        "query_time_after_ms": 5507.617312258114,
        "slowdown_factor": 316
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_e266df4cdf324466",
    "content": "Replication healthy - all 2 replicas in sync",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-01T08:49:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "21d74a95_sre_feb0ffc7",
    "content": "Alert: WAL accumulation 1198MB, lag 112s",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-01T18:55:00Z",
      "severity": "critical",
      "incident_id": "21d74a95",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_seconds": 112,
        "wal_size_mb": 1198,
        "replica_count": 2
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "21d74a95_data_engineer_46172242",
    "content": "Analytics queries on stale data - replica 112s behind",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-01T18:56:00Z",
      "severity": "critical",
      "incident_id": "21d74a95",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "lag_bytes": 63117724,
        "wal_size_mb": 1198,
        "network_latency_ms": 21,
        "lag_seconds": 112
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "21d74a95_dba_2eadf9be",
    "content": "Read replica out of sync by 112 seconds",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-01T19:04:00Z",
      "severity": "critical",
      "incident_id": "21d74a95",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "replica_count": 2,
        "lag_bytes": 63117724,
        "wait_count": 35,
        "wal_size_mb": 1198
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "21d74a95_developer_fe2977d7",
    "content": "Read-after-write inconsistency - replica 112s behind",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-01T19:06:00Z",
      "severity": "critical",
      "incident_id": "21d74a95",
      "incident_type": "replication_lag",
      "incident_category": "replication",
      "metrics": {
        "network_latency_ms": 21,
        "lag_bytes": 63117724,
        "lag_seconds": 112,
        "wal_size_mb": 1198
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "wal_shipping",
        "streaming_replication"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_49d609ae2d284a94",
    "content": "Backup successful: 43.9GB completed in 18 minutes",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-02T01:10:48Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "b1dd5106_data_engineer_1fb18e75",
    "content": "ETL jobs at risk - disk 95% full",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-03T01:51:00Z",
      "severity": "critical",
      "incident_id": "b1dd5106",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 48,
        "free_space_gb": 42,
        "growth_rate_gb_per_day": 46,
        "days_until_full": 5,
        "wal_size_gb": 22
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "b1dd5106_sre_bd3ad2ae",
    "content": "Projection: Storage full in 5 days (46GB/day growth)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-03T02:00:00Z",
      "severity": "critical",
      "incident_id": "b1dd5106",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "free_space_gb": 42,
        "growth_rate_gb_per_day": 46,
        "wal_size_gb": 22,
        "disk_usage_percent": 95,
        "days_until_full": 5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "b1dd5106_dba_7568dec8",
    "content": "Disk space critical: 95% used, growth rate 46GB/day",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-03T02:07:00Z",
      "severity": "critical",
      "incident_id": "b1dd5106",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 95,
        "days_until_full": 5,
        "growth_rate_gb_per_day": 46,
        "wal_size_gb": 22,
        "wait_count": 48
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_75f8be691a1645c9",
    "content": "Memory usage normal - 42.3GB of 128GB used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-03T15:01:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "607b3a76_sre_f6c3ad90",
    "content": "Resource exhaustion: 4 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-03T20:52:00Z",
      "severity": "critical",
      "incident_id": "607b3a76",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "buffer_cache_hit": 32,
        "memory_usage_percent": 89,
        "wait_count": 48,
        "swap_usage_gb": 6.5,
        "oom_kills": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "607b3a76_dba_d9c00c81",
    "content": "Memory exhaustion: 89% RAM, 6.5GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-03T21:02:00Z",
      "severity": "critical",
      "incident_id": "607b3a76",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "oom_kills": 4,
        "buffer_cache_hit": 32,
        "wait_count": 48,
        "swap_usage_gb": 6.5
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "607b3a76_developer_1eed0950",
    "content": "Database swapping 6.5GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-03T21:02:00Z",
      "severity": "critical",
      "incident_id": "607b3a76",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 89,
        "buffer_cache_hit": 32,
        "oom_kills": 4
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "607b3a76_data_engineer_ab05e301",
    "content": "Batch job performance degraded - cache hit 32%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-03T21:12:00Z",
      "severity": "critical",
      "incident_id": "607b3a76",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "wait_count": 48,
        "oom_kills": 4,
        "buffer_cache_hit": 32
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "a27b05f2_developer_dea39c42",
    "content": "Application errors: 57 transactions failed in 7min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-04T10:11:00Z",
      "severity": "critical",
      "incident_id": "a27b05f2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 77,
        "wait_count": 8,
        "affected_tables": "payments",
        "lock_wait_ms": 2221
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "cloudwatch"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a27b05f2_dba_751c1887",
    "content": "Deadlock cascade on 'payments' - 23/min for 7min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-04T10:16:00Z",
      "severity": "critical",
      "incident_id": "a27b05f2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 57,
        "deadlocks_per_minute": 23,
        "wait_count": 8
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "a27b05f2_data_engineer_9d411066",
    "content": "Batch process deadlocked - 7min duration",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-04T10:26:00Z",
      "severity": "critical",
      "incident_id": "a27b05f2",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 57,
        "blocked_queries": 77,
        "lock_wait_ms": 2221,
        "cascade_duration_min": 7,
        "wait_count": 8
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_1615bd1a349149fa",
    "content": "CPU utilization stable at 57%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-04T13:34:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "48a9ee60_sre_3d125f50",
    "content": "Linear growth: 263 idle connections, 187MB memory consumed",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-05T07:40:00Z",
      "severity": "critical",
      "incident_id": "48a9ee60",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_duration_hours": 4,
        "active_connections": 699.7938153406623,
        "leak_rate_per_hour": 5.2
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "48a9ee60_data_engineer_9ef7799a",
    "content": "ETL connection pool: 263 idle connections leaking at 5/hour",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-05T07:41:00Z",
      "severity": "critical",
      "incident_id": "48a9ee60",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 699,
        "wait_count": 34,
        "active_connections": 699.7938153406623,
        "leak_rate_per_hour": 5.2,
        "max_connections": 1000
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_fbd01ba1182f4e2b",
    "content": "Network latency optimal - 4.1ms average RTT",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-05T19:16:18Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_a7505e1c62ec4a66",
    "content": "Memory usage normal - 92.8GB of 128GB used",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-06T15:04:32Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_170da0569306446d",
    "content": "WAL archiving on schedule - 38 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-07T23:23:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "29e19bb4_data_engineer_8e67bf57",
    "content": "ETL performance impacted by vacuum on 'metrics'",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-07T23:50:00Z",
      "severity": "warning",
      "incident_id": "29e19bb4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 1700376,
        "tables_affected": "metrics",
        "table_bloat_percent": 67,
        "vacuum_duration_min": 253
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "cloudwatch"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "29e19bb4_sre_a56772bd",
    "content": "Performance impact: 253min vacuum operation running",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-07T23:51:00Z",
      "severity": "warning",
      "incident_id": "29e19bb4",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 67,
        "tables_affected": "metrics",
        "dead_tuples": 1700376,
        "vacuum_duration_min": 253
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "00e247f9_dba_caec080d",
    "content": "Circular wait on 'users' - 8677ms average wait time",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-08T14:00:00Z",
      "severity": "info",
      "incident_id": "00e247f9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "transaction_rollback": 23,
        "affected_tables": "users",
        "deadlocks_per_minute": 18,
        "wait_count": 12
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "00e247f9_sre_cc6ba405",
    "content": "Service disruption: 23 failed transactions in 9min",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-08T14:03:00Z",
      "severity": "info",
      "incident_id": "00e247f9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "cascade_duration_min": 9,
        "wait_count": 12,
        "blocked_queries": 10,
        "deadlocks_per_minute": 18
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "cloudwatch"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_93597b122c064e4b",
    "content": "Disk I/O normal - 4557 read IOPS, 3740 write IOPS",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-08T14:06:49Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "00e247f9_developer_4854348f",
    "content": "TransactionRollback: deadlock after 8677ms on 'users'",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-08T14:19:00Z",
      "severity": "info",
      "incident_id": "00e247f9",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 8677,
        "affected_tables": "users",
        "transaction_rollback": 23,
        "cascade_duration_min": 9
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "7b6fe4f8_data_engineer_63b73293",
    "content": "Spark job waiting 22692ms for database connection - 16 executors blocked",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-09T03:04:00Z",
      "severity": "critical",
      "incident_id": "7b6fe4f8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 89,
        "active": 969,
        "max_connections": 1000,
        "wait_count": 16
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "7b6fe4f8_dba_c35d0e9d",
    "content": "PANIC: connection limit exceeded - 969/1000 after 49.8min spike",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-09T03:20:00Z",
      "severity": "critical",
      "incident_id": "7b6fe4f8",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "threshold": 89,
        "active": 969,
        "max_connections": 1000,
        "spike_duration_min": 49.8,
        "active_connections": 969.2005453444907
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_bac051847a254ce8",
    "content": "Replication healthy - all 4 replicas in sync",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-10T02:33:37Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "164d0133_sre_364991aa",
    "content": "Incident: Lock contention causing 72 query backlog",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-10T15:09:00Z",
      "severity": "critical",
      "incident_id": "164d0133",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 14,
        "wait_count": 33,
        "transaction_rollback": 74,
        "lock_wait_ms": 6567
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "164d0133_dba_4e91145c",
    "content": "CRITICAL: 72 queries waiting, deadlock rate: 14/min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-10T15:10:00Z",
      "severity": "critical",
      "incident_id": "164d0133",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 14,
        "wait_count": 33,
        "transaction_rollback": 74
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "e25ee277_data_engineer_733ebdce",
    "content": "Data pipeline may fail: only 33GB space available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-11T05:26:00Z",
      "severity": "critical",
      "incident_id": "e25ee277",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "wait_count": 34,
        "growth_rate_gb_per_day": 19,
        "wal_size_gb": 31,
        "days_until_full": 3
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "rds"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "e25ee277_sre_fd8f7005",
    "content": "Critical infrastructure alert: WAL using 31GB of disk",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-11T05:39:00Z",
      "severity": "critical",
      "incident_id": "e25ee277",
      "incident_type": "disk_space_critical",
      "incident_category": "storage",
      "metrics": {
        "disk_usage_percent": 93,
        "wait_count": 34,
        "growth_rate_gb_per_day": 19
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "aurora",
        "ec2"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_40e4ce0d8aae4938",
    "content": "Autovacuum completed on table 'orders' - 599102 rows processed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-11T05:57:53Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_512b10066e2241ed",
    "content": "Connection recycling: 35 connections refreshed",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-11T21:51:51Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3251c96a_data_engineer_ba5e85ee",
    "content": "Data load competing with 176min vacuum operation",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-11T23:24:00Z",
      "severity": "info",
      "incident_id": "3251c96a",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "table_bloat_percent": 72,
        "tables_affected": "events",
        "vacuum_duration_min": 176
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "rds"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3251c96a_sre_6abfe532",
    "content": "Disk usage alert: 'events' table 72% bloated",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-11T23:33:00Z",
      "severity": "info",
      "incident_id": "3251c96a",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "dead_tuples": 33197151,
        "tables_affected": "events",
        "table_bloat_percent": 72,
        "vacuum_duration_min": 176
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "cloudwatch",
        "aurora"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "3251c96a_dba_626c66c2",
    "content": "Aggressive vacuum on 'events' - 33197151 dead tuples, 72% bloat",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-11T23:34:00Z",
      "severity": "info",
      "incident_id": "3251c96a",
      "incident_type": "autovacuum_issues",
      "incident_category": "maintenance",
      "metrics": {
        "tables_affected": "events",
        "wait_count": 13,
        "dead_tuples": 33197151,
        "vacuum_duration_min": 176,
        "table_bloat_percent": 72
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_d7e4420ffe764dfe",
    "content": "Connection pool stable at 248 connections (36% utilization)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-12T20:16:04Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_c6b2872719874025",
    "content": "Statistics updated for 50 tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-14T04:14:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_5c36720931154461",
    "content": "Query cache hit ratio: 90% over last 14 minutes",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-16T03:57:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_005f7534f7d34da2",
    "content": "WAL archiving on schedule - 55 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-16T18:41:44Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_7725b9731d2842d1",
    "content": "Connection pool stable at 379 connections (21% utilization)",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-17T03:22:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_0189f87d79a2493b",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-18T10:47:07Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_8b1796453b9e43cc",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-19T00:48:36Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_bc8218f6a7604fbc",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-19T06:26:10Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_722760e768e54b22",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-19T08:44:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "292025ca_dba_1cdf86f0",
    "content": "Deadlock cascade on 'shipments' - 34/min for 4min",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-19T13:48:00Z",
      "severity": "info",
      "incident_id": "292025ca",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "blocked_queries": 53,
        "affected_tables": "shipments",
        "cascade_duration_min": 4,
        "lock_wait_ms": 9151,
        "transaction_rollback": 124
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_9e1ccac8e47c4970",
    "content": "SSL certificate valid for 126 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-19T13:49:13Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "292025ca_developer_aa5efa9c",
    "content": "Application errors: 124 transactions failed in 4min",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-19T13:52:00Z",
      "severity": "info",
      "incident_id": "292025ca",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "lock_wait_ms": 9151,
        "blocked_queries": 53,
        "cascade_duration_min": 4,
        "deadlocks_per_minute": 34,
        "affected_tables": "shipments"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "aurora"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "292025ca_data_engineer_40105c2c",
    "content": "Data load failing: 124 operations rolled back",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-19T13:55:00Z",
      "severity": "warning",
      "incident_id": "292025ca",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "deadlocks_per_minute": 34,
        "wait_count": 37,
        "blocked_queries": 53
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "rds",
        "ec2"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "292025ca_sre_7dc8f3f1",
    "content": "Critical: 34 deadlocks/min on production tables",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-19T13:55:00Z",
      "severity": "info",
      "incident_id": "292025ca",
      "incident_type": "deadlock_cascade",
      "incident_category": "locking",
      "metrics": {
        "affected_tables": "shipments",
        "cascade_duration_min": 4,
        "blocked_queries": 53,
        "wait_count": 37,
        "transaction_rollback": 124
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "ec2",
        "rds"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_d71235354f0e4907",
    "content": "WAL archiving on schedule - 54 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-19T22:40:35Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "info_0d75ca1a676647ca",
    "content": "All scheduled maintenance tasks completed successfully",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-21T04:35:10Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "52abfe85_data_engineer_bdf7769e",
    "content": "Batch job performance degraded - cache hit 69%",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-22T06:17:00Z",
      "severity": "warning",
      "incident_id": "52abfe85",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 6.9,
        "buffer_cache_hit": 69,
        "work_mem_mb": 10
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "52abfe85_sre_c9f73312",
    "content": "Resource exhaustion: 5 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-22T06:17:00Z",
      "severity": "info",
      "incident_id": "52abfe85",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 92,
        "swap_usage_gb": 6.9,
        "work_mem_mb": 10
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "52abfe85_developer_21e3f6e4",
    "content": "Database swapping 6.9GB - response times degraded",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-22T06:25:00Z",
      "severity": "info",
      "incident_id": "52abfe85",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 92,
        "wait_count": 47,
        "buffer_cache_hit": 69,
        "work_mem_mb": 10
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_8cfddb9898c443ab",
    "content": "Checkpoint completed: 26393 buffers written in 1.8 seconds",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-22T10:49:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "abcdc267_data_engineer_6c0d948f",
    "content": "DBT run failed: waited 23296ms for connection from saturated pool",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-22T13:08:00Z",
      "severity": "critical",
      "incident_id": "abcdc267",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "error_code": "PG-53300",
        "max_connections": 1000,
        "threshold": 90,
        "wait_count": 35,
        "wait_ms": 23296
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "abcdc267_developer_f50a175b",
    "content": "JDBC connection failed after 23296ms: FATAL: too many clients already",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-22T13:14:00Z",
      "severity": "warning",
      "incident_id": "abcdc267",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "spike_duration_min": 43.6,
        "wait_count": 35,
        "wait_ms": 23296,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "abcdc267_dba_3c07eb6c",
    "content": "ERROR: too many connections for role 'app_user' (current: 964, max: 1000)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-22T13:17:00Z",
      "severity": "critical",
      "incident_id": "abcdc267",
      "incident_type": "connection_exhaustion_spike",
      "incident_category": "connection",
      "metrics": {
        "active": 964,
        "error_code": "PG-53300",
        "active_connections": 964.0087954835377,
        "wait_count": 35
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_f0cbf206030c468b",
    "content": "SSL certificate valid for 129 more days",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-23T04:08:08Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_3d7a82e5e70141be",
    "content": "Index 'idx_orders_date' rebuild completed - 186MB size reduction",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-23T08:30:56Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "43b101d1_dba_c09740f7",
    "content": "WARNING: 226 idle connections detected (leak rate: 18.6/hour)",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-23T12:39:00Z",
      "severity": "critical",
      "incident_id": "43b101d1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 581.2532680372265,
        "max_connections": 1000,
        "threshold": 58,
        "leak_duration_hours": 2,
        "active": 581
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "43b101d1_developer_c201ad4d",
    "content": "WARNING: 226 connections not returned to pool after 2h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-23T12:43:00Z",
      "severity": "warning",
      "incident_id": "43b101d1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "leak_rate_per_hour": 18.6,
        "active_connections": 581.2532680372265,
        "memory_usage_mb": 445
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "43b101d1_data_engineer_61140dbd",
    "content": "Spark executors holding 226 idle connections, 445MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-23T12:50:00Z",
      "severity": "critical",
      "incident_id": "43b101d1",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 581,
        "memory_usage_mb": 445,
        "leak_duration_hours": 2,
        "active_connections": 581.2532680372265
      },
      "task_context": "incident_response",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "6c8d3b78_sre_3c9e6dd0",
    "content": "Resource exhaustion: 6 processes killed due to memory limits",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-24T07:08:00Z",
      "severity": "warning",
      "incident_id": "6c8d3b78",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "swap_usage_gb": 14.5,
        "work_mem_mb": 47,
        "buffer_cache_hit": 37,
        "memory_usage_percent": 89,
        "wait_count": 38
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6c8d3b78_data_engineer_0e704ef5",
    "content": "Pipeline memory issues: work_mem only 47MB available",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-24T07:19:00Z",
      "severity": "warning",
      "incident_id": "6c8d3b78",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "memory_usage_percent": 89,
        "oom_kills": 6,
        "work_mem_mb": 47,
        "buffer_cache_hit": 37
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "6c8d3b78_dba_f1458885",
    "content": "Memory exhaustion: 89% RAM, 14.5GB swap active",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-24T07:21:00Z",
      "severity": "warning",
      "incident_id": "6c8d3b78",
      "incident_type": "memory_pressure",
      "incident_category": "resource",
      "metrics": {
        "work_mem_mb": 47,
        "swap_usage_gb": 14.5,
        "buffer_cache_hit": 37,
        "wait_count": 38
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "memory_manager",
        "oom_killer"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_00e00819d0e4489d",
    "content": "Network latency optimal - 1.3ms average RTT",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-24T14:06:40Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_37ab624ffa8b423b",
    "content": "Checkpoint completed: 29125 buffers written in 6.9 seconds",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-24T18:54:42Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "evening"
    }
  },
  {
    "doc_id": "d950532e_data_engineer_98a7b02a",
    "content": "Data pipeline: 'inventory_check' degraded from 114ms to 57836ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-25T09:26:00Z",
      "severity": "info",
      "incident_id": "d950532e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 88,
        "query_time_before_ms": 114.95729029499964,
        "slowdown_factor": 173,
        "affected_query": "inventory_check"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d950532e_sre_2181f91e",
    "content": "P99 spike: inventory_check from 114ms to 57836ms",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-25T09:40:00Z",
      "severity": "info",
      "incident_id": "d950532e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "wait_count": 28,
        "rows_scanned": 25183983,
        "slowdown_factor": 173,
        "query_time_after_ms": 57836.17848640033,
        "affected_query": "inventory_check"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "d950532e_developer_a1ee0b67",
    "content": "Service degradation: 'inventory_check' exceeding SLA by 173x factor",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-25T09:44:00Z",
      "severity": "info",
      "incident_id": "d950532e",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "cpu_usage": 88,
        "wait_count": 28,
        "slowdown_factor": 173,
        "query_time_before_ms": 114.95729029499964,
        "affected_query": "inventory_check"
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_c39fd3fcac2245e4",
    "content": "Monitoring agent heartbeat received - all systems operational",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-25T15:29:01Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_eb83257fc5f3460a",
    "content": "SSL certificate valid for 232 more days",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-26T01:55:02Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "info_5b86a8577f3842d5",
    "content": "Connection recycling: 74 connections refreshed",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-26T06:17:46Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_6c1bfe5c4c8e4e1c",
    "content": "Network latency optimal - 3.8ms average RTT",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-26T11:48:39Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  },
  {
    "doc_id": "info_fbcace03eab84540",
    "content": "Query performance within SLA - P99 latency 254ms",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-26T13:15:41Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a8981139_sre_0e53be65",
    "content": "Anomaly: Idle connections increasing 15/hour for past 6 hours",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-26T14:23:00Z",
      "severity": "warning",
      "incident_id": "a8981139",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active": 683,
        "max_connections": 1000,
        "idle_connections": 177,
        "threshold": 68,
        "memory_usage_mb": 440
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a8981139_data_engineer_7d9db9c5",
    "content": "Spark executors holding 177 idle connections, 440MB wasted",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-26T14:25:00Z",
      "severity": "warning",
      "incident_id": "a8981139",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "active_connections": 683.7497183836598,
        "memory_usage_mb": 440,
        "active": 683,
        "max_connections": 1000
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "a8981139_developer_20122ea1",
    "content": "WARNING: 177 connections not returned to pool after 6h",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-26T14:31:00Z",
      "severity": "warning",
      "incident_id": "a8981139",
      "incident_type": "slow_connection_leak",
      "incident_category": "connection",
      "metrics": {
        "wait_count": 39,
        "max_connections": 1000,
        "idle_connections": 177,
        "memory_usage_mb": 440
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "connection_pool",
        "pgbouncer"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_353607cdd90140a1",
    "content": "Connection pool stable at 344 connections (41% utilization)",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-26T17:41:17Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_ae2aee2478d8430d",
    "content": "WAL archiving on schedule - 70 segments archived",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-27T00:32:00Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4f34e4d9_data_engineer_d7e6cec6",
    "content": "Batch job delayed: user_dashboard running 12x slower",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-27T03:54:00Z",
      "severity": "warning",
      "incident_id": "4f34e4d9",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "slowdown_factor": 12,
        "affected_query": "user_dashboard",
        "rows_scanned": 4905129
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4f34e4d9_sre_c3096324",
    "content": "CloudWatch: Query latency increased 12x for 'user_dashboard'",
    "mcp_metadata": {
      "persona": "sre",
      "timestamp": "2025-05-27T03:59:00Z",
      "severity": "info",
      "incident_id": "4f34e4d9",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_before_ms": 129.26388538836198,
        "cpu_usage": 81,
        "query_time_after_ms": 6239.450595424567
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4f34e4d9_dba_93e6c302",
    "content": "Query plan changed: 'user_dashboard' now takes 6239ms scanning 4905129 rows",
    "mcp_metadata": {
      "persona": "dba",
      "timestamp": "2025-05-27T04:03:00Z",
      "severity": "info",
      "incident_id": "4f34e4d9",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 6239.450595424567,
        "wait_count": 31,
        "cpu_usage": 81
      },
      "task_context": "routine_check",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "overnight"
    }
  },
  {
    "doc_id": "4c0979db_developer_e13d7606",
    "content": "User-facing slowdown: inventory_check response time degraded 247x",
    "mcp_metadata": {
      "persona": "developer",
      "timestamp": "2025-05-27T12:04:00Z",
      "severity": "warning",
      "incident_id": "4c0979db",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "query_time_after_ms": 40114.750371951464,
        "cpu_usage": 83,
        "slowdown_factor": 247
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "4c0979db_data_engineer_35839873",
    "content": "Dashboard failing: inventory_check query 247x slower than normal",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-27T12:18:00Z",
      "severity": "warning",
      "incident_id": "4c0979db",
      "incident_type": "query_performance_degradation",
      "incident_category": "performance",
      "metrics": {
        "rows_scanned": 28661629,
        "cpu_usage": 83,
        "wait_count": 31
      },
      "task_context": "monitoring",
      "related_systems": [
        "postgresql",
        "monitoring",
        "query_optimizer",
        "indexes"
      ],
      "temporal_marker": "afternoon"
    }
  },
  {
    "doc_id": "info_88a3fbfcbb054408",
    "content": "Network latency optimal - 0.8ms average RTT",
    "mcp_metadata": {
      "persona": "data_engineer",
      "timestamp": "2025-05-28T11:04:52Z",
      "severity": "info",
      "task_context": "monitoring",
      "metrics": {},
      "related_systems": [
        "postgresql",
        "monitoring"
      ],
      "temporal_marker": "morning"
    }
  }
]